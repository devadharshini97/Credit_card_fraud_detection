{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XJ6CMsCQ-sk",
        "outputId": "5c21416f-51bd-4ecf-c420-28722d7c9ad6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import os\n",
        "import tempfile\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import sklearn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import imblearn\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import  train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import math\n",
        "import collections\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import keras.backend as K \n",
        "from tensorflow.python.ops import math_ops\n"
      ],
      "metadata": {
        "id": "LcXmMDGBLM-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import glob\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten, Reshape\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.optimizers import SGD, Adam, RMSprop, Adagrad\n",
        "from keras.layers import Dropout, InputLayer, LSTM\n",
        "from keras.layers import Bidirectional, BatchNormalization\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, BatchNormalization, Dropout, UpSampling1D, AveragePooling1D\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import classification_report, ConfusionMatrixDisplay"
      ],
      "metadata": {
        "id": "zA4kG98QjG3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data = pd.read_csv('/content/drive/MyDrive/ALDA_final_project/creditcard.csv')\n",
        "data = pd.read_csv('https://storage.googleapis.com/download.tensorflow.org/data/creditcard.csv')\n"
      ],
      "metadata": {
        "id": "y6PCFrCsRHZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.duplicated().value_counts().plot.bar()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "qFRentEXRJTp",
        "outputId": "b875bfe1-ec9c-441e-88a8-d7cd9c40d9b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "cVLM5Cj4Rtf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.duplicated().value_counts().plot.bar()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "rgJOKLyfR09y",
        "outputId": "ed140d0d-dbf7-4b87-84ec-7ac26d21ecd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['Class'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvkgrnsyR3Es",
        "outputId": "133d3abc-80d9-4d9a-8c27-c9467dd2b112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    283253\n",
              "1       473\n",
              "Name: Class, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "exwFsLJWR7va",
        "outputId": "f7fd6dfe-7d28-4b49-ab17-db5a4c99d9b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
              "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
              "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
              "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
              "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
              "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
              "\n",
              "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
              "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
              "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
              "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
              "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
              "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
              "\n",
              "        V26       V27       V28  Amount  Class  \n",
              "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
              "1  0.125895 -0.008983  0.014724    2.69      0  \n",
              "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
              "3 -0.221929  0.062723  0.061458  123.50      0  \n",
              "4  0.502292  0.219422  0.215153   69.99      0  \n",
              "\n",
              "[5 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-60219923-0a04-4998-9f2e-c8cf37add6be\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>...</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>...</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-60219923-0a04-4998-9f2e-c8cf37add6be')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-60219923-0a04-4998-9f2e-c8cf37add6be button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-60219923-0a04-4998-9f2e-c8cf37add6be');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "LPf-L3slSc5K",
        "outputId": "44d63d70-c116-4368-bd9b-1338a4379609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                Time             V1             V2             V3  \\\n",
              "count  283726.000000  283726.000000  283726.000000  283726.000000   \n",
              "mean    94811.077600       0.005917      -0.004135       0.001613   \n",
              "std     47481.047891       1.948026       1.646703       1.508682   \n",
              "min         0.000000     -56.407510     -72.715728     -48.325589   \n",
              "25%     54204.750000      -0.915951      -0.600321      -0.889682   \n",
              "50%     84692.500000       0.020384       0.063949       0.179963   \n",
              "75%    139298.000000       1.316068       0.800283       1.026960   \n",
              "max    172792.000000       2.454930      22.057729       9.382558   \n",
              "\n",
              "                  V4             V5             V6             V7  \\\n",
              "count  283726.000000  283726.000000  283726.000000  283726.000000   \n",
              "mean       -0.002966       0.001828      -0.001139       0.001801   \n",
              "std         1.414184       1.377008       1.331931       1.227664   \n",
              "min        -5.683171    -113.743307     -26.160506     -43.557242   \n",
              "25%        -0.850134      -0.689830      -0.769031      -0.552509   \n",
              "50%        -0.022248      -0.053468      -0.275168       0.040859   \n",
              "75%         0.739647       0.612218       0.396792       0.570474   \n",
              "max        16.875344      34.801666      73.301626     120.589494   \n",
              "\n",
              "                  V8             V9  ...            V21            V22  \\\n",
              "count  283726.000000  283726.000000  ...  283726.000000  283726.000000   \n",
              "mean       -0.000854      -0.001596  ...      -0.000371      -0.000015   \n",
              "std         1.179054       1.095492  ...       0.723909       0.724550   \n",
              "min       -73.216718     -13.434066  ...     -34.830382     -10.933144   \n",
              "25%        -0.208828      -0.644221  ...      -0.228305      -0.542700   \n",
              "50%         0.021898      -0.052596  ...      -0.029441       0.006675   \n",
              "75%         0.325704       0.595977  ...       0.186194       0.528245   \n",
              "max        20.007208      15.594995  ...      27.202839      10.503090   \n",
              "\n",
              "                 V23            V24            V25            V26  \\\n",
              "count  283726.000000  283726.000000  283726.000000  283726.000000   \n",
              "mean        0.000198       0.000214      -0.000232       0.000149   \n",
              "std         0.623702       0.605627       0.521220       0.482053   \n",
              "min       -44.807735      -2.836627     -10.295397      -2.604551   \n",
              "25%        -0.161703      -0.354453      -0.317485      -0.326763   \n",
              "50%        -0.011159       0.041016       0.016278      -0.052172   \n",
              "75%         0.147748       0.439738       0.350667       0.240261   \n",
              "max        22.528412       4.584549       7.519589       3.517346   \n",
              "\n",
              "                 V27            V28         Amount          Class  \n",
              "count  283726.000000  283726.000000  283726.000000  283726.000000  \n",
              "mean        0.001763       0.000547      88.472687       0.001667  \n",
              "std         0.395744       0.328027     250.399437       0.040796  \n",
              "min       -22.565679     -15.430084       0.000000       0.000000  \n",
              "25%        -0.070641      -0.052818       5.600000       0.000000  \n",
              "50%         0.001479       0.011288      22.000000       0.000000  \n",
              "75%         0.091208       0.078276      77.510000       0.000000  \n",
              "max        31.612198      33.847808   25691.160000       1.000000  \n",
              "\n",
              "[8 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eae2a7d0-8250-45fd-90e7-34fefeb7d24a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>...</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "      <td>283726.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>94811.077600</td>\n",
              "      <td>0.005917</td>\n",
              "      <td>-0.004135</td>\n",
              "      <td>0.001613</td>\n",
              "      <td>-0.002966</td>\n",
              "      <td>0.001828</td>\n",
              "      <td>-0.001139</td>\n",
              "      <td>0.001801</td>\n",
              "      <td>-0.000854</td>\n",
              "      <td>-0.001596</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000371</td>\n",
              "      <td>-0.000015</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>0.000214</td>\n",
              "      <td>-0.000232</td>\n",
              "      <td>0.000149</td>\n",
              "      <td>0.001763</td>\n",
              "      <td>0.000547</td>\n",
              "      <td>88.472687</td>\n",
              "      <td>0.001667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>47481.047891</td>\n",
              "      <td>1.948026</td>\n",
              "      <td>1.646703</td>\n",
              "      <td>1.508682</td>\n",
              "      <td>1.414184</td>\n",
              "      <td>1.377008</td>\n",
              "      <td>1.331931</td>\n",
              "      <td>1.227664</td>\n",
              "      <td>1.179054</td>\n",
              "      <td>1.095492</td>\n",
              "      <td>...</td>\n",
              "      <td>0.723909</td>\n",
              "      <td>0.724550</td>\n",
              "      <td>0.623702</td>\n",
              "      <td>0.605627</td>\n",
              "      <td>0.521220</td>\n",
              "      <td>0.482053</td>\n",
              "      <td>0.395744</td>\n",
              "      <td>0.328027</td>\n",
              "      <td>250.399437</td>\n",
              "      <td>0.040796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>-56.407510</td>\n",
              "      <td>-72.715728</td>\n",
              "      <td>-48.325589</td>\n",
              "      <td>-5.683171</td>\n",
              "      <td>-113.743307</td>\n",
              "      <td>-26.160506</td>\n",
              "      <td>-43.557242</td>\n",
              "      <td>-73.216718</td>\n",
              "      <td>-13.434066</td>\n",
              "      <td>...</td>\n",
              "      <td>-34.830382</td>\n",
              "      <td>-10.933144</td>\n",
              "      <td>-44.807735</td>\n",
              "      <td>-2.836627</td>\n",
              "      <td>-10.295397</td>\n",
              "      <td>-2.604551</td>\n",
              "      <td>-22.565679</td>\n",
              "      <td>-15.430084</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>54204.750000</td>\n",
              "      <td>-0.915951</td>\n",
              "      <td>-0.600321</td>\n",
              "      <td>-0.889682</td>\n",
              "      <td>-0.850134</td>\n",
              "      <td>-0.689830</td>\n",
              "      <td>-0.769031</td>\n",
              "      <td>-0.552509</td>\n",
              "      <td>-0.208828</td>\n",
              "      <td>-0.644221</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.228305</td>\n",
              "      <td>-0.542700</td>\n",
              "      <td>-0.161703</td>\n",
              "      <td>-0.354453</td>\n",
              "      <td>-0.317485</td>\n",
              "      <td>-0.326763</td>\n",
              "      <td>-0.070641</td>\n",
              "      <td>-0.052818</td>\n",
              "      <td>5.600000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>84692.500000</td>\n",
              "      <td>0.020384</td>\n",
              "      <td>0.063949</td>\n",
              "      <td>0.179963</td>\n",
              "      <td>-0.022248</td>\n",
              "      <td>-0.053468</td>\n",
              "      <td>-0.275168</td>\n",
              "      <td>0.040859</td>\n",
              "      <td>0.021898</td>\n",
              "      <td>-0.052596</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.029441</td>\n",
              "      <td>0.006675</td>\n",
              "      <td>-0.011159</td>\n",
              "      <td>0.041016</td>\n",
              "      <td>0.016278</td>\n",
              "      <td>-0.052172</td>\n",
              "      <td>0.001479</td>\n",
              "      <td>0.011288</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>139298.000000</td>\n",
              "      <td>1.316068</td>\n",
              "      <td>0.800283</td>\n",
              "      <td>1.026960</td>\n",
              "      <td>0.739647</td>\n",
              "      <td>0.612218</td>\n",
              "      <td>0.396792</td>\n",
              "      <td>0.570474</td>\n",
              "      <td>0.325704</td>\n",
              "      <td>0.595977</td>\n",
              "      <td>...</td>\n",
              "      <td>0.186194</td>\n",
              "      <td>0.528245</td>\n",
              "      <td>0.147748</td>\n",
              "      <td>0.439738</td>\n",
              "      <td>0.350667</td>\n",
              "      <td>0.240261</td>\n",
              "      <td>0.091208</td>\n",
              "      <td>0.078276</td>\n",
              "      <td>77.510000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>172792.000000</td>\n",
              "      <td>2.454930</td>\n",
              "      <td>22.057729</td>\n",
              "      <td>9.382558</td>\n",
              "      <td>16.875344</td>\n",
              "      <td>34.801666</td>\n",
              "      <td>73.301626</td>\n",
              "      <td>120.589494</td>\n",
              "      <td>20.007208</td>\n",
              "      <td>15.594995</td>\n",
              "      <td>...</td>\n",
              "      <td>27.202839</td>\n",
              "      <td>10.503090</td>\n",
              "      <td>22.528412</td>\n",
              "      <td>4.584549</td>\n",
              "      <td>7.519589</td>\n",
              "      <td>3.517346</td>\n",
              "      <td>31.612198</td>\n",
              "      <td>33.847808</td>\n",
              "      <td>25691.160000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eae2a7d0-8250-45fd-90e7-34fefeb7d24a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-eae2a7d0-8250-45fd-90e7-34fefeb7d24a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-eae2a7d0-8250-45fd-90e7-34fefeb7d24a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "splitting into dependent and independent variables"
      ],
      "metadata": {
        "id": "5lQbbZgbTGf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X=data.drop(columns=['Class','Time'])\n",
        "y=pd.DataFrame(data.Class, columns=['Class'])"
      ],
      "metadata": {
        "id": "6Vd48E_pTFs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "lN9C35uqTcjN",
        "outputId": "9d4717f0-8896-4d11-bd26-a1670d74511b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         V1        V2        V3        V4        V5        V6        V7  \\\n",
              "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
              "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
              "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
              "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
              "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
              "\n",
              "         V8        V9       V10  ...       V20       V21       V22       V23  \\\n",
              "0  0.098698  0.363787  0.090794  ...  0.251412 -0.018307  0.277838 -0.110474   \n",
              "1  0.085102 -0.255425 -0.166974  ... -0.069083 -0.225775 -0.638672  0.101288   \n",
              "2  0.247676 -1.514654  0.207643  ...  0.524980  0.247998  0.771679  0.909412   \n",
              "3  0.377436 -1.387024 -0.054952  ... -0.208038 -0.108300  0.005274 -0.190321   \n",
              "4 -0.270533  0.817739  0.753074  ...  0.408542 -0.009431  0.798278 -0.137458   \n",
              "\n",
              "        V24       V25       V26       V27       V28  Amount  \n",
              "0  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62  \n",
              "1 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69  \n",
              "2 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66  \n",
              "3 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50  \n",
              "4  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99  \n",
              "\n",
              "[5 rows x 29 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9a4fa74e-ebdc-4e02-8594-fef2e1558ddf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>...</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>0.090794</td>\n",
              "      <td>...</td>\n",
              "      <td>0.251412</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>-0.166974</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.069083</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>0.207643</td>\n",
              "      <td>...</td>\n",
              "      <td>0.524980</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>-0.054952</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.208038</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>0.753074</td>\n",
              "      <td>...</td>\n",
              "      <td>0.408542</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 29 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9a4fa74e-ebdc-4e02-8594-fef2e1558ddf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9a4fa74e-ebdc-4e02-8594-fef2e1558ddf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9a4fa74e-ebdc-4e02-8594-fef2e1558ddf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_t,X_test,y_t,y_test = train_test_split(X,y, random_state=0,test_size=0.2)\n",
        "X_train,X_val,y_train,y_val = train_test_split(X_t,y_t, random_state=0,test_size=0.2)"
      ],
      "metadata": {
        "id": "hPhLqzAISfzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "1TxJxPX5dsYu",
        "outputId": "c09b182a-9a03-4cd0-8579-f6ff1d021073"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              V1        V2        V3        V4        V5        V6        V7  \\\n",
              "208634  2.109692 -0.282658 -1.601397  0.157529  0.253428 -0.420871 -0.048509   \n",
              "39245   1.053748  0.013416  0.069223  0.626361 -0.335350 -1.013352  0.422449   \n",
              "47254  -0.989392  1.293318  1.115086 -0.717161  0.590306 -0.084739  0.713956   \n",
              "124587 -0.988540 -0.461739  1.980627  0.905616  0.283450 -0.722550 -0.297916   \n",
              "138934 -0.802157  1.247812  1.179001  1.261785 -0.222228  0.229187  0.170057   \n",
              "...          ...       ...       ...       ...       ...       ...       ...   \n",
              "185987  2.001189 -0.094733 -1.090200  0.570819 -0.432203 -1.822925  0.340360   \n",
              "223202  0.447240  1.334847 -3.029222  0.366486  3.793179  3.010099  0.698789   \n",
              "102753 -0.974941  0.689634  0.823912 -0.923401  1.579553 -1.074996  1.055522   \n",
              "49577   0.915184 -0.439310  0.985385  1.309120 -0.886112  0.363668 -0.572176   \n",
              "255798  0.080823  0.918607 -0.345321 -0.615058  0.847077 -0.611251  0.825226   \n",
              "\n",
              "              V8        V9       V10  ...       V20       V21       V22  \\\n",
              "208634 -0.050893  0.957046  0.136783  ... -0.314604 -0.368694 -1.102181   \n",
              "39245  -0.186552 -0.425044  0.099097  ...  0.045924 -0.266699 -1.056257   \n",
              "47254   0.084723 -0.478270 -0.299138  ...  0.004422 -0.112918 -0.418050   \n",
              "124587  0.083711  0.102661 -0.427991  ...  0.444744  0.307675  0.798343   \n",
              "138934  0.680090 -0.703300 -0.157482  ...  0.018610  0.070837  0.389977   \n",
              "...          ...       ...       ...  ...       ...       ...       ...   \n",
              "185987 -0.516663  0.382659  0.112765  ... -0.192362  0.404687  1.402467   \n",
              "223202  0.616111 -1.183914 -0.910841  ...  0.320015  0.084315  0.453962   \n",
              "102753 -0.094849 -0.588977 -1.303515  ... -0.111182 -0.015863 -0.115064   \n",
              "49577   0.353048  0.709260 -0.042226  ... -0.094166  0.214717  0.606470   \n",
              "255798  0.068572 -0.208043 -0.645341  ... -0.011671 -0.287221 -0.757737   \n",
              "\n",
              "             V23       V24       V25       V26       V27       V28  Amount  \n",
              "208634  0.307043 -0.037267 -0.266569  0.215793 -0.094362 -0.064597    8.98  \n",
              "39245   0.083267  0.510509  0.205813 -0.005070 -0.078369  0.017176   93.13  \n",
              "47254  -0.032361 -0.354185 -0.411338 -0.086474 -0.079769  0.190587    3.58  \n",
              "124587  0.128419  0.485237 -0.360081  0.499079  0.118564  0.186870   59.90  \n",
              "138934 -0.072526  0.240541 -0.130200 -0.250525  0.297960  0.133787   14.42  \n",
              "...          ...       ...       ...       ...       ...       ...     ...  \n",
              "185987 -0.014434  1.064466  0.234796  0.948717 -0.092840 -0.066925   17.90  \n",
              "223202 -0.231124  0.566220 -0.634618  1.042536  0.248615  0.270882    0.76  \n",
              "102753 -0.247861 -0.141180  0.127222  0.032042 -0.156552 -0.103905    0.76  \n",
              "49577  -0.151048  0.232794  0.444971 -0.220120  0.048760  0.029466   89.00  \n",
              "255798  0.115478  0.640878 -0.491124  0.088975  0.212796  0.074412    5.34  \n",
              "\n",
              "[181584 rows x 29 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2bf9c474-2c19-47f3-8394-3eeacc7c2cb0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>...</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>208634</th>\n",
              "      <td>2.109692</td>\n",
              "      <td>-0.282658</td>\n",
              "      <td>-1.601397</td>\n",
              "      <td>0.157529</td>\n",
              "      <td>0.253428</td>\n",
              "      <td>-0.420871</td>\n",
              "      <td>-0.048509</td>\n",
              "      <td>-0.050893</td>\n",
              "      <td>0.957046</td>\n",
              "      <td>0.136783</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.314604</td>\n",
              "      <td>-0.368694</td>\n",
              "      <td>-1.102181</td>\n",
              "      <td>0.307043</td>\n",
              "      <td>-0.037267</td>\n",
              "      <td>-0.266569</td>\n",
              "      <td>0.215793</td>\n",
              "      <td>-0.094362</td>\n",
              "      <td>-0.064597</td>\n",
              "      <td>8.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39245</th>\n",
              "      <td>1.053748</td>\n",
              "      <td>0.013416</td>\n",
              "      <td>0.069223</td>\n",
              "      <td>0.626361</td>\n",
              "      <td>-0.335350</td>\n",
              "      <td>-1.013352</td>\n",
              "      <td>0.422449</td>\n",
              "      <td>-0.186552</td>\n",
              "      <td>-0.425044</td>\n",
              "      <td>0.099097</td>\n",
              "      <td>...</td>\n",
              "      <td>0.045924</td>\n",
              "      <td>-0.266699</td>\n",
              "      <td>-1.056257</td>\n",
              "      <td>0.083267</td>\n",
              "      <td>0.510509</td>\n",
              "      <td>0.205813</td>\n",
              "      <td>-0.005070</td>\n",
              "      <td>-0.078369</td>\n",
              "      <td>0.017176</td>\n",
              "      <td>93.13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47254</th>\n",
              "      <td>-0.989392</td>\n",
              "      <td>1.293318</td>\n",
              "      <td>1.115086</td>\n",
              "      <td>-0.717161</td>\n",
              "      <td>0.590306</td>\n",
              "      <td>-0.084739</td>\n",
              "      <td>0.713956</td>\n",
              "      <td>0.084723</td>\n",
              "      <td>-0.478270</td>\n",
              "      <td>-0.299138</td>\n",
              "      <td>...</td>\n",
              "      <td>0.004422</td>\n",
              "      <td>-0.112918</td>\n",
              "      <td>-0.418050</td>\n",
              "      <td>-0.032361</td>\n",
              "      <td>-0.354185</td>\n",
              "      <td>-0.411338</td>\n",
              "      <td>-0.086474</td>\n",
              "      <td>-0.079769</td>\n",
              "      <td>0.190587</td>\n",
              "      <td>3.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124587</th>\n",
              "      <td>-0.988540</td>\n",
              "      <td>-0.461739</td>\n",
              "      <td>1.980627</td>\n",
              "      <td>0.905616</td>\n",
              "      <td>0.283450</td>\n",
              "      <td>-0.722550</td>\n",
              "      <td>-0.297916</td>\n",
              "      <td>0.083711</td>\n",
              "      <td>0.102661</td>\n",
              "      <td>-0.427991</td>\n",
              "      <td>...</td>\n",
              "      <td>0.444744</td>\n",
              "      <td>0.307675</td>\n",
              "      <td>0.798343</td>\n",
              "      <td>0.128419</td>\n",
              "      <td>0.485237</td>\n",
              "      <td>-0.360081</td>\n",
              "      <td>0.499079</td>\n",
              "      <td>0.118564</td>\n",
              "      <td>0.186870</td>\n",
              "      <td>59.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138934</th>\n",
              "      <td>-0.802157</td>\n",
              "      <td>1.247812</td>\n",
              "      <td>1.179001</td>\n",
              "      <td>1.261785</td>\n",
              "      <td>-0.222228</td>\n",
              "      <td>0.229187</td>\n",
              "      <td>0.170057</td>\n",
              "      <td>0.680090</td>\n",
              "      <td>-0.703300</td>\n",
              "      <td>-0.157482</td>\n",
              "      <td>...</td>\n",
              "      <td>0.018610</td>\n",
              "      <td>0.070837</td>\n",
              "      <td>0.389977</td>\n",
              "      <td>-0.072526</td>\n",
              "      <td>0.240541</td>\n",
              "      <td>-0.130200</td>\n",
              "      <td>-0.250525</td>\n",
              "      <td>0.297960</td>\n",
              "      <td>0.133787</td>\n",
              "      <td>14.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>185987</th>\n",
              "      <td>2.001189</td>\n",
              "      <td>-0.094733</td>\n",
              "      <td>-1.090200</td>\n",
              "      <td>0.570819</td>\n",
              "      <td>-0.432203</td>\n",
              "      <td>-1.822925</td>\n",
              "      <td>0.340360</td>\n",
              "      <td>-0.516663</td>\n",
              "      <td>0.382659</td>\n",
              "      <td>0.112765</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.192362</td>\n",
              "      <td>0.404687</td>\n",
              "      <td>1.402467</td>\n",
              "      <td>-0.014434</td>\n",
              "      <td>1.064466</td>\n",
              "      <td>0.234796</td>\n",
              "      <td>0.948717</td>\n",
              "      <td>-0.092840</td>\n",
              "      <td>-0.066925</td>\n",
              "      <td>17.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223202</th>\n",
              "      <td>0.447240</td>\n",
              "      <td>1.334847</td>\n",
              "      <td>-3.029222</td>\n",
              "      <td>0.366486</td>\n",
              "      <td>3.793179</td>\n",
              "      <td>3.010099</td>\n",
              "      <td>0.698789</td>\n",
              "      <td>0.616111</td>\n",
              "      <td>-1.183914</td>\n",
              "      <td>-0.910841</td>\n",
              "      <td>...</td>\n",
              "      <td>0.320015</td>\n",
              "      <td>0.084315</td>\n",
              "      <td>0.453962</td>\n",
              "      <td>-0.231124</td>\n",
              "      <td>0.566220</td>\n",
              "      <td>-0.634618</td>\n",
              "      <td>1.042536</td>\n",
              "      <td>0.248615</td>\n",
              "      <td>0.270882</td>\n",
              "      <td>0.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102753</th>\n",
              "      <td>-0.974941</td>\n",
              "      <td>0.689634</td>\n",
              "      <td>0.823912</td>\n",
              "      <td>-0.923401</td>\n",
              "      <td>1.579553</td>\n",
              "      <td>-1.074996</td>\n",
              "      <td>1.055522</td>\n",
              "      <td>-0.094849</td>\n",
              "      <td>-0.588977</td>\n",
              "      <td>-1.303515</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.111182</td>\n",
              "      <td>-0.015863</td>\n",
              "      <td>-0.115064</td>\n",
              "      <td>-0.247861</td>\n",
              "      <td>-0.141180</td>\n",
              "      <td>0.127222</td>\n",
              "      <td>0.032042</td>\n",
              "      <td>-0.156552</td>\n",
              "      <td>-0.103905</td>\n",
              "      <td>0.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49577</th>\n",
              "      <td>0.915184</td>\n",
              "      <td>-0.439310</td>\n",
              "      <td>0.985385</td>\n",
              "      <td>1.309120</td>\n",
              "      <td>-0.886112</td>\n",
              "      <td>0.363668</td>\n",
              "      <td>-0.572176</td>\n",
              "      <td>0.353048</td>\n",
              "      <td>0.709260</td>\n",
              "      <td>-0.042226</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.094166</td>\n",
              "      <td>0.214717</td>\n",
              "      <td>0.606470</td>\n",
              "      <td>-0.151048</td>\n",
              "      <td>0.232794</td>\n",
              "      <td>0.444971</td>\n",
              "      <td>-0.220120</td>\n",
              "      <td>0.048760</td>\n",
              "      <td>0.029466</td>\n",
              "      <td>89.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>255798</th>\n",
              "      <td>0.080823</td>\n",
              "      <td>0.918607</td>\n",
              "      <td>-0.345321</td>\n",
              "      <td>-0.615058</td>\n",
              "      <td>0.847077</td>\n",
              "      <td>-0.611251</td>\n",
              "      <td>0.825226</td>\n",
              "      <td>0.068572</td>\n",
              "      <td>-0.208043</td>\n",
              "      <td>-0.645341</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.011671</td>\n",
              "      <td>-0.287221</td>\n",
              "      <td>-0.757737</td>\n",
              "      <td>0.115478</td>\n",
              "      <td>0.640878</td>\n",
              "      <td>-0.491124</td>\n",
              "      <td>0.088975</td>\n",
              "      <td>0.212796</td>\n",
              "      <td>0.074412</td>\n",
              "      <td>5.34</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>181584 rows × 29 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2bf9c474-2c19-47f3-8394-3eeacc7c2cb0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2bf9c474-2c19-47f3-8394-3eeacc7c2cb0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2bf9c474-2c19-47f3-8394-3eeacc7c2cb0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Neural network without oversampling"
      ],
      "metadata": {
        "id": "Ry--TYTdUKX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler().fit(X_train)\n",
        "X_train_norm= scaler.transform(X_train)\n",
        "X_val_norm  = scaler.transform(X_val)\n",
        "X_test_norm = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "LGOB3hlbUApI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "METRICS = [ \n",
        "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      keras.metrics.Precision(name='precision'),\n",
        "      keras.metrics.Recall(name='recall'),\n",
        "      keras.metrics.AUC(name='auc'),\n",
        "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
        "]\n"
      ],
      "metadata": {
        "id": "NoXt47jCX6tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_results(y_true,y_pred):\n",
        "  print(classification_report(y_true,y_pred))\n",
        "  ConfusionMatrixDisplay.from_predictions(y_true,y_pred)"
      ],
      "metadata": {
        "id": "5hyzIiD18T9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_weighted_binary_crossentropy(zero_weight, one_weight):\n",
        "\n",
        "    def weighted_binary_crossentropy(y_true, y_pred):\n",
        "\n",
        "        b_ce = K.binary_crossentropy(from_logits=True)(y_pred,y_true)\n",
        "\n",
        "        # Apply the weights\n",
        "        print('error 3')\n",
        "        weight_vector = y_true * one_weight + (1. - y_true) * zero_weight\n",
        "        weighted_b_ce = weight_vector * b_ce\n",
        "        print('error 4')\n",
        "        # Return the mean error\n",
        "        return K.mean(weighted_b_ce)\n",
        "\n",
        "    return weighted_binary_crossentropy\n"
      ],
      "metadata": {
        "id": "ptfdykDHnz5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CNN"
      ],
      "metadata": {
        "id": "17cVlZ6Bi5ye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def create_cnn():\n",
        "  model = Sequential()\n",
        "  #model.add(Dense(32, activation='relu', input_dim=29))\n",
        "  model.add(Conv1D(128, 3, activation='relu', padding ='same', input_shape=(29,1)))\n",
        "  model.add(Dropout(0.2))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  model.compile(optimizer='Adam',loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),metrics=METRICS)\n",
        "  return model\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "m6qNw-_Ikpph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn=create_cnn()\n",
        "cnn.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ryr6LC8rxQg",
        "outputId": "06d87729-23f6-4a16-eff5-87df02944048"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_1 (Conv1D)           (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.fit(X_train_norm,y_train, batch_size=2048, epochs=50, verbose=1, validation_data=(X_val_norm,y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Dkkw7mWwp3x",
        "outputId": "61580973-d90f-4ec3-8d38-fdb7f7cd4714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "84/89 [===========================>..] - ETA: 0s - loss: 0.0964 - accuracy: 0.9929 - precision: 0.0309 - recall: 0.1057 - auc: 0.4212 - prc: 0.1015"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 14ms/step - loss: 0.0929 - accuracy: 0.9931 - precision: 0.0309 - recall: 0.1010 - auc: 0.4165 - prc: 0.0971 - val_loss: 0.0201 - val_accuracy: 0.9986 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.3264 - val_prc: 9.2589e-04\n",
            "Epoch 2/50\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0143 - accuracy: 0.9984 - precision: 0.7778 - recall: 0.1084 - auc: 0.6669 - prc: 0.3297 - val_loss: 0.0066 - val_accuracy: 0.9991 - val_precision: 0.8889 - val_recall: 0.3810 - val_auc: 0.8772 - val_prc: 0.6367\n",
            "Epoch 3/50\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0069 - accuracy: 0.9990 - precision: 0.8656 - recall: 0.4985 - auc: 0.8779 - prc: 0.6264 - val_loss: 0.0050 - val_accuracy: 0.9992 - val_precision: 0.8250 - val_recall: 0.5238 - val_auc: 0.8807 - val_prc: 0.6396\n",
            "Epoch 4/50\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0058 - accuracy: 0.9992 - precision: 0.8771 - recall: 0.6409 - auc: 0.8930 - prc: 0.6664 - val_loss: 0.0043 - val_accuracy: 0.9993 - val_precision: 0.8537 - val_recall: 0.5556 - val_auc: 0.8888 - val_prc: 0.6718\n",
            "Epoch 5/50\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0053 - accuracy: 0.9992 - precision: 0.8870 - recall: 0.6316 - auc: 0.8946 - prc: 0.6719 - val_loss: 0.0038 - val_accuracy: 0.9994 - val_precision: 0.8889 - val_recall: 0.6349 - val_auc: 0.8967 - val_prc: 0.6957\n",
            "Epoch 6/50\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0049 - accuracy: 0.9993 - precision: 0.8862 - recall: 0.6749 - auc: 0.8961 - prc: 0.6773 - val_loss: 0.0035 - val_accuracy: 0.9994 - val_precision: 0.9091 - val_recall: 0.6349 - val_auc: 0.8967 - val_prc: 0.7100\n",
            "Epoch 7/50\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0045 - accuracy: 0.9993 - precision: 0.8902 - recall: 0.6780 - auc: 0.9084 - prc: 0.7019 - val_loss: 0.0033 - val_accuracy: 0.9994 - val_precision: 0.8958 - val_recall: 0.6825 - val_auc: 0.9203 - val_prc: 0.7349\n",
            "Epoch 8/50\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0043 - accuracy: 0.9993 - precision: 0.8950 - recall: 0.6594 - auc: 0.9207 - prc: 0.7214 - val_loss: 0.0032 - val_accuracy: 0.9994 - val_precision: 0.8958 - val_recall: 0.6825 - val_auc: 0.9279 - val_prc: 0.7425\n",
            "Epoch 9/50\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0040 - accuracy: 0.9993 - precision: 0.8926 - recall: 0.6687 - auc: 0.9312 - prc: 0.7481 - val_loss: 0.0031 - val_accuracy: 0.9994 - val_precision: 0.9091 - val_recall: 0.6349 - val_auc: 0.9358 - val_prc: 0.7536\n",
            "Epoch 10/50\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0038 - accuracy: 0.9993 - precision: 0.8971 - recall: 0.6749 - auc: 0.9312 - prc: 0.7541 - val_loss: 0.0030 - val_accuracy: 0.9994 - val_precision: 0.9091 - val_recall: 0.6349 - val_auc: 0.9358 - val_prc: 0.7483\n",
            "Epoch 11/50\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0037 - accuracy: 0.9993 - precision: 0.8984 - recall: 0.6842 - auc: 0.9341 - prc: 0.7698 - val_loss: 0.0029 - val_accuracy: 0.9994 - val_precision: 0.9111 - val_recall: 0.6508 - val_auc: 0.9357 - val_prc: 0.7507\n",
            "Epoch 12/50\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0036 - accuracy: 0.9993 - precision: 0.8984 - recall: 0.6842 - auc: 0.9326 - prc: 0.7779 - val_loss: 0.0030 - val_accuracy: 0.9994 - val_precision: 0.8600 - val_recall: 0.6825 - val_auc: 0.9356 - val_prc: 0.7437\n",
            "Epoch 13/50\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0035 - accuracy: 0.9993 - precision: 0.8939 - recall: 0.6780 - auc: 0.9343 - prc: 0.7816 - val_loss: 0.0029 - val_accuracy: 0.9994 - val_precision: 0.8913 - val_recall: 0.6508 - val_auc: 0.9357 - val_prc: 0.7768\n",
            "Epoch 14/50\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0034 - accuracy: 0.9993 - precision: 0.9032 - recall: 0.6935 - auc: 0.9357 - prc: 0.7825 - val_loss: 0.0028 - val_accuracy: 0.9994 - val_precision: 0.9091 - val_recall: 0.6349 - val_auc: 0.9435 - val_prc: 0.7844\n",
            "Epoch 15/50\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0034 - accuracy: 0.9993 - precision: 0.8988 - recall: 0.6873 - auc: 0.9357 - prc: 0.7837 - val_loss: 0.0028 - val_accuracy: 0.9994 - val_precision: 0.9048 - val_recall: 0.6032 - val_auc: 0.9359 - val_prc: 0.7864\n",
            "Epoch 16/50\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0033 - accuracy: 0.9993 - precision: 0.8898 - recall: 0.6749 - auc: 0.9374 - prc: 0.8000 - val_loss: 0.0029 - val_accuracy: 0.9994 - val_precision: 0.8431 - val_recall: 0.6825 - val_auc: 0.9512 - val_prc: 0.7773\n",
            "Epoch 17/50\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0032 - accuracy: 0.9993 - precision: 0.9008 - recall: 0.7028 - auc: 0.9358 - prc: 0.8062 - val_loss: 0.0028 - val_accuracy: 0.9994 - val_precision: 0.8542 - val_recall: 0.6508 - val_auc: 0.9436 - val_prc: 0.7862\n",
            "Epoch 18/50\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0032 - accuracy: 0.9993 - precision: 0.9012 - recall: 0.7059 - auc: 0.9389 - prc: 0.8061 - val_loss: 0.0028 - val_accuracy: 0.9994 - val_precision: 0.8542 - val_recall: 0.6508 - val_auc: 0.9515 - val_prc: 0.7876\n",
            "Epoch 19/50\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0031 - accuracy: 0.9993 - precision: 0.9044 - recall: 0.7028 - auc: 0.9373 - prc: 0.8110 - val_loss: 0.0029 - val_accuracy: 0.9994 - val_precision: 0.8462 - val_recall: 0.6984 - val_auc: 0.9358 - val_prc: 0.7693\n",
            "Epoch 20/50\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0030 - accuracy: 0.9994 - precision: 0.9160 - recall: 0.7090 - auc: 0.9375 - prc: 0.8195 - val_loss: 0.0029 - val_accuracy: 0.9994 - val_precision: 0.8364 - val_recall: 0.7302 - val_auc: 0.9513 - val_prc: 0.7806\n",
            "Epoch 21/50\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0030 - accuracy: 0.9994 - precision: 0.8996 - recall: 0.7214 - auc: 0.9374 - prc: 0.8145 - val_loss: 0.0027 - val_accuracy: 0.9993 - val_precision: 0.8367 - val_recall: 0.6508 - val_auc: 0.9437 - val_prc: 0.7916\n",
            "Epoch 22/50\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0030 - accuracy: 0.9994 - precision: 0.9062 - recall: 0.7183 - auc: 0.9404 - prc: 0.8223 - val_loss: 0.0027 - val_accuracy: 0.9994 - val_precision: 0.8491 - val_recall: 0.7143 - val_auc: 0.9437 - val_prc: 0.7872\n",
            "Epoch 23/50\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0030 - accuracy: 0.9994 - precision: 0.9119 - recall: 0.7368 - auc: 0.9405 - prc: 0.8223 - val_loss: 0.0027 - val_accuracy: 0.9993 - val_precision: 0.8810 - val_recall: 0.5873 - val_auc: 0.9360 - val_prc: 0.7885\n",
            "Epoch 24/50\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0029 - accuracy: 0.9994 - precision: 0.9228 - recall: 0.7399 - auc: 0.9390 - prc: 0.8268 - val_loss: 0.0027 - val_accuracy: 0.9994 - val_precision: 0.8333 - val_recall: 0.7143 - val_auc: 0.9436 - val_prc: 0.7881\n",
            "Epoch 25/50\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0029 - accuracy: 0.9994 - precision: 0.9094 - recall: 0.7461 - auc: 0.9405 - prc: 0.8269 - val_loss: 0.0027 - val_accuracy: 0.9994 - val_precision: 0.8491 - val_recall: 0.7143 - val_auc: 0.9438 - val_prc: 0.7855\n",
            "Epoch 26/50\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0029 - accuracy: 0.9994 - precision: 0.9091 - recall: 0.7430 - auc: 0.9406 - prc: 0.8279 - val_loss: 0.0027 - val_accuracy: 0.9994 - val_precision: 0.8182 - val_recall: 0.7143 - val_auc: 0.9515 - val_prc: 0.7907\n",
            "Epoch 27/50\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0029 - accuracy: 0.9994 - precision: 0.8897 - recall: 0.7245 - auc: 0.9390 - prc: 0.8264 - val_loss: 0.0026 - val_accuracy: 0.9994 - val_precision: 0.8431 - val_recall: 0.6825 - val_auc: 0.9515 - val_prc: 0.8013\n",
            "Epoch 28/50\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0028 - accuracy: 0.9994 - precision: 0.9231 - recall: 0.7430 - auc: 0.9451 - prc: 0.8359 - val_loss: 0.0027 - val_accuracy: 0.9994 - val_precision: 0.8333 - val_recall: 0.7143 - val_auc: 0.9437 - val_prc: 0.7900\n",
            "Epoch 29/50\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0028 - accuracy: 0.9994 - precision: 0.9122 - recall: 0.7399 - auc: 0.9405 - prc: 0.8365 - val_loss: 0.0027 - val_accuracy: 0.9994 - val_precision: 0.8214 - val_recall: 0.7302 - val_auc: 0.9515 - val_prc: 0.7961\n",
            "Epoch 30/50\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0028 - accuracy: 0.9994 - precision: 0.9160 - recall: 0.7430 - auc: 0.9389 - prc: 0.8343 - val_loss: 0.0027 - val_accuracy: 0.9994 - val_precision: 0.8491 - val_recall: 0.7143 - val_auc: 0.9439 - val_prc: 0.7890\n",
            "Epoch 31/50\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0028 - accuracy: 0.9994 - precision: 0.9234 - recall: 0.7461 - auc: 0.9405 - prc: 0.8370 - val_loss: 0.0026 - val_accuracy: 0.9994 - val_precision: 0.8776 - val_recall: 0.6825 - val_auc: 0.9438 - val_prc: 0.7965\n",
            "Epoch 32/50\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 0.0028 - accuracy: 0.9995 - precision: 0.9288 - recall: 0.7678 - auc: 0.9405 - prc: 0.8350 - val_loss: 0.0026 - val_accuracy: 0.9994 - val_precision: 0.8936 - val_recall: 0.6667 - val_auc: 0.9439 - val_prc: 0.7993\n",
            "Epoch 33/50\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0028 - accuracy: 0.9994 - precision: 0.9198 - recall: 0.7461 - auc: 0.9421 - prc: 0.8370 - val_loss: 0.0029 - val_accuracy: 0.9994 - val_precision: 0.8103 - val_recall: 0.7460 - val_auc: 0.9515 - val_prc: 0.7902\n",
            "Epoch 34/50\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0027 - accuracy: 0.9995 - precision: 0.9286 - recall: 0.7647 - auc: 0.9405 - prc: 0.8404 - val_loss: 0.0028 - val_accuracy: 0.9994 - val_precision: 0.8333 - val_recall: 0.7143 - val_auc: 0.9593 - val_prc: 0.7904\n",
            "Epoch 35/50\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0028 - accuracy: 0.9995 - precision: 0.9278 - recall: 0.7554 - auc: 0.9421 - prc: 0.8408 - val_loss: 0.0028 - val_accuracy: 0.9994 - val_precision: 0.8182 - val_recall: 0.7143 - val_auc: 0.9437 - val_prc: 0.7838\n",
            "Epoch 36/50\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0027 - accuracy: 0.9995 - precision: 0.9389 - recall: 0.7616 - auc: 0.9406 - prc: 0.8410 - val_loss: 0.0028 - val_accuracy: 0.9994 - val_precision: 0.8070 - val_recall: 0.7302 - val_auc: 0.9514 - val_prc: 0.7932\n",
            "Epoch 37/50\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0027 - accuracy: 0.9995 - precision: 0.9216 - recall: 0.7647 - auc: 0.9421 - prc: 0.8421 - val_loss: 0.0026 - val_accuracy: 0.9994 - val_precision: 0.8654 - val_recall: 0.7143 - val_auc: 0.9516 - val_prc: 0.8029\n",
            "Epoch 38/50\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 0.0028 - accuracy: 0.9994 - precision: 0.9071 - recall: 0.7554 - auc: 0.9421 - prc: 0.8379 - val_loss: 0.0031 - val_accuracy: 0.9994 - val_precision: 0.8103 - val_recall: 0.7460 - val_auc: 0.9590 - val_prc: 0.7470\n",
            "Epoch 39/50\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0027 - accuracy: 0.9995 - precision: 0.9283 - recall: 0.7616 - auc: 0.9421 - prc: 0.8447 - val_loss: 0.0027 - val_accuracy: 0.9994 - val_precision: 0.8364 - val_recall: 0.7302 - val_auc: 0.9517 - val_prc: 0.8020\n",
            "Epoch 40/50\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 0.0027 - accuracy: 0.9995 - precision: 0.9328 - recall: 0.7740 - auc: 0.9406 - prc: 0.8445 - val_loss: 0.0030 - val_accuracy: 0.9994 - val_precision: 0.8167 - val_recall: 0.7778 - val_auc: 0.9515 - val_prc: 0.7480\n",
            "Epoch 41/50\n",
            "89/89 [==============================] - 2s 18ms/step - loss: 0.0027 - accuracy: 0.9995 - precision: 0.9318 - recall: 0.7616 - auc: 0.9452 - prc: 0.8462 - val_loss: 0.0028 - val_accuracy: 0.9994 - val_precision: 0.8070 - val_recall: 0.7302 - val_auc: 0.9438 - val_prc: 0.7877\n",
            "Epoch 42/50\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 0.0027 - accuracy: 0.9995 - precision: 0.9185 - recall: 0.7678 - auc: 0.9406 - prc: 0.8437 - val_loss: 0.0029 - val_accuracy: 0.9994 - val_precision: 0.8136 - val_recall: 0.7619 - val_auc: 0.9516 - val_prc: 0.7942\n",
            "Epoch 43/50\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0026 - accuracy: 0.9995 - precision: 0.9267 - recall: 0.7833 - auc: 0.9422 - prc: 0.8487 - val_loss: 0.0028 - val_accuracy: 0.9994 - val_precision: 0.8364 - val_recall: 0.7302 - val_auc: 0.9514 - val_prc: 0.7915\n",
            "Epoch 44/50\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0026 - accuracy: 0.9995 - precision: 0.9286 - recall: 0.7647 - auc: 0.9467 - prc: 0.8484 - val_loss: 0.0028 - val_accuracy: 0.9994 - val_precision: 0.8654 - val_recall: 0.7143 - val_auc: 0.9515 - val_prc: 0.7916\n",
            "Epoch 45/50\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0026 - accuracy: 0.9995 - precision: 0.9338 - recall: 0.7864 - auc: 0.9453 - prc: 0.8489 - val_loss: 0.0030 - val_accuracy: 0.9994 - val_precision: 0.8136 - val_recall: 0.7619 - val_auc: 0.9515 - val_prc: 0.7430\n",
            "Epoch 46/50\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0026 - accuracy: 0.9995 - precision: 0.9294 - recall: 0.7740 - auc: 0.9467 - prc: 0.8512 - val_loss: 0.0027 - val_accuracy: 0.9994 - val_precision: 0.8519 - val_recall: 0.7302 - val_auc: 0.9515 - val_prc: 0.8012\n",
            "Epoch 47/50\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0026 - accuracy: 0.9995 - precision: 0.9436 - recall: 0.7771 - auc: 0.9452 - prc: 0.8499 - val_loss: 0.0026 - val_accuracy: 0.9995 - val_precision: 0.8679 - val_recall: 0.7302 - val_auc: 0.9517 - val_prc: 0.8049\n",
            "Epoch 48/50\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0026 - accuracy: 0.9995 - precision: 0.9356 - recall: 0.7647 - auc: 0.9483 - prc: 0.8483 - val_loss: 0.0027 - val_accuracy: 0.9995 - val_precision: 0.8679 - val_recall: 0.7302 - val_auc: 0.9515 - val_prc: 0.8006\n",
            "Epoch 49/50\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0026 - accuracy: 0.9995 - precision: 0.9375 - recall: 0.7895 - auc: 0.9452 - prc: 0.8505 - val_loss: 0.0026 - val_accuracy: 0.9994 - val_precision: 0.8654 - val_recall: 0.7143 - val_auc: 0.9438 - val_prc: 0.7982\n",
            "Epoch 50/50\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0026 - accuracy: 0.9995 - precision: 0.9216 - recall: 0.7647 - auc: 0.9437 - prc: 0.8455 - val_loss: 0.0029 - val_accuracy: 0.9994 - val_precision: 0.8136 - val_recall: 0.7619 - val_auc: 0.9437 - val_prc: 0.7886\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4c481c72b0>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_test_preds= cnn.predict(X_test_norm)\n",
        "cnn_val_preds = cnn.predict(X_val_norm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVlFsOsp4_Jf",
        "outputId": "7ac4fc57-632b-4e25-d937-0fdcdfb0069b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1774/1774 [==============================] - 3s 2ms/step\n",
            "1419/1419 [==============================] - 3s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_results(y_val, cnn_val_preds>0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "id": "Wmm4W_IjgOEU",
        "outputId": "4dde49fc-5d7c-4ca1-ddb3-7adb10cb4f43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.81      0.76      0.79        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.91      0.88      0.89     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results with Adam lr=0.001"
      ],
      "metadata": {
        "id": "ZlM0pQ4ZUfa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_results(y_val, cnn_val_preds>0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "id": "MNbeIedIUU-7",
        "outputId": "142e04e8-4609-4f1b-90af-3bbd8c72dcad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.90      0.60      0.72        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.95      0.80      0.86     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_results(y_test, cnn_test_preds>0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "id": "C5Vv3y93UZSd",
        "outputId": "0b4c18bd-8324-4723-bbf2-2de52eb64411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     56659\n",
            "           1       0.87      0.60      0.71        87\n",
            "\n",
            "    accuracy                           1.00     56746\n",
            "   macro avg       0.93      0.80      0.85     56746\n",
            "weighted avg       1.00      1.00      1.00     56746\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results with RMSPROP"
      ],
      "metadata": {
        "id": "nkL2LuMjURDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_results(y_val, cnn_val_preds>0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "id": "-eJRhYEU5Bbl",
        "outputId": "2031e16c-9512-438c-f345-679d28cc685a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.82      0.78      0.80        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.91      0.89      0.90     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_results(y_test, cnn_test_preds>0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "id": "qNt4poCsQ-Ct",
        "outputId": "bb5b18db-e044-4bbc-88fa-429b71e49e9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     56659\n",
            "           1       0.80      0.77      0.78        87\n",
            "\n",
            "    accuracy                           1.00     56746\n",
            "   macro avg       0.90      0.88      0.89     56746\n",
            "weighted avg       1.00      1.00      1.00     56746\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ANN model"
      ],
      "metadata": {
        "id": "vLA0sTSIjleG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dnn():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(32, activation='relu', input_dim=29))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=METRICS)\n",
        "  return model                              "
      ],
      "metadata": {
        "id": "fax-PYn0r0Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dnn_model=create_dnn()\n",
        "dnn_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGyKW4x87pA9",
        "outputId": "9d76e35d-f36a-42d7-a42e-955bd809fa7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_17 (Dense)            (None, 32)                960       \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 993\n",
            "Trainable params: 993\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dnn_model.fit(X_train_norm,y_train, batch_size=2048, epochs=15, verbose =1, validation_data=(X_val_norm,y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5rm94uq7zXM",
        "outputId": "ecaa0b6d-cde4-4499-a36b-c3e3e3feec87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "89/89 [==============================] - 2s 11ms/step - loss: 0.2749 - accuracy: 0.7755 - precision: 0.0051 - recall: 0.6762 - auc: 0.6822 - prc: 0.0030 - val_loss: 0.1115 - val_accuracy: 0.9990 - val_precision: 0.8696 - val_recall: 0.3175 - val_auc: 0.8416 - val_prc: 0.5069\n",
            "Epoch 2/15\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 0.0573 - accuracy: 0.9989 - precision: 0.8606 - recall: 0.4396 - auc: 0.8138 - prc: 0.5233 - val_loss: 0.0228 - val_accuracy: 0.9991 - val_precision: 0.8846 - val_recall: 0.3651 - val_auc: 0.8718 - val_prc: 0.6095\n",
            "Epoch 3/15\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 0.0134 - accuracy: 0.9991 - precision: 0.8905 - recall: 0.5789 - auc: 0.8716 - prc: 0.6695 - val_loss: 0.0067 - val_accuracy: 0.9993 - val_precision: 0.8222 - val_recall: 0.5873 - val_auc: 0.9052 - val_prc: 0.6686\n",
            "Epoch 4/15\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 0.0058 - accuracy: 0.9993 - precision: 0.8980 - recall: 0.6811 - auc: 0.9055 - prc: 0.7230 - val_loss: 0.0040 - val_accuracy: 0.9994 - val_precision: 0.8400 - val_recall: 0.6667 - val_auc: 0.9175 - val_prc: 0.7104\n",
            "Epoch 5/15\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 0.0043 - accuracy: 0.9993 - precision: 0.9040 - recall: 0.6997 - auc: 0.9160 - prc: 0.7590 - val_loss: 0.0033 - val_accuracy: 0.9994 - val_precision: 0.8750 - val_recall: 0.6667 - val_auc: 0.9192 - val_prc: 0.7655\n",
            "Epoch 6/15\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 0.0037 - accuracy: 0.9994 - precision: 0.9116 - recall: 0.7028 - auc: 0.9274 - prc: 0.7839 - val_loss: 0.0030 - val_accuracy: 0.9994 - val_precision: 0.8776 - val_recall: 0.6825 - val_auc: 0.9273 - val_prc: 0.7744\n",
            "Epoch 7/15\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 0.0035 - accuracy: 0.9994 - precision: 0.9147 - recall: 0.7307 - auc: 0.9324 - prc: 0.8010 - val_loss: 0.0030 - val_accuracy: 0.9994 - val_precision: 0.8269 - val_recall: 0.6825 - val_auc: 0.9274 - val_prc: 0.7757\n",
            "Epoch 8/15\n",
            "89/89 [==============================] - 0s 6ms/step - loss: 0.0033 - accuracy: 0.9994 - precision: 0.9080 - recall: 0.7337 - auc: 0.9356 - prc: 0.8111 - val_loss: 0.0028 - val_accuracy: 0.9994 - val_precision: 0.8776 - val_recall: 0.6825 - val_auc: 0.9355 - val_prc: 0.7893\n",
            "Epoch 9/15\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 0.0032 - accuracy: 0.9994 - precision: 0.9195 - recall: 0.7430 - auc: 0.9388 - prc: 0.8180 - val_loss: 0.0028 - val_accuracy: 0.9994 - val_precision: 0.8776 - val_recall: 0.6825 - val_auc: 0.9356 - val_prc: 0.7888\n",
            "Epoch 10/15\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 0.0031 - accuracy: 0.9994 - precision: 0.9305 - recall: 0.7461 - auc: 0.9389 - prc: 0.8232 - val_loss: 0.0029 - val_accuracy: 0.9994 - val_precision: 0.8431 - val_recall: 0.6825 - val_auc: 0.9355 - val_prc: 0.7874\n",
            "Epoch 11/15\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 0.0030 - accuracy: 0.9994 - precision: 0.9228 - recall: 0.7399 - auc: 0.9420 - prc: 0.8286 - val_loss: 0.0029 - val_accuracy: 0.9993 - val_precision: 0.8113 - val_recall: 0.6825 - val_auc: 0.9355 - val_prc: 0.7870\n",
            "Epoch 12/15\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 0.0030 - accuracy: 0.9995 - precision: 0.9316 - recall: 0.7585 - auc: 0.9420 - prc: 0.8325 - val_loss: 0.0029 - val_accuracy: 0.9993 - val_precision: 0.8113 - val_recall: 0.6825 - val_auc: 0.9356 - val_prc: 0.7845\n",
            "Epoch 13/15\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 0.0029 - accuracy: 0.9995 - precision: 0.9257 - recall: 0.7709 - auc: 0.9420 - prc: 0.8347 - val_loss: 0.0028 - val_accuracy: 0.9994 - val_precision: 0.9130 - val_recall: 0.6667 - val_auc: 0.9278 - val_prc: 0.7905\n",
            "Epoch 14/15\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 0.0029 - accuracy: 0.9995 - precision: 0.9265 - recall: 0.7802 - auc: 0.9422 - prc: 0.8377 - val_loss: 0.0028 - val_accuracy: 0.9994 - val_precision: 0.8269 - val_recall: 0.6825 - val_auc: 0.9356 - val_prc: 0.7862\n",
            "Epoch 15/15\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 0.0028 - accuracy: 0.9995 - precision: 0.9366 - recall: 0.7771 - auc: 0.9452 - prc: 0.8427 - val_loss: 0.0028 - val_accuracy: 0.9994 - val_precision: 0.8269 - val_recall: 0.6825 - val_auc: 0.9278 - val_prc: 0.7870\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6d433b40a0>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dense_test_preds= dnn_model.predict(X_test_norm)\n",
        "dense_val_preds = dnn_model.predict(X_val_norm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyfVVgvByjS3",
        "outputId": "f0979d71-2558-40f8-dd68-f4b5ec24b9c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1774/1774 [==============================] - 3s 2ms/step\n",
            "1419/1419 [==============================] - 2s 1ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_results(y_val, dense_val_preds>0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "id": "TsHBx0cPz-7f",
        "outputId": "a62dc736-124e-4b95-afed-43dd75895411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.85      0.70      0.77        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.92      0.85      0.88     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_results(y_test,dense_test_preds>0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "id": "jdQ6Q-Jd3Cew",
        "outputId": "d7b070f1-1f13-4022-a265-7ac542a6881d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     56659\n",
            "           1       0.81      0.68      0.74        87\n",
            "\n",
            "    accuracy                           1.00     56746\n",
            "   macro avg       0.90      0.84      0.87     56746\n",
            "weighted avg       1.00      1.00      1.00     56746\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Weighted loss function"
      ],
      "metadata": {
        "id": "RyqnDcywR-YA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#custom loss cnn"
      ],
      "metadata": {
        "id": "YvZ2iUXNjy81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def create_cnn_cust():\n",
        "  model = Sequential()\n",
        "  #model.add(Dense(32, activation='relu', input_dim=29))\n",
        "  model.add(Conv1D(128, 3, activation='relu', padding ='same', input_shape=(29,1)))\n",
        "  model.add(Dropout(0.2))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  model.compile(optimizer='rmsprop',loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),metrics=METRICS)\n",
        "  return model\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uCoqRo_EARfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_cust=create_cnn_cust()\n",
        "cnn_cust.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqzahSjHVEsM",
        "outputId": "7885ab5b-43c6-424c-e1ba-2cbb1aad07f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_4 (Conv1D)           (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w=0.02\n",
        "validation_preds=[]\n",
        "while(w<=0.2): \n",
        "  print(\"******************************for w = \",w)\n",
        "  cnn_cust=create_cnn_cust()\n",
        "  cnn_cust.summary()\n",
        "  cnn_cust.fit(X_train_norm,y_train, batch_size=2048,epochs=20, verbose=1, validation_data=(X_val_norm,y_val), class_weight={0: w, 1: 1-w})\n",
        "  cnn_cust_val_preds = cnn_cust.predict(X_val_norm)>0.5\n",
        "  #cnn_cust_test_preds= cnn_cust.predict(X_test_norm)>0.5\n",
        "  validation_preds.append(cnn_cust_val_preds)\n",
        "  w+=0.01\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBT8soSWVOWt",
        "outputId": "aca55a20-bd4e-4004-e8d8-50fdf38fed58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "******************************for w =  0.02\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_16 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_16 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89/89 [==============================] - ETA: 0s - loss: 0.0034 - accuracy: 0.9970 - precision: 0.3289 - recall: 0.7124 - auc: 0.9179 - prc: 0.6270"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 14ms/step - loss: 0.0034 - accuracy: 0.9970 - precision: 0.3289 - recall: 0.7124 - auc: 0.9179 - prc: 0.6270 - val_loss: 0.0201 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9623 - val_prc: 0.7098\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0014 - accuracy: 0.9993 - precision: 0.7882 - recall: 0.8297 - auc: 0.9563 - prc: 0.7634 - val_loss: 0.0218 - val_accuracy: 0.9988 - val_precision: 0.5361 - val_recall: 0.8254 - val_auc: 0.9697 - val_prc: 0.7169\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0012 - accuracy: 0.9989 - precision: 0.6570 - recall: 0.8421 - auc: 0.9779 - prc: 0.7615 - val_loss: 0.0139 - val_accuracy: 0.9989 - val_precision: 0.5667 - val_recall: 0.8095 - val_auc: 0.9763 - val_prc: 0.7527\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0012 - accuracy: 0.9986 - precision: 0.5774 - recall: 0.8545 - auc: 0.9819 - prc: 0.7597 - val_loss: 0.0192 - val_accuracy: 0.9983 - val_precision: 0.4407 - val_recall: 0.8254 - val_auc: 0.9752 - val_prc: 0.7456\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 0.9986 - precision: 0.5662 - recall: 0.8607 - auc: 0.9826 - prc: 0.7687 - val_loss: 0.0212 - val_accuracy: 0.9977 - val_precision: 0.3630 - val_recall: 0.8413 - val_auc: 0.9756 - val_prc: 0.7439\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0011 - accuracy: 0.9983 - precision: 0.5082 - recall: 0.8607 - auc: 0.9821 - prc: 0.7617 - val_loss: 0.0246 - val_accuracy: 0.9968 - val_precision: 0.2834 - val_recall: 0.8413 - val_auc: 0.9765 - val_prc: 0.7407\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0011 - accuracy: 0.9985 - precision: 0.5437 - recall: 0.8669 - auc: 0.9845 - prc: 0.7690 - val_loss: 0.0103 - val_accuracy: 0.9989 - val_precision: 0.5667 - val_recall: 0.8095 - val_auc: 0.9737 - val_prc: 0.7546\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 0.9984 - precision: 0.5324 - recall: 0.8638 - auc: 0.9844 - prc: 0.7705 - val_loss: 0.0235 - val_accuracy: 0.9974 - val_precision: 0.3272 - val_recall: 0.8413 - val_auc: 0.9768 - val_prc: 0.6855\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0011 - accuracy: 0.9984 - precision: 0.5374 - recall: 0.8669 - auc: 0.9867 - prc: 0.7678 - val_loss: 0.0237 - val_accuracy: 0.9970 - val_precision: 0.2994 - val_recall: 0.8413 - val_auc: 0.9766 - val_prc: 0.6977\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0011 - accuracy: 0.9984 - precision: 0.5281 - recall: 0.8731 - auc: 0.9865 - prc: 0.7725 - val_loss: 0.0176 - val_accuracy: 0.9979 - val_precision: 0.3841 - val_recall: 0.8413 - val_auc: 0.9783 - val_prc: 0.7428\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0010 - accuracy: 0.9980 - precision: 0.4604 - recall: 0.8638 - auc: 0.9876 - prc: 0.7659 - val_loss: 0.0193 - val_accuracy: 0.9977 - val_precision: 0.3630 - val_recall: 0.8413 - val_auc: 0.9778 - val_prc: 0.7206\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0010 - accuracy: 0.9984 - precision: 0.5312 - recall: 0.8700 - auc: 0.9872 - prc: 0.7756 - val_loss: 0.0198 - val_accuracy: 0.9976 - val_precision: 0.3442 - val_recall: 0.8413 - val_auc: 0.9776 - val_prc: 0.7319\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0010 - accuracy: 0.9985 - precision: 0.5404 - recall: 0.8700 - auc: 0.9872 - prc: 0.7766 - val_loss: 0.0160 - val_accuracy: 0.9980 - val_precision: 0.3985 - val_recall: 0.8413 - val_auc: 0.9732 - val_prc: 0.7431\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0010 - accuracy: 0.9983 - precision: 0.5184 - recall: 0.8731 - auc: 0.9864 - prc: 0.7702 - val_loss: 0.0236 - val_accuracy: 0.9965 - val_precision: 0.2650 - val_recall: 0.8413 - val_auc: 0.9717 - val_prc: 0.6978\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.8191e-04 - accuracy: 0.9982 - precision: 0.5045 - recall: 0.8731 - auc: 0.9877 - prc: 0.7823 - val_loss: 0.0194 - val_accuracy: 0.9971 - val_precision: 0.3029 - val_recall: 0.8413 - val_auc: 0.9721 - val_prc: 0.7394\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 9.9401e-04 - accuracy: 0.9980 - precision: 0.4676 - recall: 0.8700 - auc: 0.9891 - prc: 0.7758 - val_loss: 0.0132 - val_accuracy: 0.9983 - val_precision: 0.4370 - val_recall: 0.8254 - val_auc: 0.9749 - val_prc: 0.7560\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 9.7687e-04 - accuracy: 0.9981 - precision: 0.4772 - recall: 0.8762 - auc: 0.9862 - prc: 0.7818 - val_loss: 0.0191 - val_accuracy: 0.9977 - val_precision: 0.3630 - val_recall: 0.8413 - val_auc: 0.9727 - val_prc: 0.7122\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 9.6484e-04 - accuracy: 0.9983 - precision: 0.5173 - recall: 0.8793 - auc: 0.9876 - prc: 0.7801 - val_loss: 0.0232 - val_accuracy: 0.9961 - val_precision: 0.2455 - val_recall: 0.8571 - val_auc: 0.9727 - val_prc: 0.6991\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 9.3402e-04 - accuracy: 0.9980 - precision: 0.4719 - recall: 0.8824 - auc: 0.9868 - prc: 0.7805 - val_loss: 0.0129 - val_accuracy: 0.9984 - val_precision: 0.4595 - val_recall: 0.8095 - val_auc: 0.9717 - val_prc: 0.7648\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 9.2059e-04 - accuracy: 0.9983 - precision: 0.5155 - recall: 0.8731 - auc: 0.9885 - prc: 0.7811 - val_loss: 0.0126 - val_accuracy: 0.9984 - val_precision: 0.4554 - val_recall: 0.8095 - val_auc: 0.9739 - val_prc: 0.7552\n",
            "1419/1419 [==============================] - 2s 2ms/step\n",
            "******************************for w =  0.03\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_17 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_17 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_17 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "84/89 [===========================>..] - ETA: 0s - loss: 0.0052 - accuracy: 0.9916 - precision: 0.1212 - recall: 0.6393 - auc: 0.8887 - prc: 0.5487"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 14ms/step - loss: 0.0051 - accuracy: 0.9919 - precision: 0.1280 - recall: 0.6451 - auc: 0.8894 - prc: 0.5561 - val_loss: 0.0177 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9569 - val_prc: 0.6904\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8080 - recall: 0.8080 - auc: 0.9522 - prc: 0.7363 - val_loss: 0.0209 - val_accuracy: 0.9991 - val_precision: 0.6375 - val_recall: 0.8095 - val_auc: 0.9643 - val_prc: 0.7166\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0014 - accuracy: 0.9992 - precision: 0.7389 - recall: 0.8235 - auc: 0.9713 - prc: 0.7585 - val_loss: 0.0105 - val_accuracy: 0.9992 - val_precision: 0.6800 - val_recall: 0.8095 - val_auc: 0.9711 - val_prc: 0.7340\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0013 - accuracy: 0.9989 - precision: 0.6485 - recall: 0.8452 - auc: 0.9768 - prc: 0.7634 - val_loss: 0.0096 - val_accuracy: 0.9991 - val_precision: 0.6220 - val_recall: 0.8095 - val_auc: 0.9732 - val_prc: 0.7658\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0013 - accuracy: 0.9990 - precision: 0.6675 - recall: 0.8390 - auc: 0.9839 - prc: 0.7615 - val_loss: 0.0145 - val_accuracy: 0.9985 - val_precision: 0.4771 - val_recall: 0.8254 - val_auc: 0.9777 - val_prc: 0.7669\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0012 - accuracy: 0.9989 - precision: 0.6328 - recall: 0.8483 - auc: 0.9824 - prc: 0.7686 - val_loss: 0.0150 - val_accuracy: 0.9983 - val_precision: 0.4435 - val_recall: 0.8095 - val_auc: 0.9782 - val_prc: 0.7413\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0012 - accuracy: 0.9989 - precision: 0.6370 - recall: 0.8421 - auc: 0.9836 - prc: 0.7681 - val_loss: 0.0215 - val_accuracy: 0.9973 - val_precision: 0.3174 - val_recall: 0.8413 - val_auc: 0.9755 - val_prc: 0.7236\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0012 - accuracy: 0.9987 - precision: 0.5851 - recall: 0.8514 - auc: 0.9817 - prc: 0.7637 - val_loss: 0.0082 - val_accuracy: 0.9991 - val_precision: 0.6220 - val_recall: 0.8095 - val_auc: 0.9754 - val_prc: 0.7502\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0012 - accuracy: 0.9987 - precision: 0.5948 - recall: 0.8545 - auc: 0.9850 - prc: 0.7660 - val_loss: 0.0136 - val_accuracy: 0.9984 - val_precision: 0.4554 - val_recall: 0.8095 - val_auc: 0.9732 - val_prc: 0.7639\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0012 - accuracy: 0.9987 - precision: 0.5983 - recall: 0.8669 - auc: 0.9856 - prc: 0.7739 - val_loss: 0.0121 - val_accuracy: 0.9986 - val_precision: 0.4904 - val_recall: 0.8095 - val_auc: 0.9730 - val_prc: 0.7664\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0012 - accuracy: 0.9987 - precision: 0.5885 - recall: 0.8545 - auc: 0.9847 - prc: 0.7717 - val_loss: 0.0104 - val_accuracy: 0.9988 - val_precision: 0.5368 - val_recall: 0.8095 - val_auc: 0.9747 - val_prc: 0.7342\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0011 - accuracy: 0.9988 - precision: 0.6239 - recall: 0.8576 - auc: 0.9856 - prc: 0.7724 - val_loss: 0.0130 - val_accuracy: 0.9984 - val_precision: 0.4636 - val_recall: 0.8095 - val_auc: 0.9741 - val_prc: 0.7308\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0012 - accuracy: 0.9986 - precision: 0.5667 - recall: 0.8545 - auc: 0.9842 - prc: 0.7704 - val_loss: 0.0100 - val_accuracy: 0.9987 - val_precision: 0.5100 - val_recall: 0.8095 - val_auc: 0.9746 - val_prc: 0.7524\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0011 - accuracy: 0.9987 - precision: 0.5858 - recall: 0.8669 - auc: 0.9843 - prc: 0.7745 - val_loss: 0.0101 - val_accuracy: 0.9987 - val_precision: 0.5312 - val_recall: 0.8095 - val_auc: 0.9740 - val_prc: 0.7689\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0011 - accuracy: 0.9987 - precision: 0.6035 - recall: 0.8576 - auc: 0.9880 - prc: 0.7763 - val_loss: 0.0146 - val_accuracy: 0.9981 - val_precision: 0.4077 - val_recall: 0.8413 - val_auc: 0.9730 - val_prc: 0.7539\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 0.9986 - precision: 0.5776 - recall: 0.8762 - auc: 0.9856 - prc: 0.7724 - val_loss: 0.0093 - val_accuracy: 0.9988 - val_precision: 0.5426 - val_recall: 0.8095 - val_auc: 0.9752 - val_prc: 0.7340\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 0.9987 - precision: 0.5912 - recall: 0.8731 - auc: 0.9873 - prc: 0.7795 - val_loss: 0.0096 - val_accuracy: 0.9986 - val_precision: 0.5050 - val_recall: 0.8095 - val_auc: 0.9752 - val_prc: 0.7664\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 0.9987 - precision: 0.6043 - recall: 0.8607 - auc: 0.9869 - prc: 0.7729 - val_loss: 0.0147 - val_accuracy: 0.9983 - val_precision: 0.4417 - val_recall: 0.8413 - val_auc: 0.9731 - val_prc: 0.7132\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 0.9987 - precision: 0.5891 - recall: 0.8700 - auc: 0.9881 - prc: 0.7816 - val_loss: 0.0208 - val_accuracy: 0.9969 - val_precision: 0.2865 - val_recall: 0.8413 - val_auc: 0.9717 - val_prc: 0.7073\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0011 - accuracy: 0.9986 - precision: 0.5685 - recall: 0.8607 - auc: 0.9881 - prc: 0.7711 - val_loss: 0.0121 - val_accuracy: 0.9983 - val_precision: 0.4370 - val_recall: 0.8254 - val_auc: 0.9741 - val_prc: 0.7635\n",
            "1419/1419 [==============================] - 2s 2ms/step\n",
            "******************************for w =  0.04\n",
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_18 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_18 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_18 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89/89 [==============================] - ETA: 0s - loss: 0.0062 - accuracy: 0.9910 - precision: 0.1090 - recall: 0.5984 - auc: 0.8650 - prc: 0.5130"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 15ms/step - loss: 0.0062 - accuracy: 0.9910 - precision: 0.1090 - recall: 0.5984 - auc: 0.8650 - prc: 0.5130 - val_loss: 0.0160 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9536 - val_prc: 0.6772\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0019 - accuracy: 0.9994 - precision: 0.8371 - recall: 0.7957 - auc: 0.9470 - prc: 0.7396 - val_loss: 0.0102 - val_accuracy: 0.9994 - val_precision: 0.7500 - val_recall: 0.8095 - val_auc: 0.9694 - val_prc: 0.7158\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7811 - recall: 0.8173 - auc: 0.9656 - prc: 0.7615 - val_loss: 0.0111 - val_accuracy: 0.9992 - val_precision: 0.6711 - val_recall: 0.8095 - val_auc: 0.9775 - val_prc: 0.7227\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9991 - precision: 0.7120 - recall: 0.8266 - auc: 0.9754 - prc: 0.7669 - val_loss: 0.0095 - val_accuracy: 0.9991 - val_precision: 0.6538 - val_recall: 0.8095 - val_auc: 0.9730 - val_prc: 0.7492\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0014 - accuracy: 0.9991 - precision: 0.7098 - recall: 0.8328 - auc: 0.9781 - prc: 0.7667 - val_loss: 0.0119 - val_accuracy: 0.9989 - val_precision: 0.5604 - val_recall: 0.8095 - val_auc: 0.9788 - val_prc: 0.7544\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0013 - accuracy: 0.9990 - precision: 0.6897 - recall: 0.8328 - auc: 0.9822 - prc: 0.7707 - val_loss: 0.0108 - val_accuracy: 0.9988 - val_precision: 0.5543 - val_recall: 0.8095 - val_auc: 0.9730 - val_prc: 0.7460\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0013 - accuracy: 0.9990 - precision: 0.6650 - recall: 0.8421 - auc: 0.9815 - prc: 0.7683 - val_loss: 0.0103 - val_accuracy: 0.9988 - val_precision: 0.5368 - val_recall: 0.8095 - val_auc: 0.9748 - val_prc: 0.7469\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0013 - accuracy: 0.9989 - precision: 0.6499 - recall: 0.8390 - auc: 0.9829 - prc: 0.7650 - val_loss: 0.0105 - val_accuracy: 0.9989 - val_precision: 0.5604 - val_recall: 0.8095 - val_auc: 0.9745 - val_prc: 0.7216\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0013 - accuracy: 0.9990 - precision: 0.6749 - recall: 0.8421 - auc: 0.9836 - prc: 0.7694 - val_loss: 0.0086 - val_accuracy: 0.9989 - val_precision: 0.5667 - val_recall: 0.8095 - val_auc: 0.9755 - val_prc: 0.7605\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0013 - accuracy: 0.9988 - precision: 0.6264 - recall: 0.8514 - auc: 0.9838 - prc: 0.7693 - val_loss: 0.0068 - val_accuracy: 0.9991 - val_precision: 0.6533 - val_recall: 0.7778 - val_auc: 0.9753 - val_prc: 0.7556\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0013 - accuracy: 0.9988 - precision: 0.6227 - recall: 0.8483 - auc: 0.9828 - prc: 0.7715 - val_loss: 0.0077 - val_accuracy: 0.9990 - val_precision: 0.6071 - val_recall: 0.8095 - val_auc: 0.9760 - val_prc: 0.7497\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0012 - accuracy: 0.9990 - precision: 0.6667 - recall: 0.8483 - auc: 0.9842 - prc: 0.7740 - val_loss: 0.0127 - val_accuracy: 0.9985 - val_precision: 0.4679 - val_recall: 0.8095 - val_auc: 0.9744 - val_prc: 0.7182\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0012 - accuracy: 0.9990 - precision: 0.6731 - recall: 0.8607 - auc: 0.9851 - prc: 0.7788 - val_loss: 0.0103 - val_accuracy: 0.9986 - val_precision: 0.5000 - val_recall: 0.8095 - val_auc: 0.9761 - val_prc: 0.7482\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0012 - accuracy: 0.9989 - precision: 0.6389 - recall: 0.8545 - auc: 0.9860 - prc: 0.7729 - val_loss: 0.0083 - val_accuracy: 0.9988 - val_precision: 0.5484 - val_recall: 0.8095 - val_auc: 0.9764 - val_prc: 0.7631\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0012 - accuracy: 0.9988 - precision: 0.6267 - recall: 0.8576 - auc: 0.9849 - prc: 0.7762 - val_loss: 0.0048 - val_accuracy: 0.9994 - val_precision: 0.7778 - val_recall: 0.7778 - val_auc: 0.9783 - val_prc: 0.7617\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0013 - accuracy: 0.9988 - precision: 0.6079 - recall: 0.8545 - auc: 0.9796 - prc: 0.7619 - val_loss: 0.0092 - val_accuracy: 0.9987 - val_precision: 0.5204 - val_recall: 0.8095 - val_auc: 0.9762 - val_prc: 0.7503\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0012 - accuracy: 0.9989 - precision: 0.6412 - recall: 0.8576 - auc: 0.9847 - prc: 0.7779 - val_loss: 0.0091 - val_accuracy: 0.9986 - val_precision: 0.4904 - val_recall: 0.8095 - val_auc: 0.9765 - val_prc: 0.7631\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0012 - accuracy: 0.9989 - precision: 0.6449 - recall: 0.8545 - auc: 0.9867 - prc: 0.7713 - val_loss: 0.0086 - val_accuracy: 0.9986 - val_precision: 0.5051 - val_recall: 0.7937 - val_auc: 0.9758 - val_prc: 0.7456\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0012 - accuracy: 0.9989 - precision: 0.6324 - recall: 0.8576 - auc: 0.9869 - prc: 0.7732 - val_loss: 0.0089 - val_accuracy: 0.9989 - val_precision: 0.5604 - val_recall: 0.8095 - val_auc: 0.9760 - val_prc: 0.7348\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0012 - accuracy: 0.9989 - precision: 0.6526 - recall: 0.8607 - auc: 0.9865 - prc: 0.7804 - val_loss: 0.0099 - val_accuracy: 0.9985 - val_precision: 0.4690 - val_recall: 0.8413 - val_auc: 0.9760 - val_prc: 0.7430\n",
            "1419/1419 [==============================] - 2s 2ms/step\n",
            "******************************for w =  0.05\n",
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_19 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_19 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_19 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86/89 [===========================>..] - ETA: 0s - loss: 0.0068 - accuracy: 0.9915 - precision: 0.1196 - recall: 0.6263 - auc: 0.8916 - prc: 0.5137"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 4s 18ms/step - loss: 0.0066 - accuracy: 0.9916 - precision: 0.1217 - recall: 0.6295 - auc: 0.8937 - prc: 0.5155 - val_loss: 0.0137 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9588 - val_prc: 0.6828\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0020 - accuracy: 0.9994 - precision: 0.8339 - recall: 0.7926 - auc: 0.9442 - prc: 0.7400 - val_loss: 0.0126 - val_accuracy: 0.9993 - val_precision: 0.6986 - val_recall: 0.8095 - val_auc: 0.9657 - val_prc: 0.7096\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.7853 - recall: 0.8266 - auc: 0.9618 - prc: 0.7569 - val_loss: 0.0093 - val_accuracy: 0.9992 - val_precision: 0.6892 - val_recall: 0.8095 - val_auc: 0.9732 - val_prc: 0.7490\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7479 - recall: 0.8266 - auc: 0.9744 - prc: 0.7605 - val_loss: 0.0106 - val_accuracy: 0.9990 - val_precision: 0.6071 - val_recall: 0.8095 - val_auc: 0.9784 - val_prc: 0.7398\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0015 - accuracy: 0.9991 - precision: 0.7120 - recall: 0.8266 - auc: 0.9798 - prc: 0.7637 - val_loss: 0.0075 - val_accuracy: 0.9991 - val_precision: 0.6375 - val_recall: 0.8095 - val_auc: 0.9751 - val_prc: 0.7631\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0014 - accuracy: 0.9991 - precision: 0.7200 - recall: 0.8359 - auc: 0.9784 - prc: 0.7625 - val_loss: 0.0118 - val_accuracy: 0.9986 - val_precision: 0.5050 - val_recall: 0.8095 - val_auc: 0.9735 - val_prc: 0.7152\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0014 - accuracy: 0.9991 - precision: 0.6972 - recall: 0.8483 - auc: 0.9818 - prc: 0.7625 - val_loss: 0.0084 - val_accuracy: 0.9991 - val_precision: 0.6538 - val_recall: 0.8095 - val_auc: 0.9739 - val_prc: 0.7196\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0014 - accuracy: 0.9991 - precision: 0.6941 - recall: 0.8359 - auc: 0.9822 - prc: 0.7645 - val_loss: 0.0079 - val_accuracy: 0.9990 - val_precision: 0.6000 - val_recall: 0.8095 - val_auc: 0.9758 - val_prc: 0.7449\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0014 - accuracy: 0.9990 - precision: 0.6880 - recall: 0.8328 - auc: 0.9815 - prc: 0.7673 - val_loss: 0.0104 - val_accuracy: 0.9986 - val_precision: 0.5000 - val_recall: 0.8095 - val_auc: 0.9740 - val_prc: 0.7456\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0014 - accuracy: 0.9989 - precision: 0.6506 - recall: 0.8359 - auc: 0.9823 - prc: 0.7695 - val_loss: 0.0078 - val_accuracy: 0.9991 - val_precision: 0.6456 - val_recall: 0.8095 - val_auc: 0.9757 - val_prc: 0.7206\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0013 - accuracy: 0.9991 - precision: 0.6947 - recall: 0.8452 - auc: 0.9818 - prc: 0.7725 - val_loss: 0.0073 - val_accuracy: 0.9992 - val_precision: 0.6667 - val_recall: 0.7937 - val_auc: 0.9753 - val_prc: 0.7528\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0014 - accuracy: 0.9990 - precision: 0.6843 - recall: 0.8390 - auc: 0.9820 - prc: 0.7573 - val_loss: 0.0127 - val_accuracy: 0.9983 - val_precision: 0.4474 - val_recall: 0.8095 - val_auc: 0.9749 - val_prc: 0.7188\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0013 - accuracy: 0.9990 - precision: 0.6749 - recall: 0.8421 - auc: 0.9811 - prc: 0.7694 - val_loss: 0.0087 - val_accuracy: 0.9988 - val_precision: 0.5495 - val_recall: 0.7937 - val_auc: 0.9749 - val_prc: 0.7572\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0013 - accuracy: 0.9990 - precision: 0.6757 - recall: 0.8452 - auc: 0.9792 - prc: 0.7751 - val_loss: 0.0114 - val_accuracy: 0.9985 - val_precision: 0.4722 - val_recall: 0.8095 - val_auc: 0.9742 - val_prc: 0.7432\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0013 - accuracy: 0.9990 - precision: 0.6832 - recall: 0.8545 - auc: 0.9856 - prc: 0.7728 - val_loss: 0.0067 - val_accuracy: 0.9992 - val_precision: 0.6892 - val_recall: 0.8095 - val_auc: 0.9774 - val_prc: 0.7225\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0013 - accuracy: 0.9991 - precision: 0.7069 - recall: 0.8514 - auc: 0.9827 - prc: 0.7738 - val_loss: 0.0074 - val_accuracy: 0.9989 - val_precision: 0.5882 - val_recall: 0.7937 - val_auc: 0.9752 - val_prc: 0.7551\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0013 - accuracy: 0.9991 - precision: 0.6917 - recall: 0.8545 - auc: 0.9847 - prc: 0.7690 - val_loss: 0.0074 - val_accuracy: 0.9989 - val_precision: 0.5862 - val_recall: 0.8095 - val_auc: 0.9764 - val_prc: 0.7617\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0013 - accuracy: 0.9990 - precision: 0.6833 - recall: 0.8483 - auc: 0.9851 - prc: 0.7730 - val_loss: 0.0117 - val_accuracy: 0.9984 - val_precision: 0.4513 - val_recall: 0.8095 - val_auc: 0.9747 - val_prc: 0.7493\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0013 - accuracy: 0.9989 - precision: 0.6541 - recall: 0.8607 - auc: 0.9860 - prc: 0.7768 - val_loss: 0.0090 - val_accuracy: 0.9987 - val_precision: 0.5204 - val_recall: 0.8095 - val_auc: 0.9757 - val_prc: 0.7588\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0013 - accuracy: 0.9989 - precision: 0.6546 - recall: 0.8390 - auc: 0.9874 - prc: 0.7650 - val_loss: 0.0116 - val_accuracy: 0.9985 - val_precision: 0.4766 - val_recall: 0.8095 - val_auc: 0.9751 - val_prc: 0.7097\n",
            "1419/1419 [==============================] - 2s 1ms/step\n",
            "******************************for w =  0.060000000000000005\n",
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_20 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_20 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_20 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85/89 [===========================>..] - ETA: 0s - loss: 0.0088 - accuracy: 0.9872 - precision: 0.0722 - recall: 0.5568 - auc: 0.8494 - prc: 0.4332"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 17ms/step - loss: 0.0085 - accuracy: 0.9876 - precision: 0.0761 - recall: 0.5648 - auc: 0.8498 - prc: 0.4442 - val_loss: 0.0134 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9389 - val_prc: 0.6627\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0021 - accuracy: 0.9994 - precision: 0.8433 - recall: 0.7833 - auc: 0.9414 - prc: 0.7292 - val_loss: 0.0098 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9617 - val_prc: 0.7170\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0018 - accuracy: 0.9993 - precision: 0.8006 - recall: 0.8080 - auc: 0.9626 - prc: 0.7487 - val_loss: 0.0077 - val_accuracy: 0.9993 - val_precision: 0.7286 - val_recall: 0.8095 - val_auc: 0.9681 - val_prc: 0.7475\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7739 - recall: 0.8266 - auc: 0.9747 - prc: 0.7610 - val_loss: 0.0078 - val_accuracy: 0.9992 - val_precision: 0.6892 - val_recall: 0.8095 - val_auc: 0.9676 - val_prc: 0.7633\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7666 - recall: 0.8235 - auc: 0.9789 - prc: 0.7661 - val_loss: 0.0089 - val_accuracy: 0.9990 - val_precision: 0.6000 - val_recall: 0.8095 - val_auc: 0.9737 - val_prc: 0.7624\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9991 - precision: 0.7147 - recall: 0.8297 - auc: 0.9786 - prc: 0.7692 - val_loss: 0.0089 - val_accuracy: 0.9990 - val_precision: 0.6145 - val_recall: 0.8095 - val_auc: 0.9742 - val_prc: 0.7483\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0014 - accuracy: 0.9992 - precision: 0.7330 - recall: 0.8328 - auc: 0.9767 - prc: 0.7679 - val_loss: 0.0136 - val_accuracy: 0.9984 - val_precision: 0.4513 - val_recall: 0.8095 - val_auc: 0.9722 - val_prc: 0.7486\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0015 - accuracy: 0.9991 - precision: 0.7219 - recall: 0.8359 - auc: 0.9763 - prc: 0.7685 - val_loss: 0.0081 - val_accuracy: 0.9990 - val_precision: 0.6000 - val_recall: 0.8095 - val_auc: 0.9750 - val_prc: 0.7602\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0014 - accuracy: 0.9991 - precision: 0.7116 - recall: 0.8328 - auc: 0.9795 - prc: 0.7692 - val_loss: 0.0082 - val_accuracy: 0.9989 - val_precision: 0.5730 - val_recall: 0.8095 - val_auc: 0.9750 - val_prc: 0.7429\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0014 - accuracy: 0.9991 - precision: 0.7071 - recall: 0.8297 - auc: 0.9810 - prc: 0.7643 - val_loss: 0.0074 - val_accuracy: 0.9991 - val_precision: 0.6220 - val_recall: 0.8095 - val_auc: 0.9766 - val_prc: 0.7482\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0014 - accuracy: 0.9991 - precision: 0.7139 - recall: 0.8421 - auc: 0.9799 - prc: 0.7754 - val_loss: 0.0113 - val_accuracy: 0.9986 - val_precision: 0.5050 - val_recall: 0.8095 - val_auc: 0.9745 - val_prc: 0.7232\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0014 - accuracy: 0.9991 - precision: 0.7109 - recall: 0.8452 - auc: 0.9819 - prc: 0.7731 - val_loss: 0.0098 - val_accuracy: 0.9987 - val_precision: 0.5100 - val_recall: 0.8095 - val_auc: 0.9752 - val_prc: 0.7522\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0014 - accuracy: 0.9992 - precision: 0.7253 - recall: 0.8421 - auc: 0.9839 - prc: 0.7727 - val_loss: 0.0084 - val_accuracy: 0.9989 - val_precision: 0.5795 - val_recall: 0.8095 - val_auc: 0.9757 - val_prc: 0.7219\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0014 - accuracy: 0.9991 - precision: 0.7169 - recall: 0.8390 - auc: 0.9805 - prc: 0.7749 - val_loss: 0.0069 - val_accuracy: 0.9990 - val_precision: 0.6098 - val_recall: 0.7937 - val_auc: 0.9762 - val_prc: 0.7266\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0014 - accuracy: 0.9991 - precision: 0.6985 - recall: 0.8390 - auc: 0.9821 - prc: 0.7692 - val_loss: 0.0097 - val_accuracy: 0.9986 - val_precision: 0.5050 - val_recall: 0.8095 - val_auc: 0.9752 - val_prc: 0.7430\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0014 - accuracy: 0.9991 - precision: 0.7065 - recall: 0.8421 - auc: 0.9852 - prc: 0.7687 - val_loss: 0.0063 - val_accuracy: 0.9992 - val_precision: 0.6849 - val_recall: 0.7937 - val_auc: 0.9755 - val_prc: 0.7410\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0013 - accuracy: 0.9991 - precision: 0.7162 - recall: 0.8359 - auc: 0.9853 - prc: 0.7725 - val_loss: 0.0070 - val_accuracy: 0.9990 - val_precision: 0.6098 - val_recall: 0.7937 - val_auc: 0.9756 - val_prc: 0.7540\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0013 - accuracy: 0.9991 - precision: 0.7008 - recall: 0.8483 - auc: 0.9808 - prc: 0.7763 - val_loss: 0.0071 - val_accuracy: 0.9987 - val_precision: 0.5319 - val_recall: 0.7937 - val_auc: 0.9762 - val_prc: 0.7437\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0013 - accuracy: 0.9990 - precision: 0.6816 - recall: 0.8483 - auc: 0.9820 - prc: 0.7750 - val_loss: 0.0073 - val_accuracy: 0.9989 - val_precision: 0.5882 - val_recall: 0.7937 - val_auc: 0.9767 - val_prc: 0.7396\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0013 - accuracy: 0.9991 - precision: 0.6911 - recall: 0.8452 - auc: 0.9830 - prc: 0.7745 - val_loss: 0.0082 - val_accuracy: 0.9987 - val_precision: 0.5204 - val_recall: 0.8095 - val_auc: 0.9764 - val_prc: 0.7262\n",
            "1419/1419 [==============================] - 2s 2ms/step\n",
            "******************************for w =  0.07\n",
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_21 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_21 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_21 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88/89 [============================>.] - ETA: 0s - loss: 0.0094 - accuracy: 0.9954 - precision: 0.1915 - recall: 0.5288 - auc: 0.8176 - prc: 0.4419"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 14ms/step - loss: 0.0094 - accuracy: 0.9954 - precision: 0.1928 - recall: 0.5285 - auc: 0.8168 - prc: 0.4399 - val_loss: 0.0116 - val_accuracy: 0.9994 - val_precision: 0.8000 - val_recall: 0.7619 - val_auc: 0.9454 - val_prc: 0.6641\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0023 - accuracy: 0.9994 - precision: 0.8508 - recall: 0.7771 - auc: 0.9361 - prc: 0.7286 - val_loss: 0.0100 - val_accuracy: 0.9994 - val_precision: 0.7812 - val_recall: 0.7937 - val_auc: 0.9695 - val_prc: 0.7210\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0019 - accuracy: 0.9993 - precision: 0.8062 - recall: 0.7988 - auc: 0.9576 - prc: 0.7578 - val_loss: 0.0096 - val_accuracy: 0.9992 - val_precision: 0.6892 - val_recall: 0.8095 - val_auc: 0.9662 - val_prc: 0.7254\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.7958 - recall: 0.8204 - auc: 0.9633 - prc: 0.7619 - val_loss: 0.0062 - val_accuracy: 0.9993 - val_precision: 0.7183 - val_recall: 0.8095 - val_auc: 0.9691 - val_prc: 0.7614\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7794 - recall: 0.8204 - auc: 0.9759 - prc: 0.7663 - val_loss: 0.0086 - val_accuracy: 0.9991 - val_precision: 0.6538 - val_recall: 0.8095 - val_auc: 0.9739 - val_prc: 0.7477\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0016 - accuracy: 0.9992 - precision: 0.7564 - recall: 0.8266 - auc: 0.9781 - prc: 0.7658 - val_loss: 0.0067 - val_accuracy: 0.9993 - val_precision: 0.6986 - val_recall: 0.8095 - val_auc: 0.9748 - val_prc: 0.7624\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0015 - accuracy: 0.9993 - precision: 0.7778 - recall: 0.8235 - auc: 0.9741 - prc: 0.7685 - val_loss: 0.0090 - val_accuracy: 0.9990 - val_precision: 0.5930 - val_recall: 0.8095 - val_auc: 0.9738 - val_prc: 0.7452\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7350 - recall: 0.8328 - auc: 0.9801 - prc: 0.7693 - val_loss: 0.0060 - val_accuracy: 0.9993 - val_precision: 0.7042 - val_recall: 0.7937 - val_auc: 0.9773 - val_prc: 0.7484\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7528 - recall: 0.8297 - auc: 0.9776 - prc: 0.7641 - val_loss: 0.0062 - val_accuracy: 0.9993 - val_precision: 0.7143 - val_recall: 0.7937 - val_auc: 0.9760 - val_prc: 0.7586\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7370 - recall: 0.8328 - auc: 0.9771 - prc: 0.7698 - val_loss: 0.0059 - val_accuracy: 0.9993 - val_precision: 0.7246 - val_recall: 0.7937 - val_auc: 0.9764 - val_prc: 0.7537\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7410 - recall: 0.8328 - auc: 0.9764 - prc: 0.7648 - val_loss: 0.0077 - val_accuracy: 0.9989 - val_precision: 0.5862 - val_recall: 0.8095 - val_auc: 0.9757 - val_prc: 0.7198\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0014 - accuracy: 0.9992 - precision: 0.7377 - recall: 0.8359 - auc: 0.9792 - prc: 0.7747 - val_loss: 0.0081 - val_accuracy: 0.9989 - val_precision: 0.5747 - val_recall: 0.7937 - val_auc: 0.9751 - val_prc: 0.7374\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0015 - accuracy: 0.9991 - precision: 0.7112 - recall: 0.8235 - auc: 0.9766 - prc: 0.7572 - val_loss: 0.0051 - val_accuracy: 0.9994 - val_precision: 0.7656 - val_recall: 0.7778 - val_auc: 0.9631 - val_prc: 0.7546\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9991 - precision: 0.7162 - recall: 0.8359 - auc: 0.9808 - prc: 0.7638 - val_loss: 0.0065 - val_accuracy: 0.9992 - val_precision: 0.6757 - val_recall: 0.7937 - val_auc: 0.9761 - val_prc: 0.7519\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0014 - accuracy: 0.9992 - precision: 0.7570 - recall: 0.8390 - auc: 0.9794 - prc: 0.7750 - val_loss: 0.0050 - val_accuracy: 0.9994 - val_precision: 0.7778 - val_recall: 0.7778 - val_auc: 0.9702 - val_prc: 0.7563\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0014 - accuracy: 0.9991 - precision: 0.7150 - recall: 0.8390 - auc: 0.9783 - prc: 0.7690 - val_loss: 0.0050 - val_accuracy: 0.9994 - val_precision: 0.7903 - val_recall: 0.7778 - val_auc: 0.9776 - val_prc: 0.7584\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0014 - accuracy: 0.9992 - precision: 0.7391 - recall: 0.8421 - auc: 0.9769 - prc: 0.7761 - val_loss: 0.0052 - val_accuracy: 0.9994 - val_precision: 0.7778 - val_recall: 0.7778 - val_auc: 0.9772 - val_prc: 0.7584\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0014 - accuracy: 0.9991 - precision: 0.6921 - recall: 0.8421 - auc: 0.9817 - prc: 0.7690 - val_loss: 0.0053 - val_accuracy: 0.9993 - val_precision: 0.7246 - val_recall: 0.7937 - val_auc: 0.9699 - val_prc: 0.7606\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0014 - accuracy: 0.9992 - precision: 0.7370 - recall: 0.8328 - auc: 0.9794 - prc: 0.7784 - val_loss: 0.0058 - val_accuracy: 0.9993 - val_precision: 0.7042 - val_recall: 0.7937 - val_auc: 0.9769 - val_prc: 0.7537\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0014 - accuracy: 0.9992 - precision: 0.7344 - recall: 0.8390 - auc: 0.9828 - prc: 0.7699 - val_loss: 0.0056 - val_accuracy: 0.9993 - val_precision: 0.7353 - val_recall: 0.7937 - val_auc: 0.9698 - val_prc: 0.7497\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "******************************for w =  0.08\n",
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_22 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_22 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_22 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "83/89 [==========================>...] - ETA: 0s - loss: 0.0116 - accuracy: 0.9900 - precision: 0.0816 - recall: 0.4690 - auc: 0.7852 - prc: 0.4025"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 14ms/step - loss: 0.0110 - accuracy: 0.9905 - precision: 0.0867 - recall: 0.4819 - auc: 0.7936 - prc: 0.4118 - val_loss: 0.0115 - val_accuracy: 0.9994 - val_precision: 0.7833 - val_recall: 0.7460 - val_auc: 0.9319 - val_prc: 0.6393\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0024 - accuracy: 0.9994 - precision: 0.8630 - recall: 0.7802 - auc: 0.9285 - prc: 0.7122 - val_loss: 0.0091 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9627 - val_prc: 0.7078\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0020 - accuracy: 0.9993 - precision: 0.8269 - recall: 0.7988 - auc: 0.9551 - prc: 0.7480 - val_loss: 0.0076 - val_accuracy: 0.9993 - val_precision: 0.7391 - val_recall: 0.8095 - val_auc: 0.9689 - val_prc: 0.7174\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0018 - accuracy: 0.9993 - precision: 0.8062 - recall: 0.8111 - auc: 0.9594 - prc: 0.7590 - val_loss: 0.0067 - val_accuracy: 0.9993 - val_precision: 0.7083 - val_recall: 0.8095 - val_auc: 0.9695 - val_prc: 0.7436\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.7922 - recall: 0.8142 - auc: 0.9706 - prc: 0.7619 - val_loss: 0.0071 - val_accuracy: 0.9992 - val_precision: 0.6892 - val_recall: 0.8095 - val_auc: 0.9739 - val_prc: 0.7592\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7771 - recall: 0.8204 - auc: 0.9764 - prc: 0.7613 - val_loss: 0.0077 - val_accuracy: 0.9992 - val_precision: 0.6711 - val_recall: 0.8095 - val_auc: 0.9738 - val_prc: 0.7460\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0016 - accuracy: 0.9992 - precision: 0.7666 - recall: 0.8235 - auc: 0.9744 - prc: 0.7681 - val_loss: 0.0069 - val_accuracy: 0.9992 - val_precision: 0.6711 - val_recall: 0.8095 - val_auc: 0.9752 - val_prc: 0.7422\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0016 - accuracy: 0.9992 - precision: 0.7644 - recall: 0.8235 - auc: 0.9712 - prc: 0.7665 - val_loss: 0.0047 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9636 - val_prc: 0.7552\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0016 - accuracy: 0.9992 - precision: 0.7528 - recall: 0.8297 - auc: 0.9733 - prc: 0.7662 - val_loss: 0.0060 - val_accuracy: 0.9993 - val_precision: 0.7042 - val_recall: 0.7937 - val_auc: 0.9693 - val_prc: 0.7529\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7348 - recall: 0.8235 - auc: 0.9765 - prc: 0.7661 - val_loss: 0.0060 - val_accuracy: 0.9993 - val_precision: 0.7083 - val_recall: 0.8095 - val_auc: 0.9699 - val_prc: 0.7268\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9993 - precision: 0.7797 - recall: 0.8328 - auc: 0.9728 - prc: 0.7689 - val_loss: 0.0082 - val_accuracy: 0.9989 - val_precision: 0.5604 - val_recall: 0.8095 - val_auc: 0.9759 - val_prc: 0.7507\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7424 - recall: 0.8297 - auc: 0.9726 - prc: 0.7668 - val_loss: 0.0062 - val_accuracy: 0.9991 - val_precision: 0.6579 - val_recall: 0.7937 - val_auc: 0.9698 - val_prc: 0.7550\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7514 - recall: 0.8328 - auc: 0.9786 - prc: 0.7699 - val_loss: 0.0048 - val_accuracy: 0.9994 - val_precision: 0.7812 - val_recall: 0.7937 - val_auc: 0.9639 - val_prc: 0.7397\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7500 - recall: 0.8266 - auc: 0.9788 - prc: 0.7631 - val_loss: 0.0058 - val_accuracy: 0.9993 - val_precision: 0.7143 - val_recall: 0.7937 - val_auc: 0.9702 - val_prc: 0.7560\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7535 - recall: 0.8328 - auc: 0.9785 - prc: 0.7776 - val_loss: 0.0045 - val_accuracy: 0.9994 - val_precision: 0.7812 - val_recall: 0.7937 - val_auc: 0.9572 - val_prc: 0.7169\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7403 - recall: 0.8297 - auc: 0.9790 - prc: 0.7744 - val_loss: 0.0055 - val_accuracy: 0.9993 - val_precision: 0.7246 - val_recall: 0.7937 - val_auc: 0.9698 - val_prc: 0.7522\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7370 - recall: 0.8328 - auc: 0.9777 - prc: 0.7751 - val_loss: 0.0097 - val_accuracy: 0.9987 - val_precision: 0.5152 - val_recall: 0.8095 - val_auc: 0.9752 - val_prc: 0.7283\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0014 - accuracy: 0.9992 - precision: 0.7335 - recall: 0.8266 - auc: 0.9829 - prc: 0.7719 - val_loss: 0.0046 - val_accuracy: 0.9995 - val_precision: 0.8448 - val_recall: 0.7778 - val_auc: 0.9707 - val_prc: 0.7663\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0014 - accuracy: 0.9992 - precision: 0.7584 - recall: 0.8359 - auc: 0.9822 - prc: 0.7734 - val_loss: 0.0070 - val_accuracy: 0.9989 - val_precision: 0.5814 - val_recall: 0.7937 - val_auc: 0.9753 - val_prc: 0.7523\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0014 - accuracy: 0.9992 - precision: 0.7583 - recall: 0.8452 - auc: 0.9788 - prc: 0.7765 - val_loss: 0.0055 - val_accuracy: 0.9992 - val_precision: 0.6944 - val_recall: 0.7937 - val_auc: 0.9773 - val_prc: 0.7581\n",
            "1419/1419 [==============================] - 2s 2ms/step\n",
            "******************************for w =  0.09\n",
            "Model: \"sequential_23\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_23 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_23 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_23 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89/89 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 0.9944 - precision: 0.1489 - recall: 0.4845 - auc: 0.7832 - prc: 0.4107"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 4s 22ms/step - loss: 0.0114 - accuracy: 0.9944 - precision: 0.1489 - recall: 0.4845 - auc: 0.7832 - prc: 0.4107 - val_loss: 0.0128 - val_accuracy: 0.9994 - val_precision: 0.7903 - val_recall: 0.7778 - val_auc: 0.9361 - val_prc: 0.6678\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0025 - accuracy: 0.9994 - precision: 0.8605 - recall: 0.7833 - auc: 0.9256 - prc: 0.7167 - val_loss: 0.0085 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9624 - val_prc: 0.7082\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0021 - accuracy: 0.9994 - precision: 0.8472 - recall: 0.7895 - auc: 0.9489 - prc: 0.7441 - val_loss: 0.0067 - val_accuracy: 0.9994 - val_precision: 0.7812 - val_recall: 0.7937 - val_auc: 0.9702 - val_prc: 0.7189\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0019 - accuracy: 0.9994 - precision: 0.8296 - recall: 0.7988 - auc: 0.9569 - prc: 0.7576 - val_loss: 0.0074 - val_accuracy: 0.9993 - val_precision: 0.6986 - val_recall: 0.8095 - val_auc: 0.9693 - val_prc: 0.7206\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.7964 - recall: 0.8235 - auc: 0.9694 - prc: 0.7643 - val_loss: 0.0050 - val_accuracy: 0.9994 - val_precision: 0.7812 - val_recall: 0.7937 - val_auc: 0.9707 - val_prc: 0.7584\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.7831 - recall: 0.8050 - auc: 0.9706 - prc: 0.7587 - val_loss: 0.0082 - val_accuracy: 0.9991 - val_precision: 0.6538 - val_recall: 0.8095 - val_auc: 0.9752 - val_prc: 0.7339\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7853 - recall: 0.8266 - auc: 0.9758 - prc: 0.7648 - val_loss: 0.0055 - val_accuracy: 0.9993 - val_precision: 0.7143 - val_recall: 0.7937 - val_auc: 0.9695 - val_prc: 0.7613\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7788 - recall: 0.8173 - auc: 0.9778 - prc: 0.7639 - val_loss: 0.0055 - val_accuracy: 0.9993 - val_precision: 0.7246 - val_recall: 0.7937 - val_auc: 0.9695 - val_prc: 0.7585\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9992 - precision: 0.7644 - recall: 0.8235 - auc: 0.9711 - prc: 0.7641 - val_loss: 0.0058 - val_accuracy: 0.9992 - val_precision: 0.6944 - val_recall: 0.7937 - val_auc: 0.9769 - val_prc: 0.7589\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7768 - recall: 0.8297 - auc: 0.9755 - prc: 0.7668 - val_loss: 0.0064 - val_accuracy: 0.9992 - val_precision: 0.6623 - val_recall: 0.8095 - val_auc: 0.9772 - val_prc: 0.7433\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7659 - recall: 0.8204 - auc: 0.9790 - prc: 0.7717 - val_loss: 0.0069 - val_accuracy: 0.9991 - val_precision: 0.6538 - val_recall: 0.8095 - val_auc: 0.9765 - val_prc: 0.7181\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0015 - accuracy: 0.9993 - precision: 0.7721 - recall: 0.8390 - auc: 0.9708 - prc: 0.7734 - val_loss: 0.0060 - val_accuracy: 0.9992 - val_precision: 0.6757 - val_recall: 0.7937 - val_auc: 0.9691 - val_prc: 0.7527\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7528 - recall: 0.8297 - auc: 0.9715 - prc: 0.7675 - val_loss: 0.0062 - val_accuracy: 0.9992 - val_precision: 0.6757 - val_recall: 0.7937 - val_auc: 0.9767 - val_prc: 0.7574\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7570 - recall: 0.8390 - auc: 0.9728 - prc: 0.7751 - val_loss: 0.0040 - val_accuracy: 0.9995 - val_precision: 0.8421 - val_recall: 0.7619 - val_auc: 0.9567 - val_prc: 0.7674\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7642 - recall: 0.8328 - auc: 0.9763 - prc: 0.7707 - val_loss: 0.0053 - val_accuracy: 0.9993 - val_precision: 0.7206 - val_recall: 0.7778 - val_auc: 0.9705 - val_prc: 0.7471\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7637 - recall: 0.8204 - auc: 0.9770 - prc: 0.7713 - val_loss: 0.0054 - val_accuracy: 0.9993 - val_precision: 0.7101 - val_recall: 0.7778 - val_auc: 0.9703 - val_prc: 0.7527\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7629 - recall: 0.8266 - auc: 0.9720 - prc: 0.7733 - val_loss: 0.0057 - val_accuracy: 0.9992 - val_precision: 0.6944 - val_recall: 0.7937 - val_auc: 0.9702 - val_prc: 0.7362\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7535 - recall: 0.8328 - auc: 0.9748 - prc: 0.7696 - val_loss: 0.0066 - val_accuracy: 0.9990 - val_precision: 0.6098 - val_recall: 0.7937 - val_auc: 0.9765 - val_prc: 0.7526\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0014 - accuracy: 0.9993 - precision: 0.7649 - recall: 0.8359 - auc: 0.9789 - prc: 0.7724 - val_loss: 0.0051 - val_accuracy: 0.9993 - val_precision: 0.7353 - val_recall: 0.7937 - val_auc: 0.9699 - val_prc: 0.7544\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7389 - recall: 0.8235 - auc: 0.9794 - prc: 0.7737 - val_loss: 0.0059 - val_accuracy: 0.9992 - val_precision: 0.6757 - val_recall: 0.7937 - val_auc: 0.9774 - val_prc: 0.7366\n",
            "1419/1419 [==============================] - 2s 2ms/step\n",
            "******************************for w =  0.09999999999999999\n",
            "Model: \"sequential_24\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_24 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_24 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_24 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86/89 [===========================>..] - ETA: 0s - loss: 0.0131 - accuracy: 0.9939 - precision: 0.1292 - recall: 0.4558 - auc: 0.7375 - prc: 0.3817"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 19ms/step - loss: 0.0129 - accuracy: 0.9940 - precision: 0.1343 - recall: 0.4611 - auc: 0.7434 - prc: 0.3883 - val_loss: 0.0102 - val_accuracy: 0.9994 - val_precision: 0.7931 - val_recall: 0.7302 - val_auc: 0.9101 - val_prc: 0.6467\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0026 - accuracy: 0.9994 - precision: 0.8576 - recall: 0.7647 - auc: 0.9234 - prc: 0.7108 - val_loss: 0.0086 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9691 - val_prc: 0.7046\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0021 - accuracy: 0.9994 - precision: 0.8472 - recall: 0.7895 - auc: 0.9495 - prc: 0.7430 - val_loss: 0.0068 - val_accuracy: 0.9994 - val_precision: 0.7692 - val_recall: 0.7937 - val_auc: 0.9703 - val_prc: 0.7161\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0019 - accuracy: 0.9994 - precision: 0.8382 - recall: 0.8019 - auc: 0.9567 - prc: 0.7569 - val_loss: 0.0057 - val_accuracy: 0.9994 - val_precision: 0.7576 - val_recall: 0.7937 - val_auc: 0.9706 - val_prc: 0.7553\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0018 - accuracy: 0.9993 - precision: 0.8162 - recall: 0.8111 - auc: 0.9634 - prc: 0.7561 - val_loss: 0.0061 - val_accuracy: 0.9993 - val_precision: 0.7083 - val_recall: 0.8095 - val_auc: 0.9698 - val_prc: 0.7561\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.7939 - recall: 0.8111 - auc: 0.9728 - prc: 0.7585 - val_loss: 0.0069 - val_accuracy: 0.9993 - val_precision: 0.6986 - val_recall: 0.8095 - val_auc: 0.9685 - val_prc: 0.7319\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.7870 - recall: 0.8235 - auc: 0.9691 - prc: 0.7631 - val_loss: 0.0052 - val_accuracy: 0.9993 - val_precision: 0.7143 - val_recall: 0.7937 - val_auc: 0.9711 - val_prc: 0.7528\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.7814 - recall: 0.8080 - auc: 0.9722 - prc: 0.7613 - val_loss: 0.0068 - val_accuracy: 0.9992 - val_precision: 0.6800 - val_recall: 0.8095 - val_auc: 0.9762 - val_prc: 0.7414\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7817 - recall: 0.8204 - auc: 0.9727 - prc: 0.7601 - val_loss: 0.0059 - val_accuracy: 0.9992 - val_precision: 0.6944 - val_recall: 0.7937 - val_auc: 0.9698 - val_prc: 0.7399\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7726 - recall: 0.8204 - auc: 0.9725 - prc: 0.7604 - val_loss: 0.0055 - val_accuracy: 0.9993 - val_precision: 0.7246 - val_recall: 0.7937 - val_auc: 0.9636 - val_prc: 0.7176\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7830 - recall: 0.8266 - auc: 0.9689 - prc: 0.7678 - val_loss: 0.0049 - val_accuracy: 0.9993 - val_precision: 0.7424 - val_recall: 0.7778 - val_auc: 0.9713 - val_prc: 0.7479\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7801 - recall: 0.8235 - auc: 0.9772 - prc: 0.7647 - val_loss: 0.0051 - val_accuracy: 0.9994 - val_precision: 0.7692 - val_recall: 0.7937 - val_auc: 0.9644 - val_prc: 0.7146\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7917 - recall: 0.8235 - auc: 0.9687 - prc: 0.7649 - val_loss: 0.0066 - val_accuracy: 0.9991 - val_precision: 0.6494 - val_recall: 0.7937 - val_auc: 0.9696 - val_prc: 0.7350\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0015 - accuracy: 0.9993 - precision: 0.7843 - recall: 0.8328 - auc: 0.9690 - prc: 0.7660 - val_loss: 0.0073 - val_accuracy: 0.9989 - val_precision: 0.5814 - val_recall: 0.7937 - val_auc: 0.9761 - val_prc: 0.7339\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0016 - accuracy: 0.9992 - precision: 0.7659 - recall: 0.8204 - auc: 0.9726 - prc: 0.7666 - val_loss: 0.0045 - val_accuracy: 0.9994 - val_precision: 0.7903 - val_recall: 0.7778 - val_auc: 0.9567 - val_prc: 0.7417\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0015 - accuracy: 0.9993 - precision: 0.8030 - recall: 0.8204 - auc: 0.9749 - prc: 0.7761 - val_loss: 0.0047 - val_accuracy: 0.9994 - val_precision: 0.7903 - val_recall: 0.7778 - val_auc: 0.9632 - val_prc: 0.7482\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0015 - accuracy: 0.9993 - precision: 0.7692 - recall: 0.8359 - auc: 0.9768 - prc: 0.7696 - val_loss: 0.0084 - val_accuracy: 0.9988 - val_precision: 0.5484 - val_recall: 0.8095 - val_auc: 0.9758 - val_prc: 0.7177\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0015 - accuracy: 0.9993 - precision: 0.7686 - recall: 0.8328 - auc: 0.9717 - prc: 0.7711 - val_loss: 0.0050 - val_accuracy: 0.9994 - val_precision: 0.7576 - val_recall: 0.7937 - val_auc: 0.9713 - val_prc: 0.7446\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7591 - recall: 0.8390 - auc: 0.9754 - prc: 0.7694 - val_loss: 0.0040 - val_accuracy: 0.9994 - val_precision: 0.8167 - val_recall: 0.7778 - val_auc: 0.9567 - val_prc: 0.7654\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0015 - accuracy: 0.9993 - precision: 0.7781 - recall: 0.8359 - auc: 0.9752 - prc: 0.7745 - val_loss: 0.0043 - val_accuracy: 0.9994 - val_precision: 0.7903 - val_recall: 0.7778 - val_auc: 0.9562 - val_prc: 0.7697\n",
            "1419/1419 [==============================] - 2s 2ms/step\n",
            "******************************for w =  0.10999999999999999\n",
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_25 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_25 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_25 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87/89 [============================>.] - ETA: 0s - loss: 0.0139 - accuracy: 0.9918 - precision: 0.0869 - recall: 0.4063 - auc: 0.7294 - prc: 0.3625"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 17ms/step - loss: 0.0137 - accuracy: 0.9919 - precision: 0.0894 - recall: 0.4119 - auc: 0.7334 - prc: 0.3689 - val_loss: 0.0103 - val_accuracy: 0.9994 - val_precision: 0.7931 - val_recall: 0.7302 - val_auc: 0.9044 - val_prc: 0.6272\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0027 - accuracy: 0.9994 - precision: 0.8636 - recall: 0.7647 - auc: 0.9176 - prc: 0.7059 - val_loss: 0.0075 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9553 - val_prc: 0.6849\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0022 - accuracy: 0.9994 - precision: 0.8649 - recall: 0.7926 - auc: 0.9477 - prc: 0.7421 - val_loss: 0.0061 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9698 - val_prc: 0.7058\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0019 - accuracy: 0.9994 - precision: 0.8404 - recall: 0.7988 - auc: 0.9588 - prc: 0.7531 - val_loss: 0.0061 - val_accuracy: 0.9993 - val_precision: 0.7286 - val_recall: 0.8095 - val_auc: 0.9704 - val_prc: 0.7372\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9993 - precision: 0.8182 - recall: 0.8080 - auc: 0.9633 - prc: 0.7569 - val_loss: 0.0050 - val_accuracy: 0.9994 - val_precision: 0.7692 - val_recall: 0.7937 - val_auc: 0.9711 - val_prc: 0.7551\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0018 - accuracy: 0.9993 - precision: 0.8075 - recall: 0.8050 - auc: 0.9658 - prc: 0.7578 - val_loss: 0.0056 - val_accuracy: 0.9993 - val_precision: 0.7083 - val_recall: 0.8095 - val_auc: 0.9705 - val_prc: 0.7555\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8080 - recall: 0.8080 - auc: 0.9679 - prc: 0.7555 - val_loss: 0.0056 - val_accuracy: 0.9993 - val_precision: 0.7042 - val_recall: 0.7937 - val_auc: 0.9699 - val_prc: 0.7544\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8072 - recall: 0.8297 - auc: 0.9685 - prc: 0.7598 - val_loss: 0.0082 - val_accuracy: 0.9990 - val_precision: 0.5930 - val_recall: 0.8095 - val_auc: 0.9746 - val_prc: 0.7395\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8092 - recall: 0.8142 - auc: 0.9666 - prc: 0.7584 - val_loss: 0.0068 - val_accuracy: 0.9991 - val_precision: 0.6296 - val_recall: 0.8095 - val_auc: 0.9756 - val_prc: 0.7516\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.7811 - recall: 0.8173 - auc: 0.9655 - prc: 0.7583 - val_loss: 0.0053 - val_accuracy: 0.9993 - val_precision: 0.7353 - val_recall: 0.7937 - val_auc: 0.9707 - val_prc: 0.7533\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7870 - recall: 0.8235 - auc: 0.9717 - prc: 0.7579 - val_loss: 0.0052 - val_accuracy: 0.9993 - val_precision: 0.7143 - val_recall: 0.7937 - val_auc: 0.9710 - val_prc: 0.7472\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7940 - recall: 0.8235 - auc: 0.9689 - prc: 0.7612 - val_loss: 0.0042 - val_accuracy: 0.9994 - val_precision: 0.8033 - val_recall: 0.7778 - val_auc: 0.9566 - val_prc: 0.7509\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7876 - recall: 0.8266 - auc: 0.9690 - prc: 0.7665 - val_loss: 0.0048 - val_accuracy: 0.9994 - val_precision: 0.7812 - val_recall: 0.7937 - val_auc: 0.9568 - val_prc: 0.7130\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.8006 - recall: 0.8204 - auc: 0.9660 - prc: 0.7685 - val_loss: 0.0046 - val_accuracy: 0.9994 - val_precision: 0.7812 - val_recall: 0.7937 - val_auc: 0.9644 - val_prc: 0.7486\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7982 - recall: 0.8204 - auc: 0.9647 - prc: 0.7627 - val_loss: 0.0061 - val_accuracy: 0.9991 - val_precision: 0.6410 - val_recall: 0.7937 - val_auc: 0.9693 - val_prc: 0.7509\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.8018 - recall: 0.8142 - auc: 0.9675 - prc: 0.7680 - val_loss: 0.0065 - val_accuracy: 0.9991 - val_precision: 0.6410 - val_recall: 0.7937 - val_auc: 0.9697 - val_prc: 0.7493\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7759 - recall: 0.8359 - auc: 0.9688 - prc: 0.7716 - val_loss: 0.0039 - val_accuracy: 0.9994 - val_precision: 0.8033 - val_recall: 0.7778 - val_auc: 0.9571 - val_prc: 0.7679\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0015 - accuracy: 0.9993 - precision: 0.7935 - recall: 0.8328 - auc: 0.9722 - prc: 0.7719 - val_loss: 0.0038 - val_accuracy: 0.9994 - val_precision: 0.8033 - val_recall: 0.7778 - val_auc: 0.9575 - val_prc: 0.7472\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0015 - accuracy: 0.9993 - precision: 0.8006 - recall: 0.8204 - auc: 0.9722 - prc: 0.7657 - val_loss: 0.0059 - val_accuracy: 0.9991 - val_precision: 0.6579 - val_recall: 0.7937 - val_auc: 0.9706 - val_prc: 0.7118\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9993 - precision: 0.7965 - recall: 0.8359 - auc: 0.9709 - prc: 0.7756 - val_loss: 0.0053 - val_accuracy: 0.9993 - val_precision: 0.7206 - val_recall: 0.7778 - val_auc: 0.9702 - val_prc: 0.7494\n",
            "1419/1419 [==============================] - 2s 2ms/step\n",
            "******************************for w =  0.11999999999999998\n",
            "Model: \"sequential_26\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_26 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_26 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_26 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87/89 [============================>.] - ETA: 0s - loss: 0.0155 - accuracy: 0.9901 - precision: 0.0706 - recall: 0.4016 - auc: 0.7161 - prc: 0.3424"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 14ms/step - loss: 0.0152 - accuracy: 0.9902 - precision: 0.0749 - recall: 0.4171 - auc: 0.7248 - prc: 0.3605 - val_loss: 0.0109 - val_accuracy: 0.9994 - val_precision: 0.8070 - val_recall: 0.7302 - val_auc: 0.9086 - val_prc: 0.6343\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0029 - accuracy: 0.9993 - precision: 0.8669 - recall: 0.7461 - auc: 0.9098 - prc: 0.6951 - val_loss: 0.0088 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9594 - val_prc: 0.6909\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0023 - accuracy: 0.9994 - precision: 0.8615 - recall: 0.7895 - auc: 0.9455 - prc: 0.7349 - val_loss: 0.0074 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9672 - val_prc: 0.7104\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0021 - accuracy: 0.9994 - precision: 0.8528 - recall: 0.7895 - auc: 0.9559 - prc: 0.7470 - val_loss: 0.0053 - val_accuracy: 0.9994 - val_precision: 0.8167 - val_recall: 0.7778 - val_auc: 0.9706 - val_prc: 0.7075\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0020 - accuracy: 0.9993 - precision: 0.8285 - recall: 0.7926 - auc: 0.9557 - prc: 0.7533 - val_loss: 0.0044 - val_accuracy: 0.9995 - val_precision: 0.8596 - val_recall: 0.7778 - val_auc: 0.9644 - val_prc: 0.7478\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8333 - recall: 0.8050 - auc: 0.9650 - prc: 0.7526 - val_loss: 0.0053 - val_accuracy: 0.9994 - val_precision: 0.7576 - val_recall: 0.7937 - val_auc: 0.9706 - val_prc: 0.7558\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8302 - recall: 0.8173 - auc: 0.9692 - prc: 0.7603 - val_loss: 0.0052 - val_accuracy: 0.9994 - val_precision: 0.7500 - val_recall: 0.8095 - val_auc: 0.9634 - val_prc: 0.7541\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8265 - recall: 0.8111 - auc: 0.9638 - prc: 0.7591 - val_loss: 0.0043 - val_accuracy: 0.9995 - val_precision: 0.8448 - val_recall: 0.7778 - val_auc: 0.9563 - val_prc: 0.7556\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8148 - recall: 0.8173 - auc: 0.9689 - prc: 0.7676 - val_loss: 0.0056 - val_accuracy: 0.9993 - val_precision: 0.7286 - val_recall: 0.8095 - val_auc: 0.9700 - val_prc: 0.7555\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.7765 - recall: 0.8173 - auc: 0.9674 - prc: 0.7639 - val_loss: 0.0037 - val_accuracy: 0.9995 - val_precision: 0.8868 - val_recall: 0.7460 - val_auc: 0.9571 - val_prc: 0.7568\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8106 - recall: 0.8080 - auc: 0.9718 - prc: 0.7611 - val_loss: 0.0050 - val_accuracy: 0.9993 - val_precision: 0.7463 - val_recall: 0.7937 - val_auc: 0.9711 - val_prc: 0.7546\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.8148 - recall: 0.8173 - auc: 0.9634 - prc: 0.7629 - val_loss: 0.0053 - val_accuracy: 0.9993 - val_precision: 0.7143 - val_recall: 0.7937 - val_auc: 0.9703 - val_prc: 0.7550\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9994 - precision: 0.8302 - recall: 0.8173 - auc: 0.9696 - prc: 0.7663 - val_loss: 0.0044 - val_accuracy: 0.9994 - val_precision: 0.7903 - val_recall: 0.7778 - val_auc: 0.9566 - val_prc: 0.7529\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7982 - recall: 0.8204 - auc: 0.9646 - prc: 0.7619 - val_loss: 0.0059 - val_accuracy: 0.9992 - val_precision: 0.6849 - val_recall: 0.7937 - val_auc: 0.9699 - val_prc: 0.7483\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.8055 - recall: 0.8204 - auc: 0.9691 - prc: 0.7623 - val_loss: 0.0038 - val_accuracy: 0.9994 - val_precision: 0.8167 - val_recall: 0.7778 - val_auc: 0.9578 - val_prc: 0.7451\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.8079 - recall: 0.8204 - auc: 0.9708 - prc: 0.7660 - val_loss: 0.0045 - val_accuracy: 0.9994 - val_precision: 0.7812 - val_recall: 0.7937 - val_auc: 0.9643 - val_prc: 0.7459\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0016 - accuracy: 0.9994 - precision: 0.8219 - recall: 0.8142 - auc: 0.9697 - prc: 0.7726 - val_loss: 0.0040 - val_accuracy: 0.9994 - val_precision: 0.7903 - val_recall: 0.7778 - val_auc: 0.9574 - val_prc: 0.7464\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.8085 - recall: 0.8235 - auc: 0.9700 - prc: 0.7701 - val_loss: 0.0040 - val_accuracy: 0.9994 - val_precision: 0.8136 - val_recall: 0.7619 - val_auc: 0.9642 - val_prc: 0.7596\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7887 - recall: 0.8204 - auc: 0.9683 - prc: 0.7802 - val_loss: 0.0049 - val_accuracy: 0.9993 - val_precision: 0.7463 - val_recall: 0.7937 - val_auc: 0.9714 - val_prc: 0.7520\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9994 - precision: 0.8302 - recall: 0.8328 - auc: 0.9708 - prc: 0.7755 - val_loss: 0.0041 - val_accuracy: 0.9994 - val_precision: 0.8065 - val_recall: 0.7937 - val_auc: 0.9577 - val_prc: 0.7141\n",
            "1419/1419 [==============================] - 2s 2ms/step\n",
            "******************************for w =  0.12999999999999998\n",
            "Model: \"sequential_27\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_27 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_27 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_27 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88/89 [============================>.] - ETA: 0s - loss: 0.0162 - accuracy: 0.9905 - precision: 0.0745 - recall: 0.4058 - auc: 0.7100 - prc: 0.3314"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 14ms/step - loss: 0.0161 - accuracy: 0.9905 - precision: 0.0759 - recall: 0.4093 - auc: 0.7118 - prc: 0.3356 - val_loss: 0.0103 - val_accuracy: 0.9993 - val_precision: 0.7895 - val_recall: 0.7143 - val_auc: 0.9012 - val_prc: 0.6491\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0028 - accuracy: 0.9993 - precision: 0.8628 - recall: 0.7399 - auc: 0.9119 - prc: 0.6918 - val_loss: 0.0081 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9609 - val_prc: 0.6957\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0023 - accuracy: 0.9994 - precision: 0.8664 - recall: 0.7833 - auc: 0.9444 - prc: 0.7337 - val_loss: 0.0053 - val_accuracy: 0.9994 - val_precision: 0.8033 - val_recall: 0.7778 - val_auc: 0.9700 - val_prc: 0.7331\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0021 - accuracy: 0.9994 - precision: 0.8678 - recall: 0.7926 - auc: 0.9524 - prc: 0.7512 - val_loss: 0.0056 - val_accuracy: 0.9994 - val_precision: 0.7576 - val_recall: 0.7937 - val_auc: 0.9704 - val_prc: 0.7578\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0019 - accuracy: 0.9994 - precision: 0.8624 - recall: 0.7957 - auc: 0.9589 - prc: 0.7537 - val_loss: 0.0051 - val_accuracy: 0.9994 - val_precision: 0.7576 - val_recall: 0.7937 - val_auc: 0.9713 - val_prc: 0.7566\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0019 - accuracy: 0.9994 - precision: 0.8286 - recall: 0.8080 - auc: 0.9622 - prc: 0.7588 - val_loss: 0.0049 - val_accuracy: 0.9994 - val_precision: 0.7812 - val_recall: 0.7937 - val_auc: 0.9707 - val_prc: 0.7553\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0018 - accuracy: 0.9993 - precision: 0.8182 - recall: 0.8080 - auc: 0.9667 - prc: 0.7572 - val_loss: 0.0046 - val_accuracy: 0.9994 - val_precision: 0.7778 - val_recall: 0.7778 - val_auc: 0.9637 - val_prc: 0.7551\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0018 - accuracy: 0.9993 - precision: 0.8062 - recall: 0.8111 - auc: 0.9661 - prc: 0.7569 - val_loss: 0.0045 - val_accuracy: 0.9994 - val_precision: 0.7812 - val_recall: 0.7937 - val_auc: 0.9565 - val_prc: 0.7517\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8067 - recall: 0.8142 - auc: 0.9689 - prc: 0.7556 - val_loss: 0.0054 - val_accuracy: 0.9993 - val_precision: 0.7143 - val_recall: 0.7937 - val_auc: 0.9626 - val_prc: 0.7542\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8376 - recall: 0.8142 - auc: 0.9721 - prc: 0.7589 - val_loss: 0.0060 - val_accuracy: 0.9992 - val_precision: 0.6757 - val_recall: 0.7937 - val_auc: 0.9693 - val_prc: 0.7540\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8116 - recall: 0.8266 - auc: 0.9650 - prc: 0.7648 - val_loss: 0.0055 - val_accuracy: 0.9993 - val_precision: 0.7246 - val_recall: 0.7937 - val_auc: 0.9707 - val_prc: 0.7389\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.7970 - recall: 0.8142 - auc: 0.9649 - prc: 0.7604 - val_loss: 0.0047 - val_accuracy: 0.9994 - val_precision: 0.7576 - val_recall: 0.7937 - val_auc: 0.9635 - val_prc: 0.7512\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8270 - recall: 0.8142 - auc: 0.9606 - prc: 0.7616 - val_loss: 0.0042 - val_accuracy: 0.9995 - val_precision: 0.8305 - val_recall: 0.7778 - val_auc: 0.9564 - val_prc: 0.7635\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.7851 - recall: 0.8142 - auc: 0.9665 - prc: 0.7641 - val_loss: 0.0042 - val_accuracy: 0.9994 - val_precision: 0.7778 - val_recall: 0.7778 - val_auc: 0.9567 - val_prc: 0.7520\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0016 - accuracy: 0.9994 - precision: 0.8154 - recall: 0.8204 - auc: 0.9609 - prc: 0.7645 - val_loss: 0.0055 - val_accuracy: 0.9993 - val_precision: 0.7463 - val_recall: 0.7937 - val_auc: 0.9702 - val_prc: 0.7306\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0016 - accuracy: 0.9994 - precision: 0.8297 - recall: 0.8142 - auc: 0.9665 - prc: 0.7677 - val_loss: 0.0052 - val_accuracy: 0.9993 - val_precision: 0.7353 - val_recall: 0.7937 - val_auc: 0.9709 - val_prc: 0.7487\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7952 - recall: 0.8173 - auc: 0.9667 - prc: 0.7647 - val_loss: 0.0049 - val_accuracy: 0.9993 - val_precision: 0.7463 - val_recall: 0.7937 - val_auc: 0.9715 - val_prc: 0.7460\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7906 - recall: 0.8297 - auc: 0.9682 - prc: 0.7745 - val_loss: 0.0040 - val_accuracy: 0.9994 - val_precision: 0.7903 - val_recall: 0.7778 - val_auc: 0.9570 - val_prc: 0.7477\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.8073 - recall: 0.8173 - auc: 0.9698 - prc: 0.7771 - val_loss: 0.0045 - val_accuracy: 0.9994 - val_precision: 0.7692 - val_recall: 0.7937 - val_auc: 0.9567 - val_prc: 0.7054\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9994 - precision: 0.8232 - recall: 0.8359 - auc: 0.9697 - prc: 0.7721 - val_loss: 0.0045 - val_accuracy: 0.9994 - val_precision: 0.7576 - val_recall: 0.7937 - val_auc: 0.9564 - val_prc: 0.7443\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "******************************for w =  0.13999999999999999\n",
            "Model: \"sequential_28\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_28 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_28 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_28 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85/89 [===========================>..] - ETA: 0s - loss: 0.0169 - accuracy: 0.9925 - precision: 0.0911 - recall: 0.3780 - auc: 0.6879 - prc: 0.3031"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 14ms/step - loss: 0.0164 - accuracy: 0.9928 - precision: 0.0963 - recall: 0.3886 - auc: 0.6934 - prc: 0.3104 - val_loss: 0.0100 - val_accuracy: 0.9994 - val_precision: 0.8182 - val_recall: 0.7143 - val_auc: 0.9071 - val_prc: 0.6263\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0029 - accuracy: 0.9993 - precision: 0.8686 - recall: 0.7368 - auc: 0.9105 - prc: 0.6931 - val_loss: 0.0078 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9545 - val_prc: 0.6886\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0024 - accuracy: 0.9994 - precision: 0.8694 - recall: 0.7833 - auc: 0.9459 - prc: 0.7349 - val_loss: 0.0062 - val_accuracy: 0.9994 - val_precision: 0.8033 - val_recall: 0.7778 - val_auc: 0.9696 - val_prc: 0.7119\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0021 - accuracy: 0.9994 - precision: 0.8664 - recall: 0.7833 - auc: 0.9546 - prc: 0.7428 - val_loss: 0.0052 - val_accuracy: 0.9994 - val_precision: 0.8167 - val_recall: 0.7778 - val_auc: 0.9713 - val_prc: 0.7405\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0020 - accuracy: 0.9994 - precision: 0.8557 - recall: 0.7895 - auc: 0.9573 - prc: 0.7509 - val_loss: 0.0054 - val_accuracy: 0.9993 - val_precision: 0.7246 - val_recall: 0.7937 - val_auc: 0.9707 - val_prc: 0.7578\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0019 - accuracy: 0.9994 - precision: 0.8296 - recall: 0.7988 - auc: 0.9581 - prc: 0.7536 - val_loss: 0.0056 - val_accuracy: 0.9993 - val_precision: 0.7183 - val_recall: 0.8095 - val_auc: 0.9710 - val_prc: 0.7418\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8328 - recall: 0.8019 - auc: 0.9654 - prc: 0.7542 - val_loss: 0.0037 - val_accuracy: 0.9995 - val_precision: 0.8727 - val_recall: 0.7619 - val_auc: 0.9578 - val_prc: 0.7546\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8307 - recall: 0.8050 - auc: 0.9591 - prc: 0.7560 - val_loss: 0.0055 - val_accuracy: 0.9993 - val_precision: 0.7286 - val_recall: 0.8095 - val_auc: 0.9706 - val_prc: 0.7487\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8176 - recall: 0.8050 - auc: 0.9678 - prc: 0.7608 - val_loss: 0.0052 - val_accuracy: 0.9993 - val_precision: 0.7353 - val_recall: 0.7937 - val_auc: 0.9632 - val_prc: 0.7513\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8297 - recall: 0.8142 - auc: 0.9666 - prc: 0.7635 - val_loss: 0.0050 - val_accuracy: 0.9994 - val_precision: 0.7692 - val_recall: 0.7937 - val_auc: 0.9636 - val_prc: 0.7456\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8296 - recall: 0.7988 - auc: 0.9638 - prc: 0.7627 - val_loss: 0.0044 - val_accuracy: 0.9994 - val_precision: 0.7812 - val_recall: 0.7937 - val_auc: 0.9569 - val_prc: 0.7460\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8413 - recall: 0.8204 - auc: 0.9624 - prc: 0.7579 - val_loss: 0.0045 - val_accuracy: 0.9993 - val_precision: 0.7538 - val_recall: 0.7778 - val_auc: 0.9565 - val_prc: 0.7484\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8098 - recall: 0.8173 - auc: 0.9664 - prc: 0.7657 - val_loss: 0.0042 - val_accuracy: 0.9994 - val_precision: 0.8000 - val_recall: 0.7619 - val_auc: 0.9565 - val_prc: 0.7589\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9994 - precision: 0.8245 - recall: 0.8142 - auc: 0.9683 - prc: 0.7718 - val_loss: 0.0048 - val_accuracy: 0.9994 - val_precision: 0.7692 - val_recall: 0.7937 - val_auc: 0.9643 - val_prc: 0.7368\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0016 - accuracy: 0.9994 - precision: 0.8302 - recall: 0.8173 - auc: 0.9608 - prc: 0.7667 - val_loss: 0.0045 - val_accuracy: 0.9994 - val_precision: 0.7903 - val_recall: 0.7778 - val_auc: 0.9642 - val_prc: 0.7485\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.7964 - recall: 0.8111 - auc: 0.9592 - prc: 0.7652 - val_loss: 0.0055 - val_accuracy: 0.9993 - val_precision: 0.7042 - val_recall: 0.7937 - val_auc: 0.9705 - val_prc: 0.7498\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.8110 - recall: 0.8235 - auc: 0.9712 - prc: 0.7713 - val_loss: 0.0034 - val_accuracy: 0.9994 - val_precision: 0.8393 - val_recall: 0.7460 - val_auc: 0.9581 - val_prc: 0.7616\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.8117 - recall: 0.8142 - auc: 0.9655 - prc: 0.7865 - val_loss: 0.0045 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9566 - val_prc: 0.7326\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0016 - accuracy: 0.9994 - precision: 0.8286 - recall: 0.8080 - auc: 0.9657 - prc: 0.7728 - val_loss: 0.0057 - val_accuracy: 0.9991 - val_precision: 0.6494 - val_recall: 0.7937 - val_auc: 0.9638 - val_prc: 0.7101\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7953 - recall: 0.8297 - auc: 0.9625 - prc: 0.7818 - val_loss: 0.0034 - val_accuracy: 0.9994 - val_precision: 0.8393 - val_recall: 0.7460 - val_auc: 0.9583 - val_prc: 0.7546\n",
            "1419/1419 [==============================] - 2s 2ms/step\n",
            "******************************for w =  0.15\n",
            "Model: \"sequential_29\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_29 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_29 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_29 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87/89 [============================>.] - ETA: 0s - loss: 0.0179 - accuracy: 0.9939 - precision: 0.1054 - recall: 0.3456 - auc: 0.6588 - prc: 0.2868"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 14ms/step - loss: 0.0176 - accuracy: 0.9940 - precision: 0.1089 - recall: 0.3523 - auc: 0.6648 - prc: 0.2941 - val_loss: 0.0104 - val_accuracy: 0.9993 - val_precision: 0.7925 - val_recall: 0.6667 - val_auc: 0.8870 - val_prc: 0.6405\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0030 - accuracy: 0.9993 - precision: 0.8664 - recall: 0.7430 - auc: 0.9001 - prc: 0.6873 - val_loss: 0.0064 - val_accuracy: 0.9994 - val_precision: 0.8136 - val_recall: 0.7619 - val_auc: 0.9522 - val_prc: 0.6688\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0024 - accuracy: 0.9994 - precision: 0.8655 - recall: 0.7771 - auc: 0.9443 - prc: 0.7248 - val_loss: 0.0059 - val_accuracy: 0.9994 - val_precision: 0.8033 - val_recall: 0.7778 - val_auc: 0.9688 - val_prc: 0.7140\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0022 - accuracy: 0.9994 - precision: 0.8789 - recall: 0.7864 - auc: 0.9540 - prc: 0.7479 - val_loss: 0.0056 - val_accuracy: 0.9994 - val_precision: 0.8033 - val_recall: 0.7778 - val_auc: 0.9702 - val_prc: 0.7533\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0020 - accuracy: 0.9994 - precision: 0.8615 - recall: 0.7895 - auc: 0.9519 - prc: 0.7529 - val_loss: 0.0047 - val_accuracy: 0.9995 - val_precision: 0.8333 - val_recall: 0.7937 - val_auc: 0.9716 - val_prc: 0.7569\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0019 - accuracy: 0.9994 - precision: 0.8500 - recall: 0.7895 - auc: 0.9554 - prc: 0.7529 - val_loss: 0.0055 - val_accuracy: 0.9993 - val_precision: 0.7391 - val_recall: 0.8095 - val_auc: 0.9711 - val_prc: 0.7402\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0019 - accuracy: 0.9994 - precision: 0.8409 - recall: 0.8019 - auc: 0.9613 - prc: 0.7559 - val_loss: 0.0047 - val_accuracy: 0.9994 - val_precision: 0.7903 - val_recall: 0.7778 - val_auc: 0.9638 - val_prc: 0.7519\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8301 - recall: 0.8019 - auc: 0.9562 - prc: 0.7583 - val_loss: 0.0050 - val_accuracy: 0.9994 - val_precision: 0.7656 - val_recall: 0.7778 - val_auc: 0.9625 - val_prc: 0.7521\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8254 - recall: 0.8050 - auc: 0.9661 - prc: 0.7574 - val_loss: 0.0041 - val_accuracy: 0.9994 - val_precision: 0.8276 - val_recall: 0.7619 - val_auc: 0.9571 - val_prc: 0.7493\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8254 - recall: 0.8050 - auc: 0.9608 - prc: 0.7642 - val_loss: 0.0036 - val_accuracy: 0.9995 - val_precision: 0.8679 - val_recall: 0.7302 - val_auc: 0.9576 - val_prc: 0.7610\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8296 - recall: 0.7988 - auc: 0.9621 - prc: 0.7589 - val_loss: 0.0042 - val_accuracy: 0.9994 - val_precision: 0.7869 - val_recall: 0.7619 - val_auc: 0.9568 - val_prc: 0.7458\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8050 - recall: 0.8050 - auc: 0.9639 - prc: 0.7591 - val_loss: 0.0056 - val_accuracy: 0.9993 - val_precision: 0.7083 - val_recall: 0.8095 - val_auc: 0.9705 - val_prc: 0.7355\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8286 - recall: 0.8080 - auc: 0.9593 - prc: 0.7622 - val_loss: 0.0047 - val_accuracy: 0.9994 - val_precision: 0.7692 - val_recall: 0.7937 - val_auc: 0.9562 - val_prc: 0.7443\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8188 - recall: 0.8111 - auc: 0.9624 - prc: 0.7713 - val_loss: 0.0039 - val_accuracy: 0.9994 - val_precision: 0.8276 - val_recall: 0.7619 - val_auc: 0.9574 - val_prc: 0.7422\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8228 - recall: 0.8050 - auc: 0.9609 - prc: 0.7654 - val_loss: 0.0044 - val_accuracy: 0.9994 - val_precision: 0.7903 - val_recall: 0.7778 - val_auc: 0.9638 - val_prc: 0.7482\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8307 - recall: 0.8050 - auc: 0.9636 - prc: 0.7701 - val_loss: 0.0040 - val_accuracy: 0.9994 - val_precision: 0.7903 - val_recall: 0.7778 - val_auc: 0.9569 - val_prc: 0.7508\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8160 - recall: 0.8235 - auc: 0.9642 - prc: 0.7673 - val_loss: 0.0043 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9570 - val_prc: 0.7328\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0016 - accuracy: 0.9994 - precision: 0.8333 - recall: 0.8204 - auc: 0.9671 - prc: 0.7700 - val_loss: 0.0040 - val_accuracy: 0.9994 - val_precision: 0.7903 - val_recall: 0.7778 - val_auc: 0.9571 - val_prc: 0.7500\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0016 - accuracy: 0.9994 - precision: 0.8276 - recall: 0.8173 - auc: 0.9612 - prc: 0.7686 - val_loss: 0.0052 - val_accuracy: 0.9993 - val_precision: 0.7183 - val_recall: 0.8095 - val_auc: 0.9715 - val_prc: 0.7371\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9994 - precision: 0.8323 - recall: 0.8297 - auc: 0.9611 - prc: 0.7723 - val_loss: 0.0048 - val_accuracy: 0.9993 - val_precision: 0.7424 - val_recall: 0.7778 - val_auc: 0.9636 - val_prc: 0.7589\n",
            "1419/1419 [==============================] - 2s 2ms/step\n",
            "******************************for w =  0.16\n",
            "Model: \"sequential_30\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_30 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_30 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_30 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "84/89 [===========================>..] - ETA: 0s - loss: 0.0190 - accuracy: 0.9962 - precision: 0.1804 - recall: 0.3541 - auc: 0.6402 - prc: 0.3046"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 16ms/step - loss: 0.0182 - accuracy: 0.9963 - precision: 0.1922 - recall: 0.3679 - auc: 0.6498 - prc: 0.3174 - val_loss: 0.0104 - val_accuracy: 0.9994 - val_precision: 0.8182 - val_recall: 0.7143 - val_auc: 0.8940 - val_prc: 0.6416\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0031 - accuracy: 0.9993 - precision: 0.8655 - recall: 0.7368 - auc: 0.8980 - prc: 0.6844 - val_loss: 0.0074 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9543 - val_prc: 0.6867\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0025 - accuracy: 0.9994 - precision: 0.8720 - recall: 0.7802 - auc: 0.9440 - prc: 0.7238 - val_loss: 0.0054 - val_accuracy: 0.9994 - val_precision: 0.8167 - val_recall: 0.7778 - val_auc: 0.9626 - val_prc: 0.7316\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0022 - accuracy: 0.9994 - precision: 0.8803 - recall: 0.7740 - auc: 0.9515 - prc: 0.7449 - val_loss: 0.0054 - val_accuracy: 0.9994 - val_precision: 0.8033 - val_recall: 0.7778 - val_auc: 0.9706 - val_prc: 0.7544\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0021 - accuracy: 0.9994 - precision: 0.8610 - recall: 0.7864 - auc: 0.9558 - prc: 0.7454 - val_loss: 0.0040 - val_accuracy: 0.9996 - val_precision: 0.8909 - val_recall: 0.7778 - val_auc: 0.9572 - val_prc: 0.7506\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0020 - accuracy: 0.9994 - precision: 0.8439 - recall: 0.7864 - auc: 0.9524 - prc: 0.7473 - val_loss: 0.0047 - val_accuracy: 0.9994 - val_precision: 0.7812 - val_recall: 0.7937 - val_auc: 0.9643 - val_prc: 0.7525\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0019 - accuracy: 0.9994 - precision: 0.8339 - recall: 0.8080 - auc: 0.9527 - prc: 0.7490 - val_loss: 0.0042 - val_accuracy: 0.9994 - val_precision: 0.8167 - val_recall: 0.7778 - val_auc: 0.9570 - val_prc: 0.7518\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8464 - recall: 0.8019 - auc: 0.9603 - prc: 0.7552 - val_loss: 0.0041 - val_accuracy: 0.9994 - val_precision: 0.8167 - val_recall: 0.7778 - val_auc: 0.9572 - val_prc: 0.7531\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8525 - recall: 0.8050 - auc: 0.9607 - prc: 0.7585 - val_loss: 0.0044 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9568 - val_prc: 0.7518\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8323 - recall: 0.8142 - auc: 0.9623 - prc: 0.7621 - val_loss: 0.0041 - val_accuracy: 0.9994 - val_precision: 0.8136 - val_recall: 0.7619 - val_auc: 0.9568 - val_prc: 0.7513\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8344 - recall: 0.7957 - auc: 0.9608 - prc: 0.7564 - val_loss: 0.0048 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9560 - val_prc: 0.7315\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8497 - recall: 0.8050 - auc: 0.9563 - prc: 0.7661 - val_loss: 0.0040 - val_accuracy: 0.9994 - val_precision: 0.8000 - val_recall: 0.7619 - val_auc: 0.9570 - val_prc: 0.7478\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8365 - recall: 0.8080 - auc: 0.9609 - prc: 0.7582 - val_loss: 0.0034 - val_accuracy: 0.9995 - val_precision: 0.8545 - val_recall: 0.7460 - val_auc: 0.9580 - val_prc: 0.7630\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8291 - recall: 0.8111 - auc: 0.9626 - prc: 0.7624 - val_loss: 0.0038 - val_accuracy: 0.9994 - val_precision: 0.8136 - val_recall: 0.7619 - val_auc: 0.9573 - val_prc: 0.7479\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8290 - recall: 0.7957 - auc: 0.9628 - prc: 0.7746 - val_loss: 0.0035 - val_accuracy: 0.9994 - val_precision: 0.8276 - val_recall: 0.7619 - val_auc: 0.9580 - val_prc: 0.7460\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8489 - recall: 0.8173 - auc: 0.9598 - prc: 0.7663 - val_loss: 0.0044 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9568 - val_prc: 0.7411\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8037 - recall: 0.8111 - auc: 0.9628 - prc: 0.7682 - val_loss: 0.0040 - val_accuracy: 0.9994 - val_precision: 0.8033 - val_recall: 0.7778 - val_auc: 0.9574 - val_prc: 0.7402\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8429 - recall: 0.8142 - auc: 0.9643 - prc: 0.7754 - val_loss: 0.0044 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9572 - val_prc: 0.7049\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8376 - recall: 0.8142 - auc: 0.9596 - prc: 0.7634 - val_loss: 0.0049 - val_accuracy: 0.9994 - val_precision: 0.7812 - val_recall: 0.7937 - val_auc: 0.9570 - val_prc: 0.7108\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8413 - recall: 0.8204 - auc: 0.9599 - prc: 0.7799 - val_loss: 0.0044 - val_accuracy: 0.9994 - val_precision: 0.7869 - val_recall: 0.7619 - val_auc: 0.9562 - val_prc: 0.7547\n",
            "1419/1419 [==============================] - 2s 2ms/step\n",
            "******************************for w =  0.17\n",
            "Model: \"sequential_31\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_31 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_31 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_31 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89/89 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9978 - precision: 0.3455 - recall: 0.3420 - auc: 0.6499 - prc: 0.3059"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 4s 24ms/step - loss: 0.0186 - accuracy: 0.9978 - precision: 0.3455 - recall: 0.3420 - auc: 0.6499 - prc: 0.3059 - val_loss: 0.0109 - val_accuracy: 0.9993 - val_precision: 0.7963 - val_recall: 0.6825 - val_auc: 0.9040 - val_prc: 0.6424\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0031 - accuracy: 0.9993 - precision: 0.8681 - recall: 0.7337 - auc: 0.8882 - prc: 0.6787 - val_loss: 0.0062 - val_accuracy: 0.9994 - val_precision: 0.7869 - val_recall: 0.7619 - val_auc: 0.9483 - val_prc: 0.6728\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0025 - accuracy: 0.9994 - precision: 0.8799 - recall: 0.7709 - auc: 0.9386 - prc: 0.7194 - val_loss: 0.0050 - val_accuracy: 0.9994 - val_precision: 0.8167 - val_recall: 0.7778 - val_auc: 0.9632 - val_prc: 0.7278\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0022 - accuracy: 0.9994 - precision: 0.8838 - recall: 0.7771 - auc: 0.9499 - prc: 0.7381 - val_loss: 0.0048 - val_accuracy: 0.9994 - val_precision: 0.8167 - val_recall: 0.7778 - val_auc: 0.9713 - val_prc: 0.7524\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0021 - accuracy: 0.9994 - precision: 0.8694 - recall: 0.7833 - auc: 0.9505 - prc: 0.7472 - val_loss: 0.0041 - val_accuracy: 0.9996 - val_precision: 0.8909 - val_recall: 0.7778 - val_auc: 0.9649 - val_prc: 0.7535\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0020 - accuracy: 0.9994 - precision: 0.8620 - recall: 0.7926 - auc: 0.9543 - prc: 0.7514 - val_loss: 0.0043 - val_accuracy: 0.9995 - val_precision: 0.8448 - val_recall: 0.7778 - val_auc: 0.9644 - val_prc: 0.7529\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0019 - accuracy: 0.9994 - precision: 0.8459 - recall: 0.7988 - auc: 0.9525 - prc: 0.7496 - val_loss: 0.0041 - val_accuracy: 0.9994 - val_precision: 0.8167 - val_recall: 0.7778 - val_auc: 0.9573 - val_prc: 0.7507\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8492 - recall: 0.8019 - auc: 0.9577 - prc: 0.7587 - val_loss: 0.0039 - val_accuracy: 0.9994 - val_precision: 0.8364 - val_recall: 0.7302 - val_auc: 0.9572 - val_prc: 0.7650\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8444 - recall: 0.7895 - auc: 0.9635 - prc: 0.7621 - val_loss: 0.0046 - val_accuracy: 0.9994 - val_precision: 0.7692 - val_recall: 0.7937 - val_auc: 0.9562 - val_prc: 0.7512\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8275 - recall: 0.8019 - auc: 0.9595 - prc: 0.7596 - val_loss: 0.0034 - val_accuracy: 0.9994 - val_precision: 0.8654 - val_recall: 0.7143 - val_auc: 0.9503 - val_prc: 0.7634\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8285 - recall: 0.7926 - auc: 0.9641 - prc: 0.7612 - val_loss: 0.0037 - val_accuracy: 0.9994 - val_precision: 0.8246 - val_recall: 0.7460 - val_auc: 0.9575 - val_prc: 0.7652\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8529 - recall: 0.8080 - auc: 0.9627 - prc: 0.7607 - val_loss: 0.0045 - val_accuracy: 0.9994 - val_precision: 0.7692 - val_recall: 0.7937 - val_auc: 0.9563 - val_prc: 0.7492\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8520 - recall: 0.8019 - auc: 0.9627 - prc: 0.7704 - val_loss: 0.0037 - val_accuracy: 0.9994 - val_precision: 0.8000 - val_recall: 0.7619 - val_auc: 0.9580 - val_prc: 0.7463\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8419 - recall: 0.8080 - auc: 0.9613 - prc: 0.7649 - val_loss: 0.0037 - val_accuracy: 0.9994 - val_precision: 0.8136 - val_recall: 0.7619 - val_auc: 0.9575 - val_prc: 0.7570\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8275 - recall: 0.8019 - auc: 0.9598 - prc: 0.7728 - val_loss: 0.0038 - val_accuracy: 0.9994 - val_precision: 0.8000 - val_recall: 0.7619 - val_auc: 0.9574 - val_prc: 0.7441\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8239 - recall: 0.8111 - auc: 0.9627 - prc: 0.7704 - val_loss: 0.0037 - val_accuracy: 0.9994 - val_precision: 0.8136 - val_recall: 0.7619 - val_auc: 0.9575 - val_prc: 0.7450\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8479 - recall: 0.8111 - auc: 0.9599 - prc: 0.7752 - val_loss: 0.0040 - val_accuracy: 0.9994 - val_precision: 0.8033 - val_recall: 0.7778 - val_auc: 0.9572 - val_prc: 0.7413\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9994 - precision: 0.8618 - recall: 0.8111 - auc: 0.9647 - prc: 0.7860 - val_loss: 0.0057 - val_accuracy: 0.9991 - val_precision: 0.6494 - val_recall: 0.7937 - val_auc: 0.9631 - val_prc: 0.7261\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8137 - recall: 0.8111 - auc: 0.9654 - prc: 0.7933 - val_loss: 0.0030 - val_accuracy: 0.9994 - val_precision: 0.8393 - val_recall: 0.7460 - val_auc: 0.9508 - val_prc: 0.7644\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0016 - accuracy: 0.9994 - precision: 0.8484 - recall: 0.8142 - auc: 0.9630 - prc: 0.7983 - val_loss: 0.0048 - val_accuracy: 0.9993 - val_precision: 0.7042 - val_recall: 0.7937 - val_auc: 0.9565 - val_prc: 0.7493\n",
            "1419/1419 [==============================] - 2s 2ms/step\n",
            "******************************for w =  0.18000000000000002\n",
            "Model: \"sequential_32\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_32 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_32 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_32 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89/89 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9944 - precision: 0.1038 - recall: 0.3005 - auc: 0.6170 - prc: 0.2560"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 19ms/step - loss: 0.0204 - accuracy: 0.9944 - precision: 0.1038 - recall: 0.3005 - auc: 0.6170 - prc: 0.2560 - val_loss: 0.0100 - val_accuracy: 0.9993 - val_precision: 0.8298 - val_recall: 0.6190 - val_auc: 0.8773 - val_prc: 0.6572\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0033 - accuracy: 0.9993 - precision: 0.8755 - recall: 0.7183 - auc: 0.8933 - prc: 0.6747 - val_loss: 0.0056 - val_accuracy: 0.9994 - val_precision: 0.8246 - val_recall: 0.7460 - val_auc: 0.9347 - val_prc: 0.6840\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0026 - accuracy: 0.9994 - precision: 0.8813 - recall: 0.7585 - auc: 0.9396 - prc: 0.7072 - val_loss: 0.0054 - val_accuracy: 0.9994 - val_precision: 0.8033 - val_recall: 0.7778 - val_auc: 0.9615 - val_prc: 0.7264\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0023 - accuracy: 0.9994 - precision: 0.8780 - recall: 0.7802 - auc: 0.9514 - prc: 0.7440 - val_loss: 0.0047 - val_accuracy: 0.9995 - val_precision: 0.8305 - val_recall: 0.7778 - val_auc: 0.9710 - val_prc: 0.7498\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0021 - accuracy: 0.9994 - precision: 0.8842 - recall: 0.7802 - auc: 0.9495 - prc: 0.7469 - val_loss: 0.0046 - val_accuracy: 0.9995 - val_precision: 0.8448 - val_recall: 0.7778 - val_auc: 0.9717 - val_prc: 0.7539\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0020 - accuracy: 0.9994 - precision: 0.8673 - recall: 0.7895 - auc: 0.9500 - prc: 0.7513 - val_loss: 0.0043 - val_accuracy: 0.9995 - val_precision: 0.8571 - val_recall: 0.7619 - val_auc: 0.9644 - val_prc: 0.7522\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0020 - accuracy: 0.9994 - precision: 0.8591 - recall: 0.7926 - auc: 0.9511 - prc: 0.7502 - val_loss: 0.0039 - val_accuracy: 0.9995 - val_precision: 0.8421 - val_recall: 0.7619 - val_auc: 0.9576 - val_prc: 0.7511\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0019 - accuracy: 0.9994 - precision: 0.8562 - recall: 0.7926 - auc: 0.9546 - prc: 0.7525 - val_loss: 0.0038 - val_accuracy: 0.9995 - val_precision: 0.8421 - val_recall: 0.7619 - val_auc: 0.9579 - val_prc: 0.7498\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0019 - accuracy: 0.9994 - precision: 0.8591 - recall: 0.7926 - auc: 0.9534 - prc: 0.7566 - val_loss: 0.0043 - val_accuracy: 0.9994 - val_precision: 0.7812 - val_recall: 0.7937 - val_auc: 0.9568 - val_prc: 0.7527\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8409 - recall: 0.8019 - auc: 0.9581 - prc: 0.7541 - val_loss: 0.0045 - val_accuracy: 0.9994 - val_precision: 0.7812 - val_recall: 0.7937 - val_auc: 0.9563 - val_prc: 0.7509\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8429 - recall: 0.8142 - auc: 0.9595 - prc: 0.7590 - val_loss: 0.0045 - val_accuracy: 0.9994 - val_precision: 0.7692 - val_recall: 0.7937 - val_auc: 0.9563 - val_prc: 0.7449\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8360 - recall: 0.8050 - auc: 0.9582 - prc: 0.7578 - val_loss: 0.0035 - val_accuracy: 0.9995 - val_precision: 0.8571 - val_recall: 0.7619 - val_auc: 0.9577 - val_prc: 0.7660\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8286 - recall: 0.8080 - auc: 0.9583 - prc: 0.7754 - val_loss: 0.0036 - val_accuracy: 0.9994 - val_precision: 0.8246 - val_recall: 0.7460 - val_auc: 0.9578 - val_prc: 0.7460\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8609 - recall: 0.8050 - auc: 0.9613 - prc: 0.7658 - val_loss: 0.0033 - val_accuracy: 0.9995 - val_precision: 0.8704 - val_recall: 0.7460 - val_auc: 0.9581 - val_prc: 0.7624\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8393 - recall: 0.7926 - auc: 0.9629 - prc: 0.7818 - val_loss: 0.0044 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9565 - val_prc: 0.7397\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8403 - recall: 0.8142 - auc: 0.9642 - prc: 0.7776 - val_loss: 0.0034 - val_accuracy: 0.9994 - val_precision: 0.8393 - val_recall: 0.7460 - val_auc: 0.9583 - val_prc: 0.7588\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8365 - recall: 0.8080 - auc: 0.9585 - prc: 0.7740 - val_loss: 0.0033 - val_accuracy: 0.9995 - val_precision: 0.8704 - val_recall: 0.7460 - val_auc: 0.9580 - val_prc: 0.7608\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8392 - recall: 0.8080 - auc: 0.9617 - prc: 0.7722 - val_loss: 0.0036 - val_accuracy: 0.9994 - val_precision: 0.8276 - val_recall: 0.7619 - val_auc: 0.9575 - val_prc: 0.7548\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8459 - recall: 0.7988 - auc: 0.9659 - prc: 0.7907 - val_loss: 0.0039 - val_accuracy: 0.9994 - val_precision: 0.7903 - val_recall: 0.7778 - val_auc: 0.9573 - val_prc: 0.7483\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8386 - recall: 0.8204 - auc: 0.9570 - prc: 0.7969 - val_loss: 0.0035 - val_accuracy: 0.9994 - val_precision: 0.8276 - val_recall: 0.7619 - val_auc: 0.9576 - val_prc: 0.7664\n",
            "1419/1419 [==============================] - 2s 2ms/step\n",
            "******************************for w =  0.19000000000000003\n",
            "Model: \"sequential_33\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_33 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_33 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_33 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89/89 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9928 - precision: 0.0779 - recall: 0.3005 - auc: 0.6307 - prc: 0.2678"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 14ms/step - loss: 0.0208 - accuracy: 0.9928 - precision: 0.0779 - recall: 0.3005 - auc: 0.6307 - prc: 0.2678 - val_loss: 0.0112 - val_accuracy: 0.9993 - val_precision: 0.8163 - val_recall: 0.6349 - val_auc: 0.8744 - val_prc: 0.6373\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0034 - accuracy: 0.9993 - precision: 0.8783 - recall: 0.7152 - auc: 0.8870 - prc: 0.6713 - val_loss: 0.0066 - val_accuracy: 0.9994 - val_precision: 0.7833 - val_recall: 0.7460 - val_auc: 0.9438 - val_prc: 0.6665\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0027 - accuracy: 0.9994 - precision: 0.8781 - recall: 0.7585 - auc: 0.9413 - prc: 0.7099 - val_loss: 0.0056 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9628 - val_prc: 0.7010\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0023 - accuracy: 0.9994 - precision: 0.8838 - recall: 0.7771 - auc: 0.9488 - prc: 0.7393 - val_loss: 0.0043 - val_accuracy: 0.9995 - val_precision: 0.8596 - val_recall: 0.7778 - val_auc: 0.9644 - val_prc: 0.7407\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0021 - accuracy: 0.9994 - precision: 0.8776 - recall: 0.7771 - auc: 0.9495 - prc: 0.7401 - val_loss: 0.0043 - val_accuracy: 0.9996 - val_precision: 0.8909 - val_recall: 0.7778 - val_auc: 0.9645 - val_prc: 0.7492\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0020 - accuracy: 0.9994 - precision: 0.8715 - recall: 0.7771 - auc: 0.9501 - prc: 0.7506 - val_loss: 0.0037 - val_accuracy: 0.9995 - val_precision: 0.8846 - val_recall: 0.7302 - val_auc: 0.9579 - val_prc: 0.7637\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0020 - accuracy: 0.9994 - precision: 0.8477 - recall: 0.7926 - auc: 0.9516 - prc: 0.7490 - val_loss: 0.0042 - val_accuracy: 0.9994 - val_precision: 0.8167 - val_recall: 0.7778 - val_auc: 0.9570 - val_prc: 0.7515\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0019 - accuracy: 0.9994 - precision: 0.8523 - recall: 0.7864 - auc: 0.9549 - prc: 0.7491 - val_loss: 0.0039 - val_accuracy: 0.9994 - val_precision: 0.8276 - val_recall: 0.7619 - val_auc: 0.9578 - val_prc: 0.7484\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8387 - recall: 0.8050 - auc: 0.9566 - prc: 0.7531 - val_loss: 0.0035 - val_accuracy: 0.9995 - val_precision: 0.8679 - val_recall: 0.7302 - val_auc: 0.9501 - val_prc: 0.7655\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8591 - recall: 0.7926 - auc: 0.9596 - prc: 0.7558 - val_loss: 0.0042 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9568 - val_prc: 0.7440\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8487 - recall: 0.7988 - auc: 0.9581 - prc: 0.7553 - val_loss: 0.0038 - val_accuracy: 0.9994 - val_precision: 0.8000 - val_recall: 0.7619 - val_auc: 0.9574 - val_prc: 0.7487\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8586 - recall: 0.8080 - auc: 0.9599 - prc: 0.7701 - val_loss: 0.0039 - val_accuracy: 0.9994 - val_precision: 0.7869 - val_recall: 0.7619 - val_auc: 0.9573 - val_prc: 0.7439\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8399 - recall: 0.7957 - auc: 0.9617 - prc: 0.7668 - val_loss: 0.0046 - val_accuracy: 0.9994 - val_precision: 0.7576 - val_recall: 0.7937 - val_auc: 0.9562 - val_prc: 0.7469\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8469 - recall: 0.8050 - auc: 0.9613 - prc: 0.7796 - val_loss: 0.0040 - val_accuracy: 0.9994 - val_precision: 0.8065 - val_recall: 0.7937 - val_auc: 0.9578 - val_prc: 0.7365\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8482 - recall: 0.7957 - auc: 0.9617 - prc: 0.7670 - val_loss: 0.0042 - val_accuracy: 0.9994 - val_precision: 0.7903 - val_recall: 0.7778 - val_auc: 0.9574 - val_prc: 0.7038\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8442 - recall: 0.8050 - auc: 0.9586 - prc: 0.7794 - val_loss: 0.0042 - val_accuracy: 0.9993 - val_precision: 0.7500 - val_recall: 0.7619 - val_auc: 0.9567 - val_prc: 0.7388\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8371 - recall: 0.8111 - auc: 0.9556 - prc: 0.7838 - val_loss: 0.0044 - val_accuracy: 0.9993 - val_precision: 0.7273 - val_recall: 0.7619 - val_auc: 0.9565 - val_prc: 0.7574\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8502 - recall: 0.8080 - auc: 0.9614 - prc: 0.7916 - val_loss: 0.0035 - val_accuracy: 0.9994 - val_precision: 0.8246 - val_recall: 0.7460 - val_auc: 0.9578 - val_prc: 0.7583\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8553 - recall: 0.8050 - auc: 0.9619 - prc: 0.7969 - val_loss: 0.0035 - val_accuracy: 0.9994 - val_precision: 0.8136 - val_recall: 0.7619 - val_auc: 0.9578 - val_prc: 0.7573\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8429 - recall: 0.8142 - auc: 0.9616 - prc: 0.7822 - val_loss: 0.0034 - val_accuracy: 0.9994 - val_precision: 0.8136 - val_recall: 0.7619 - val_auc: 0.9582 - val_prc: 0.7622\n",
            "1419/1419 [==============================] - 2s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification Report for various weight distributions"
      ],
      "metadata": {
        "id": "QozWxQs_wO9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(validation_preds)):\n",
        "  print(\"CLASSIFICATION REPORT FOR W=\", 0.01 +i*(0.01))\n",
        "  print(classification_report(y_val, validation_preds[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3fFzMWIwNgb",
        "outputId": "90b61b9f-d1e6-4303-ebaa-e07b7a8d9bca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLASSIFICATION REPORT FOR W= 0.01\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.46      0.81      0.58        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.73      0.90      0.79     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n",
            "CLASSIFICATION REPORT FOR W= 0.02\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.44      0.83      0.57        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.72      0.91      0.79     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n",
            "CLASSIFICATION REPORT FOR W= 0.03\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.47      0.84      0.60        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.73      0.92      0.80     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n",
            "CLASSIFICATION REPORT FOR W= 0.04\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.48      0.81      0.60        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.74      0.90      0.80     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n",
            "CLASSIFICATION REPORT FOR W= 0.05\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.52      0.81      0.63        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.76      0.90      0.82     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n",
            "CLASSIFICATION REPORT FOR W= 0.060000000000000005\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.74      0.79      0.76        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.87      0.90      0.88     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n",
            "CLASSIFICATION REPORT FOR W= 0.06999999999999999\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.69      0.79      0.74        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.85      0.90      0.87     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n",
            "CLASSIFICATION REPORT FOR W= 0.08\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.68      0.79      0.73        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.84      0.90      0.86     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n",
            "CLASSIFICATION REPORT FOR W= 0.09\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.79      0.78      0.78        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.90      0.89      0.89     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n",
            "CLASSIFICATION REPORT FOR W= 0.09999999999999999\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.72      0.78      0.75        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.86      0.89      0.87     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n",
            "CLASSIFICATION REPORT FOR W= 0.11\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.81      0.79      0.80        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.90      0.90      0.90     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n",
            "CLASSIFICATION REPORT FOR W= 0.12\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.76      0.79      0.78        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.88      0.90      0.89     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n",
            "CLASSIFICATION REPORT FOR W= 0.13\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.84      0.75      0.79        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.92      0.87      0.89     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n",
            "CLASSIFICATION REPORT FOR W= 0.14\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.74      0.78      0.76        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.87      0.89      0.88     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n",
            "CLASSIFICATION REPORT FOR W= 0.15000000000000002\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.79      0.76      0.77        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.89      0.88      0.89     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n",
            "CLASSIFICATION REPORT FOR W= 0.16\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.70      0.79      0.75        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.85      0.90      0.87     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n",
            "CLASSIFICATION REPORT FOR W= 0.17\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.83      0.76      0.79        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.91      0.88      0.90     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n",
            "CLASSIFICATION REPORT FOR W= 0.18000000000000002\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.81      0.76      0.79        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.91      0.88      0.89     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_cust_val_preds = cnn_cust.predict(X_val_norm)>0.5\n",
        "cnn_cust_test_preds= cnn_cust.predict(X_test_norm)>0.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czNZM9cKVoqA",
        "outputId": "8322e6fb-948a-43d6-cdc8-14756c6841ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1419/1419 [==============================] - 4s 3ms/step\n",
            "1774/1774 [==============================] - 3s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_results(y_val,cnn_cust_val_preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "id": "ohc4K9gKh0Oq",
        "outputId": "12b00e17-bcce-484d-a234-ac35fa82f859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.71      0.79      0.75        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.86      0.90      0.88     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgwAAAGwCAYAAADFZj2cAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA5iUlEQVR4nO3de3gU9dn/8c8mIZsD2UBAEgPhYKNAKgcJGmOrQo1EpQqKj2jRRgR8xECBKAJVTqLiD+sBBMWKGu0jFdRCBRRKQRAkigRjEQEFQQ4hAYpJIJBssju/PzBbt0E3y2xYknm/rmuuh535zsy9vXjcm/v+zndshmEYAgAA+BkhwQ4AAACc+0gYAACATyQMAADAJxIGAADgEwkDAADwiYQBAAD4RMIAAAB8Cgt2AGa43W4VFhYqJiZGNpst2OEAAPxkGIaOHTumxMREhYTU379hKyoq5HQ6TV8nPDxcERERAYio4WnQCUNhYaGSkpKCHQYAwKR9+/apTZs29XLtiooKdWjXVEWHXKavlZCQoN27d1syaWjQCUNMTIwk6bvN7eVoSncFjdPNF3UJdghAvalWldbrfc9/z+uD0+lU0SGXvstvL0fMmf9WlB1zq13qHjmdThKGhqamDeFoGmLqLwFwLguzNQl2CED9+eHlBGejrdw0xqamMWd+H7es3fpu0AkDAAB15TLccpl4e5LLcAcumAaIhAEAYAluGXLrzDMGM+c2BtTxAQCAT1QYAACW4JZbZpoK5s5u+EgYAACW4DIMuYwzbyuYObcxoCUBAAB8osIAALAEJj2aQ8IAALAEtwy5SBjOGC0JAADgExUGAIAl0JIwh4QBAGAJPCVhDi0JAADgExUGAIAluH/YzJxvZSQMAABLcJl8SsLMuY0BCQMAwBJchky+rTJwsTREzGEAAAA+UWEAAFgCcxjMIWEAAFiCWza5ZDN1vpXRkgAAAD5RYQAAWILbOLWZOd/KSBgAAJbgMtmSMHNuY0BLAgAA+ESFAQBgCVQYzCFhAABYgtuwyW2YeErCxLmNAS0JAADgExUGAIAl0JIwh4QBAGAJLoXIZaKw7gpgLA0RCQMAwBIMk3MYDOYwAAAA/DwqDAAAS2AOgzkkDAAAS3AZIXIZJuYwWHxpaFoSAADAJyoMAABLcMsmt4l/J7tl7RIDCQMAwBKYw2AOLQkAAOATFQYAgCWYn/RISwIAgEbv1BwGEy+foiUBAADq05NPPimbzabRo0d79lVUVCg7O1stWrRQ06ZNNWDAABUXF3udt3fvXvXt21dRUVFq1aqVxo4dq+rqaq8xa9asUY8ePWS325WcnKzc3Nxa958zZ47at2+viIgIpaWlaePGjX5/BxIGAIAluH94l8SZbmf6hMVnn32ml156SV27dvXaP2bMGC1ZskRvv/221q5dq8LCQt1yyy2e4y6XS3379pXT6dSGDRv0+uuvKzc3V5MmTfKM2b17t/r27avevXuroKBAo0eP1tChQ7VixQrPmAULFignJ0eTJ0/W5s2b1a1bN2VmZurQoUN+fQ8SBgCAJdTMYTCzSVJZWZnXVllZ+ZP3PH78uAYNGqSXX35ZzZs39+wvLS3VK6+8omeeeUa/+c1vlJqaqtdee00bNmzQJ598Ikn6xz/+oa+++kr/93//p+7du+v666/XtGnTNGfOHDmdTknS3Llz1aFDBz399NPq3LmzRowYoVtvvVXPPvus517PPPOMhg0bpsGDByslJUVz585VVFSUXn31Vb/+9yNhAABYgvuHKoGZTZKSkpIUGxvr2aZPn/6T98zOzlbfvn2VkZHhtT8/P19VVVVe+zt16qS2bdsqLy9PkpSXl6cuXbooPj7eMyYzM1NlZWXaunWrZ8x/XzszM9NzDafTqfz8fK8xISEhysjI8IypKyY9AgDgh3379snhcHg+2+3204576623tHnzZn322We1jhUVFSk8PFzNmjXz2h8fH6+ioiLPmB8nCzXHa4793JiysjKdPHlS33//vVwu12nHbN++vQ7f9j9IGAAAluAybHKZeEV1zbkOh8MrYTidffv2adSoUVq5cqUiIiLO+J7nEloSAABLMDPhsWarq/z8fB06dEg9evRQWFiYwsLCtHbtWs2aNUthYWGKj4+X0+lUSUmJ13nFxcVKSEiQJCUkJNR6aqLms68xDodDkZGRatmypUJDQ087puYadUXCAABAgF1zzTXasmWLCgoKPFvPnj01aNAgz5+bNGmiVatWec7ZsWOH9u7dq/T0dElSenq6tmzZ4vU0w8qVK+VwOJSSkuIZ8+Nr1IypuUZ4eLhSU1O9xrjdbq1atcozpq5oSQAALMFthMhtYqVHtx8rPcbExOjiiy/22hcdHa0WLVp49g8ZMkQ5OTmKi4uTw+HQyJEjlZ6erssvv1yS1KdPH6WkpOiuu+7SjBkzVFRUpEceeUTZ2dmeeRP33XefZs+erYceekj33HOPVq9erYULF2rZsmWe++bk5CgrK0s9e/bUZZddpueee07l5eUaPHiwX9+fhAEAYAn+thVqnx/YpaGfffZZhYSEaMCAAaqsrFRmZqZeeOEFz/HQ0FAtXbpUw4cPV3p6uqKjo5WVlaVHH33UM6ZDhw5atmyZxowZo5kzZ6pNmzaaN2+eMjMzPWMGDhyow4cPa9KkSSoqKlL37t21fPnyWhMhfbEZRsNdHLusrEyxsbH6/usL5Iihu4LGKTOxe7BDAOpNtVGlNfq7SktLfU4kPFM1vxUvb05VVEzoGV/nxDGXhvXIr9dYz2VUGAAAluCWTD0l4Q5cKA0SCQMAwBJ+vPjSmZ5vZdb+9gAAoE6oMAAALOHH74M40/OtjIQBAGAJbtnklpk5DGd+bmNAwgAAsAQqDOZY+9sDAIA6ocIAALAE8ws3Wfvf2CQMAABLcBs2uc2sw2Di3MbA2ukSAACoEyoMAABLcJtsSVh94SYSBgCAJZh/W6W1EwZrf3sAAFAnVBgAAJbgkk0uE4svmTm3MSBhAABYAi0Jc6z97QEAQJ1QYQAAWIJL5toKrsCF0iCRMAAALIGWhDkkDAAAS+DlU+ZY+9sDAIA6ocIAALAEQza5TcxhMHisEgCAxo+WhDnW/vYAAKBOqDAAACyB11ubQ8IAALAEl8m3VZo5tzGw9rcHAAB1QoUBAGAJtCTMIWEAAFiCWyFymyismzm3MbD2twcAAHVChQEAYAkuwyaXibaCmXMbAxIGAIAlMIfBHBIGAIAlGCbfVmmw0iMAAMDPo8IAALAEl2xymXiBlJlzGwMSBgCAJbgNc/MQ3EYAg2mAaEkAAACfqDBYyILnW+nV6YnqP/Swhj96QJI0dkCy/pXX1GvcDXcd0aj/t1+SVHY0VE+OaKfd2yJ17PtQxbaoVnpmqQZPOKjoGLck6d/FYfrz1Nb65l+RKtxtV78hRzzXr7H+/Vi9NStehXvsqq6SWndwasB9h5Rx6/dn4ZsD3gaOKNavbihVUnKlnBUh+mpTlF55/Hzt3xXhGdPE7ta9kwvV66YSNbEbyl8To+cntFbJkSZBjBxmuE1OejRzbmNAwmAROwoitez/WqhDyslax64fdES/H1vk+WyPdHv+bAuR0jNLdfe4g4ptUa3C3XbN/mMbHSsJ04QXvpMkVTlD1KxFte4YVaxFfz7vtPePaebSHaOKlZRcobAmhj79p0NPj2mrZi2r1bPXsQB/W+DndU0v15Lclvq6IEqhYYbuHn9QT/z1Ww27uqMqT4ZKku6bUqjLMsr02P+2U3lZqLIfP6BJr+xRTr8Lgxw9zpRbNrlNzEMwc25jcE6kS3PmzFH79u0VERGhtLQ0bdy4MdghNSony0P0/0a00+in9ikm1lXruD3SUFyras9WUzmQTv3Q35j1b13U7aTi21TpkiuP68asI/ry02jPmIQkp4ZPO6Br/+d7RTvcta4vSd2uOK5fXV+qthdWKrG9UzcPPaILOp/U1o3Rpx0P1KeHB12glQvj9N3XEfr2q0g9Pbqt4ttU6cKupxLqqBiXMu84qpemJOqLj2O0c0uUnslJ0i8vPaFOPcqDHD0QHEFPGBYsWKCcnBxNnjxZmzdvVrdu3ZSZmalDhw4FO7RGY/Yf2+iya8rU46rjpz3+4d+a639+ebHu7d1Rrz5xvipO/HQW/e+iMH38QTN1TT/9terCMKTP1zXVvl12XZx25tcBAiXacSqRPlZyqrpwYdcTahJu6PN1MZ4x+3ZGqHh/E3VOPRGUGGFezUqPZjYrC3pL4plnntGwYcM0ePBgSdLcuXO1bNkyvfrqqxo/fnyQo2v41ixupp1bIvX8+1+f9njvm79XqzZOtYiv0u5tkT/0ce2a9Moer3HTh7dT3opYVVaE6PJrSzXmT/v8jqW8LES/6/FLVTlDFBJqaOQT+5V6NQkDgstmM3Tf1AP6cmOUvtsRKUmKa1UtZ6VN5WWhXmNLDocprlVVMMJEADCHwZygJgxOp1P5+fmaMGGCZ19ISIgyMjKUl5dXa3xlZaUqKys9n8vKys5KnA3VoQNN9OKk1pr+1i6FR5z+eaAb7vy3588dOlcorlWVxt2WrMI94Ups7/Qc+9+pBzQop0gHvrXr1enn66WprTVy+n6/4ols6tYLK3eoojxUn69vqpemtlZCO6e6XUHSgOAZ8cQBtetUoQf6Jwc7FOCcFtSE4ciRI3K5XIqPj/faHx8fr+3bt9caP336dE2dOvVshdfg7fxXlEqONFF2ZkfPPrfLpi2fROu911pq6Z4vFOr9Dyh16nGq3Fq4x+6VMNTMb2h7YaVimrn0wM0X6neji9QivrrO8YSEnHo6QpJ+cfFJ7fsmQgueb0XCgKDJfny/0q4t0wM3/0JHDoZ79h89FKZwu6Foh8urytDsvGodPcRTEg2VWybfJWHxSY9Bb0n4Y8KECcrJyfF8LisrU1JSUhAjOrd1v/KYXlrtnXg9PaatkpIrdFv2oVrJgiTt+rKmJPvTZVfjh2JFldNcec7tNn8N4MwYyn78gK64rlRjb01W8T6719Fv/hWlKqdNl/z6mNa/30yS1OYXFYpvU6Vt+VFBiBeBYJh8SsIgYQieli1bKjQ0VMXFxV77i4uLlZCQUGu83W6X3W6vtR+nF9XUrfadKrz2RUS5FdPcpfadKlS4J1wfLmquy64pU0xzl3Z/FaGXprRWl8uP64KUU+dtXBWj7w83UcfuJxQR7dZ3OyI0b1qifnnpcSUk/acCUZNonCwPUem/Q7Xry0iFhbvV7qJTLaS3nm+lC7ueUGJ7p6qcNm1c5dCqd+M0crr/cyEAs0Y8cUC9b/5eUwZ30MnjIWp+3qkEufxYqJwVITpxLFQr/hqne6cU6lhJmMqPhSj78QP6alOUtm/myZ6GirdVmhPUhCE8PFypqalatWqV+vfvL0lyu91atWqVRowYEczQLCGsyalZ4IvmnaeKEyE6L7FKv76hRHeM/k8CFx5h6IM3W+ilKa1V5bTpvESnfnV9qQaO8H6K5f4+/2l7fPOvKH24KE7xbZx6Y+NXkqSKEyGa/cckHTnYROERbiX9olIPPf+devUrOSvfFfixG+8+NXfnT3/b5bX/T6OTtHJhnCRp7pREuQ1p4st71MRuaNOaGM2e0PqsxwqcK2yGYQR1dewFCxYoKytLL730ki677DI999xzWrhwobZv315rbsN/KysrU2xsrL7/+gI5Yihto3HKTOwe7BCAelNtVGmN/q7S0lI5HI56uUfNb8XNKwerSXS47xN+QlW5U4uufa1eYz2XBX0Ow8CBA3X48GFNmjRJRUVF6t69u5YvX+4zWQAAwB+0JMwJesIgSSNGjKAFAQDAOeycSBgAAKhvvEvCHBIGAIAl0JIwh5mCAADAJyoMAABLoMJgDgkDAMASSBjMoSUBAAB8osIAALAEKgzmkDAAACzBkLlHI4O6LPI5gIQBAGAJVBjMYQ4DAADwiQoDAMASqDCYQ8IAALAEEgZzaEkAAACfqDAAACyBCoM5JAwAAEswDJsMEz/6Zs5tDGhJAAAAn6gwAAAswS2bqYWbzJzbGJAwAAAsgTkM5tCSAAAAPlFhAABYApMezSFhAABYAi0Jc2hJAAAsoabCYGbzx4svvqiuXbvK4XDI4XAoPT1dH3zwged4RUWFsrOz1aJFCzVt2lQDBgxQcXGx1zX27t2rvn37KioqSq1atdLYsWNVXV3tNWbNmjXq0aOH7Ha7kpOTlZubWyuWOXPmqH379oqIiFBaWpo2btzo13eRSBgAAKgXbdq00ZNPPqn8/Hxt2rRJv/nNb9SvXz9t3bpVkjRmzBgtWbJEb7/9ttauXavCwkLdcsstnvNdLpf69u0rp9OpDRs26PXXX1dubq4mTZrkGbN792717dtXvXv3VkFBgUaPHq2hQ4dqxYoVnjELFixQTk6OJk+erM2bN6tbt27KzMzUoUOH/Po+NsMwGuwrvsvKyhQbG6vvv75AjhhyHzROmYndgx0CUG+qjSqt0d9VWloqh8NRL/eo+a3o8U6OQqPtZ3wdV3mlNt/6jKlY4+Li9NRTT+nWW2/Veeedp/nz5+vWW2+VJG3fvl2dO3dWXl6eLr/8cn3wwQf67W9/q8LCQsXHx0uS5s6dq3Hjxunw4cMKDw/XuHHjtGzZMn355Zeee9x+++0qKSnR8uXLJUlpaWm69NJLNXv2bEmS2+1WUlKSRo4cqfHjx9c5dn5lAQCWYEgyDBPbD9cpKyvz2iorK33e2+Vy6a233lJ5ebnS09OVn5+vqqoqZWRkeMZ06tRJbdu2VV5eniQpLy9PXbp08SQLkpSZmamysjJPlSIvL8/rGjVjaq7hdDqVn5/vNSYkJEQZGRmeMXVFwgAAgB+SkpIUGxvr2aZPn/6TY7ds2aKmTZvKbrfrvvvu06JFi5SSkqKioiKFh4erWbNmXuPj4+NVVFQkSSoqKvJKFmqO1xz7uTFlZWU6efKkjhw5IpfLddoxNdeoK56SAABYgls22QKw0uO+ffu8WhJ2+0+3OTp27KiCggKVlpbqnXfeUVZWltauXXvGMQQTCQMAwBICtQ5DzVMPdREeHq7k5GRJUmpqqj777DPNnDlTAwcOlNPpVElJiVeVobi4WAkJCZKkhISEWk8z1DxF8eMx//1kRXFxsRwOhyIjIxUaGqrQ0NDTjqm5Rl3RkgAA4Cxxu92qrKxUamqqmjRpolWrVnmO7dixQ3v37lV6erokKT09XVu2bPF6mmHlypVyOBxKSUnxjPnxNWrG1FwjPDxcqampXmPcbrdWrVrlGVNXVBgAAJbgNmyyncWFmyZMmKDrr79ebdu21bFjxzR//nytWbNGK1asUGxsrIYMGaKcnBzFxcXJ4XBo5MiRSk9P1+WXXy5J6tOnj1JSUnTXXXdpxowZKioq0iOPPKLs7GxPG+S+++7T7Nmz9dBDD+mee+7R6tWrtXDhQi1btswTR05OjrKystSzZ09ddtlleu6551ReXq7Bgwf79X1IGAAAllDztIOZ8/1x6NAh/f73v9fBgwcVGxurrl27asWKFbr22mslSc8++6xCQkI0YMAAVVZWKjMzUy+88ILn/NDQUC1dulTDhw9Xenq6oqOjlZWVpUcffdQzpkOHDlq2bJnGjBmjmTNnqk2bNpo3b54yMzM9YwYOHKjDhw9r0qRJKioqUvfu3bV8+fJaEyF9YR0G4BzHOgxozM7mOgy/XDBWoVEm1mE4UamtA5+q11jPZVQYAACWwMunzCFhAABYAgmDOSQMAABLONuTHhsbGv8AAMAnKgwAAEs4209JNDYkDAAASziVMJiZwxDAYBogWhIAAMAnKgwAAEvgKQlzSBgAAJZg/LCZOd/KaEkAAACfqDAAACyBloQ5JAwAAGugJ2EKCQMAwBpMVhhk8QoDcxgAAIBPVBgAAJbASo/mkDAAACyBSY/m0JIAAAA+UWEAAFiDYTM3cdHiFQYSBgCAJTCHwRxaEgAAwCcqDAAAa2DhJlNIGAAAlsBTEubUKWF477336nzBm2666YyDAQAA56Y6JQz9+/ev08VsNptcLpeZeAAAqD8WbyuYUaeEwe1213ccAADUK1oS5ph6SqKioiJQcQAAUL+MAGwW5nfC4HK5NG3aNLVu3VpNmzbVt99+K0maOHGiXnnllYAHCAAAgs/vhOHxxx9Xbm6uZsyYofDwcM/+iy++WPPmzQtocAAABI4tAJt1+Z0wvPHGG/rzn/+sQYMGKTQ01LO/W7du2r59e0CDAwAgYGhJmOJ3wnDgwAElJyfX2u92u1VVVRWQoAAAwLnF74QhJSVF69atq7X/nXfe0SWXXBKQoAAACDgqDKb4vdLjpEmTlJWVpQMHDsjtdutvf/ubduzYoTfeeENLly6tjxgBADCPt1Wa4neFoV+/flqyZIn++c9/Kjo6WpMmTdK2bdu0ZMkSXXvttfURIwAACLIzepfElVdeqZUrVwY6FgAA6g2vtzbnjF8+tWnTJm3btk3SqXkNqampAQsKAICA422VpvidMOzfv1933HGHPv74YzVr1kySVFJSoiuuuEJvvfWW2rRpE+gYAQBAkPk9h2Ho0KGqqqrStm3bdPToUR09elTbtm2T2+3W0KFD6yNGAADMq5n0aGazML8rDGvXrtWGDRvUsWNHz76OHTvq+eef15VXXhnQ4AAACBSbcWozc76V+Z0wJCUlnXaBJpfLpcTExIAEBQBAwDGHwRS/WxJPPfWURo4cqU2bNnn2bdq0SaNGjdKf/vSngAYHAADODXWqMDRv3lw22396N+Xl5UpLS1NY2KnTq6urFRYWpnvuuUf9+/evl0ABADCFhZtMqVPC8Nxzz9VzGAAA1DNaEqbUKWHIysqq7zgAAMA57IwXbpKkiooKOZ1Or30Oh8NUQAAA1AsqDKb4PemxvLxcI0aMUKtWrRQdHa3mzZt7bQAAnJN4W6UpficMDz30kFavXq0XX3xRdrtd8+bN09SpU5WYmKg33nijPmIEAABB5ndLYsmSJXrjjTfUq1cvDR48WFdeeaWSk5PVrl07vfnmmxo0aFB9xAkAgDk8JWGK3xWGo0eP6oILLpB0ar7C0aNHJUm//vWv9dFHHwU2OgAAAqRmpUczm5X5nTBccMEF2r17tySpU6dOWrhwoaRTlYeal1EBAIDGxe+EYfDgwfriiy8kSePHj9ecOXMUERGhMWPGaOzYsQEPEACAgGDSoyl+z2EYM2aM588ZGRnavn278vPzlZycrK5duwY0OAAAcG4wtQ6DJLVr107t2rULRCwAANQbm0y+rTJgkTRMdUoYZs2aVecL/uEPfzjjYAAAwLmpTgnDs88+W6eL2Wy2oCQMN1/URWG2Jmf9vgCABoTHKk2pU8JQ81QEAAANFktDm+L3UxIAAMB6TE96BACgQaDCYAoJAwDAEsyu1shKjwAAAD5QYQAAWAMtCVPOqMKwbt063XnnnUpPT9eBAwckSX/5y1+0fv36gAYHAEDAsDS0KX4nDO+++64yMzMVGRmpzz//XJWVlZKk0tJSPfHEEwEPEAAABJ/fCcNjjz2muXPn6uWXX1aTJv9ZLOlXv/qVNm/eHNDgAAAIFF5vbY7fcxh27Nihq666qtb+2NhYlZSUBCImAAACj5UeTfG7wpCQkKCdO3fW2r9+/XpdcMEFAQkKAICAYw6DKX4nDMOGDdOoUaP06aefymazqbCwUG+++aYefPBBDR8+vD5iBAAAQeZ3S2L8+PFyu9265pprdOLECV111VWy2+168MEHNXLkyPqIEQAA01i4yRy/EwabzaaHH35YY8eO1c6dO3X8+HGlpKSoadOm9REfAACBwToMppzxwk3h4eFKSUkJZCwAAOAc5XfC0Lt3b9lsPz1TdPXq1aYCAgCgXph9NJIKg3+6d+/u9bmqqkoFBQX68ssvlZWVFai4AAAILFoSpvj9lMSzzz7rtc2ePVvr16/X6NGjvRZyAgDAyqZPn65LL71UMTExatWqlfr3768dO3Z4jamoqFB2drZatGihpk2basCAASouLvYas3fvXvXt21dRUVFq1aqVxo4dq+rqaq8xa9asUY8ePWS325WcnKzc3Nxa8cyZM0ft27dXRESE0tLStHHjRr++T8DeVnnnnXfq1VdfDdTlAAAIrLO8DsPatWuVnZ2tTz75RCtXrlRVVZX69Omj8vJyz5gxY8ZoyZIlevvtt7V27VoVFhbqlltu8Rx3uVzq27evnE6nNmzYoNdff125ubmaNGmSZ8zu3bvVt29f9e7dWwUFBRo9erSGDh2qFStWeMYsWLBAOTk5mjx5sjZv3qxu3bopMzNThw4dqvP3sRmGEZAiy1/+8heNGzdOhYWFgbhcnZSVlSk2Nla91E9hNqobANDQVBtVWqO/q7S0VA6Ho17uUfNb8Ys/PqHQiIgzvo6rokK7nvjjGcd6+PBhtWrVSmvXrtVVV12l0tJSnXfeeZo/f75uvfVWSdL27dvVuXNn5eXl6fLLL9cHH3yg3/72tyosLFR8fLwkae7cuRo3bpwOHz6s8PBwjRs3TsuWLdOXX37pudftt9+ukpISLV++XJKUlpamSy+9VLNnz5Ykud1uJSUlaeTIkRo/fnyd4vd7DsOPMx9JMgxDBw8e1KZNmzRx4kR/LwcAQINSVlbm9dlut8tut/s8r7S0VJIUFxcnScrPz1dVVZUyMjI8Yzp16qS2bdt6Eoa8vDx16dLFkyxIUmZmpoYPH66tW7fqkksuUV5entc1asaMHj1akuR0OpWfn68JEyZ4joeEhCgjI0N5eXl1/t5+tyRiY2O9tri4OPXq1Uvvv/++Jk+e7O/lAABoUJKSkrx+B6dPn+7zHLfbrdGjR+tXv/qVLr74YklSUVGRwsPD1axZM6+x8fHxKioq8oz5cbJQc7zm2M+NKSsr08mTJ3XkyBG5XK7Tjqm5Rl34VWFwuVwaPHiwunTpoubNm/tzKgAAwRWgpyT27dvn1ZKoS3UhOztbX375pdavX28igODyq8IQGhqqPn368FZKAECDE6jXWzscDq/NV8IwYsQILV26VB9++KHatGnj2Z+QkCCn01nrN7W4uFgJCQmeMf/91ETNZ19jHA6HIiMj1bJlS4WGhp52TM016sLvlsTFF1+sb7/91t/TAACwFMMwNGLECC1atEirV69Whw4dvI6npqaqSZMmWrVqlWffjh07tHfvXqWnp0uS0tPTtWXLFq+nGVauXCmHw+FZbTk9Pd3rGjVjaq4RHh6u1NRUrzFut1urVq3yjKkLvyc9PvbYY3rwwQc1bdo0paamKjo62ut4fc1yBQDAtLO4+FJ2drbmz5+vv//974qJifHMF4iNjVVkZKRiY2M1ZMgQ5eTkKC4uTg6HQyNHjlR6erouv/xySVKfPn2UkpKiu+66SzNmzFBRUZEeeeQRZWdneyob9913n2bPnq2HHnpI99xzj1avXq2FCxdq2bJlnlhycnKUlZWlnj176rLLLtNzzz2n8vJyDR48uM7fp84Jw6OPPqoHHnhAN9xwgyTppptu8loi2jAM2Ww2uVyuOt8cAICz5iyv9Pjiiy9Kknr16uW1/7XXXtPdd98t6dRiiCEhIRowYIAqKyuVmZmpF154wTM2NDRUS5cu1fDhw5Wenq7o6GhlZWXp0Ucf9Yzp0KGDli1bpjFjxmjmzJlq06aN5s2bp8zMTM+YgQMH6vDhw5o0aZKKiorUvXt3LV++vNZEyJ9T53UYQkNDdfDgQW3btu1nx1199dV1vrlZrMMAAA3b2VyHIXncEwq1m1iHobJCO//fma/D0NDVucJQk1eczYQAAIBA+fHExTM938r8msPwc2+pBADgnMbLp0zxK2G46KKLfCYNR48eNRUQAAA49/iVMEydOlWxsbH1FQsAAPWGloQ5fiUMt99+u1q1alVfsQAAUH9oSZhS54WbmL8AAIB1+f2UBAAADRIVBlPqnDC43e76jAMAgHrFHAZz/F4aGgCABokKgyl+v3wKAABYDxUGAIA1UGEwhYQBAGAJzGEwh5YEAADwiQoDAMAaaEmYQsIAALAEWhLm0JIAAAA+UWEAAFgDLQlTSBgAANZAwmAKLQkAAOATFQYAgCXYftjMnG9lJAwAAGugJWEKCQMAwBJ4rNIc5jAAAACfqDAAAKyBloQpJAwAAOuw+I++GbQkAACAT1QYAACWwKRHc0gYAADWwBwGU2hJAAAAn6gwAAAsgZaEOSQMAABroCVhCi0JAADgExUGAIAl0JIwh4QBAGANtCRMIWEAAFgDCYMpzGEAAAA+UWEAAFgCcxjMIWEAAFgDLQlTaEkAAACfqDAAACzBZhiyGWdeJjBzbmNAwgAAsAZaEqbQkgAAAD5RYQAAWAJPSZhDwgAAsAZaEqbQkgAAAD5RYQAAWAItCXNIGAAA1kBLwhQSBgCAJVBhMIc5DAAAwCcqDAAAa6AlYQoJAwDAMqzeVjCDlgQAAPCJCgMAwBoM49Rm5nwLI2EAAFgCT0mYQ0sCAAD4RIUBAGANPCVhCgkDAMASbO5Tm5nzrYyWBAAA8IkKA2q5OO24/uf+w7qwywm1SKjWlHvaK295rOf4nQ8UqVe/Ep2XWKUqp007t0TqtScTtOPz6CBGDdTdnQ8U6a4Hir327dtp19CrOkmSmtjdundyoXrdVKImdkP5a2L0/ITWKjnSJBjhIlBoSZhCwoBaIqLc+nZrhFb8NU6TX91T6/iBb+2a83BrHfwuXPYIQzffe1jT//qtBl/RWaVH+SuFhmHP9giNH3iB57PLZfP8+b4phboso0yP/W87lZeFKvvxA5r0yh7l9LswGKEiQHhKwpygtiQ++ugj3XjjjUpMTJTNZtPixYuDGQ5+sOlDh16fcb42/Kiq8GMfLmquz9fFqGivXd99HaE/T0lUtMOtDiknz3KkwJlzuaTvDzfxbGU/JLtRMS5l3nFUL01J1Bcfx2jnlig9k5OkX156Qp16lAc5aphSsw6Dmc3CgpowlJeXq1u3bpozZ04ww4AJYU3cuuHOf+t4aYi+/Soy2OEAdda6g1PzN29Vbt42jZv9nc5r7ZQkXdj1hJqEG/p8XYxn7L6dESre30SdU08EK1wg6IJaP77++ut1/fXX13l8ZWWlKisrPZ/LysrqIyzUQVpGmSa8+J3skW4dLQ7ThNt/4fkXGnCu2745Sn8anaT9u+yKa1WlOx8o1tOLdup/e3dUXKtqOSttKi8L9Tqn5HCY4lpVBSliBAItCXMa1H/hp0+frqlTpwY7DEgq+Dha9197kRxx1bp+0FE9/NJ3+kPfZJX+m0lhOPdt+tDh+fPubZHa/nm0/rLxK111U4mcFTw81mgx6dGUBvX/GRMmTFBpaaln27dvX7BDsqzKk6Eq3GPX9s3RevaBJLmqpevuOBrssIAzUl4Wqv3f2pXY3qmjh8IUbjcU7XB5jWl2XrWOHiIhhnU1qITBbrfL4XB4bTg32EKkJnaLp99osCKiXEpsdypZ+OZfUapy2nTJr495jrf5RYXi21RpW35UEKOEWTUtCTOblTWolgTOjogolxI7OD2fE5KcuuCXJ3WsJFRlR0P1u1GHlPcPh44WN5Ejrlo3DT6ilglVWrekWfCCBvwwbFKhPvmHQ4f2h6tFQpXuerBILre0ZlFznTgWqhV/jdO9Uwp1rCRM5cdClP34AX21KUrbN7PWSIPG2ypNIWFALRd1O6mn3t3l+Xzf1EJJ0j8WNNes8W3UJrlSE/9njxxxLh37PlRffxGlB25O1ndfRwQrZMAvLc+v0oQXvlNMc5dK/x2mrZ9Fa/RvL/SsIzJ3SqLchjTx5T1qYje0aU2MZk9oHeSogeAKasJw/Phx7dy50/N59+7dKigoUFxcnNq2bRvEyKztX3lNlZnY7SePTxva/uwFA9SD6cPb/ezxqsoQzfljG835Y5uzFBHOBp6SMCeoCcOmTZvUu3dvz+ecnBxJUlZWlnJzc4MUFQCgUeIpCVOCmjD06tVLhsV7QgAANAQN6ikJAADO1Nl+SsLX6w8Mw9CkSZN0/vnnKzIyUhkZGfrmm2+8xhw9elSDBg2Sw+FQs2bNNGTIEB0/ftxrzL/+9S9deeWVioiIUFJSkmbMmFErlrfffludOnVSRESEunTpovfff9+/LyMSBgCAVbgN85sffL3+YMaMGZo1a5bmzp2rTz/9VNHR0crMzFRFRYVnzKBBg7R161atXLlSS5cu1UcffaR7773Xc7ysrEx9+vRRu3btlJ+fr6eeekpTpkzRn//8Z8+YDRs26I477tCQIUP0+eefq3///urfv7++/PJLv76PzWjAPYGysjLFxsaql/opzMaCKgDQ0FQbVVqjv6u0tLTe1tap+a24ImOqwpqc+dNc1VUV2vDPydq3b59XrHa7XXa7/WfPtdlsWrRokfr37y/pVHUhMTFRDzzwgB588EFJUmlpqeLj45Wbm6vbb79d27ZtU0pKij777DP17NlTkrR8+XLdcMMN2r9/vxITE/Xiiy/q4YcfVlFRkcLDwyVJ48eP1+LFi7V9+3ZJ0sCBA1VeXq6lS5d64rn88svVvXt3zZ07t87fnwoDAAB+SEpKUmxsrGebPn2639fYvXu3ioqKlJGR4dkXGxurtLQ05eXlSZLy8vLUrFkzT7IgSRkZGQoJCdGnn37qGXPVVVd5kgVJyszM1I4dO/T99997xvz4PjVjau5TV6zDAACwBJtMPlb5w/89XYXBX0VFRZKk+Ph4r/3x8fGeY0VFRWrVqpXX8bCwMMXFxXmN6dChQ61r1Bxr3ry5ioqKfvY+dUXCAACwhgCt9GjVVxPQkgAA4CxLSEiQJBUXF3vtLy4u9hxLSEjQoUOHvI5XV1fr6NGjXmNOd40f3+OnxtQcrysSBgCAJZxLL5/q0KGDEhIStGrVKs++srIyffrpp0pPT5ckpaenq6SkRPn5+Z4xq1evltvtVlpammfMRx99pKqqKs+YlStXqmPHjmrevLlnzI/vUzOm5j51RcIAALAGIwCbH44fP66CggIVFBRI+s/rD/bu3SubzabRo0frscce03vvvactW7bo97//vRITEz1PUnTu3FnXXXedhg0bpo0bN+rjjz/WiBEjdPvttysxMVGS9Lvf/U7h4eEaMmSItm7dqgULFmjmzJmelZMladSoUVq+fLmefvppbd++XVOmTNGmTZs0YsQIv74PcxgAAKgHvl5/8NBDD6m8vFz33nuvSkpK9Otf/1rLly9XRMR/Hv188803NWLECF1zzTUKCQnRgAEDNGvWLM/x2NhY/eMf/1B2drZSU1PVsmVLTZo0yWuthiuuuELz58/XI488oj/+8Y+68MILtXjxYl188cV+fR/WYQAABM3ZXIfhyl6TFRZmYh2G6gqtWzO1XmM9l1FhAABYg/uHzcz5FsYcBgAA4BMVBgCAJdgMQzYTXXgz5zYGJAwAAGs4gycdap1vYSQMAABrCNBKj1bFHAYAAOATFQYAgCWYXa0xkCs9NkQkDAAAa6AlYQotCQAA4BMVBgCAJdjcpzYz51sZCQMAwBpoSZhCSwIAAPhEhQEAYA0s3GQKCQMAwBJYGtocWhIAAMAnKgwAAGtg0qMpJAwAAGswJJl5NNLa+QIJAwDAGpjDYA5zGAAAgE9UGAAA1mDI5ByGgEXSIJEwAACsgUmPptCSAAAAPlFhAABYg1uSzeT5FkbCAACwBJ6SMIeWBAAA8IkKAwDAGpj0aAoJAwDAGkgYTKElAQAAfKLCAACwBioMppAwAACsgccqTSFhAABYAo9VmsMcBgAA4BMVBgCANTCHwRQSBgCANbgNyWbiR99t7YSBlgQAAPCJCgMAwBpoSZhCwgAAsAiTCYOsnTDQkgAAAD5RYQAAWAMtCVNIGAAA1uA2ZKqtwFMSAAAAP48KAwDAGgz3qc3M+RZGwgAAsAbmMJhCwgAAsAbmMJjCHAYAAOATFQYAgDXQkjCFhAEAYA2GTCYMAYukQaIlAQAAfKLCAACwBloSppAwAACswe2WZGItBbe112GgJQEAAHyiwgAAsAZaEqaQMAAArIGEwRRaEgAAwCcqDAAAa2BpaFNIGAAAlmAYbhkm3jhp5tzGgIQBAGANhmGuSsAcBgAAgJ9HhQEAYA2GyTkMFq8wkDAAAKzB7ZZsJuYhWHwOAy0JAADgExUGAIA10JIwhYQBAGAJhtstw0RLwuqPVdKSAAAAPlFhAABYAy0JU0gYAADW4DYkGwnDmaIlAQAAfKLCAACwBsOQZGYdBmtXGEgYAACWYLgNGSZaEgYJAwAAFmC4Za7CwGOVAAAAP4sKAwDAEmhJmEPCAACwBloSpjTohKEm26tWlam1OAAAwVGtKkln51/vZn8ramK1qgadMBw7dkyStF7vBzkSAIAZx44dU2xsbL1cOzw8XAkJCVpfZP63IiEhQeHh4QGIquGxGQ24KeN2u1VYWKiYmBjZbLZgh2MJZWVlSkpK0r59++RwOIIdDhBQ/P0++wzD0LFjx5SYmKiQkPqbh19RUSGn02n6OuHh4YqIiAhARA1Pg64whISEqE2bNsEOw5IcDgf/QUWjxd/vs6u+Kgs/FhERYdkf+kDhsUoAAOATCQMAAPCJhAF+sdvtmjx5sux2e7BDAQKOv9/AT2vQkx4BAMDZQYUBAAD4RMIAAAB8ImEAAAA+kTAAAACfSBhQZ3PmzFH79u0VERGhtLQ0bdy4MdghAQHx0Ucf6cYbb1RiYqJsNpsWL14c7JCAcw4JA+pkwYIFysnJ0eTJk7V582Z169ZNmZmZOnToULBDA0wrLy9Xt27dNGfOnGCHApyzeKwSdZKWlqZLL71Us2fPlnTqPR5JSUkaOXKkxo8fH+TogMCx2WxatGiR+vfvH+xQgHMKFQb45HQ6lZ+fr4yMDM++kJAQZWRkKC8vL4iRAQDOFhIG+HTkyBG5XC7Fx8d77Y+Pj1dRUVGQogIAnE0kDAAAwCcSBvjUsmVLhYaGqri42Gt/cXGxEhISghQVAOBsImGAT+Hh4UpNTdWqVas8+9xut1atWqX09PQgRgYAOFvCgh0AGoacnBxlZWWpZ8+euuyyy/Tcc8+pvLxcgwcPDnZogGnHjx/Xzp07PZ93796tgoICxcXFqW3btkGMDDh38Fgl6mz27Nl66qmnVFRUpO7du2vWrFlKS0sLdliAaWvWrFHv3r1r7c/KylJubu7ZDwg4B5EwAAAAn5jDAAAAfCJhAAAAPpEwAAAAn0gYAACATyQMAADAJxIGAADgEwkDAADwiYQBAAD4RMIAmHT33Xerf//+ns+9evXS6NGjz3oca9askc1mU0lJyU+OsdlsWrx4cZ2vOWXKFHXv3t1UXHv27JHNZlNBQYGp6wAILhIGNEp33323bDabbDabwsPDlZycrEcffVTV1dX1fu+//e1vmjZtWp3G1uVHHgDOBbx8Co3Wddddp9dee02VlZV6//33lZ2drSZNmmjChAm1xjqdToWHhwfkvnFxcQG5DgCcS6gwoNGy2+1KSEhQu3btNHz4cGVkZOi9996T9J82wuOPP67ExER17NhRkrRv3z7ddtttatasmeLi4tSvXz/t2bPHc02Xy6WcnBw1a9ZMLVq00EMPPaT/fh3Lf7ckKisrNW7cOCUlJclutys5OVmvvPKK9uzZ43nhUfPmzWWz2XT33XdLOvX68OnTp6tDhw6KjIxUt27d9M4773jd5/3339dFF12kyMhI9e7d2yvOuho3bpwuuugiRUVF6YILLtDEiRNVVVVVa9xLL72kpKQkRUVF6bbbblNpaanX8Xnz5qlz586KiIhQp06d9MILL/gdC4BzGwkDLCMyMlJOp9PzedWqVdqxY4dWrlyppUuXqqqqSpmZmYqJidG6dev08ccfq2nTprruuus85z399NPKzc3Vq6++qvXr1+vo0aNatGjRz97397//vf76179q1qxZ2rZtm1566SU1bdpUSUlJevfddyVJO3bs0MGDBzVz5kxJ0vTp0/XGG29o7ty52rp1q8aMGaM777xTa9eulXQqsbnlllt04403qqCgQEOHDtX48eP9/t8kJiZGubm5+uqrrzRz5ky9/PLLevbZZ73G7Ny5UwsXLtSSJUu0fPlyff7557r//vs9x998801NmjRJjz/+uLZt26YnnnhCEydO1Ouvv+53PADOYQbQCGVlZRn9+vUzDMMw3G63sXLlSsNutxsPPvig53h8fLxRWVnpOecvf/mL0bFjR8Ptdnv2VVZWGpGRkcaKFSsMwzCM888/35gxY4bneFVVldGmTRvPvQzDMK6++mpj1KhRhmEYxo4dOwxJxsqVK08b54cffmhIMr7//nvPvoqKCiMqKsrYsGGD19ghQ4YYd9xxh2EYhjFhwgQjJSXF6/i4ceNqXeu/STIWLVr0k8efeuopIzU11fN58uTJRmhoqLF//37Pvg8++MAICQkxDh48aBiGYfziF78w5s+f73WdadOmGenp6YZhGMbu3bsNScbnn3/+k/cFcO5jDgMaraVLl6pp06aqqqqS2+3W7373O02ZMsVzvEuXLl7zFr744gvt3LlTMTExXtepqKjQrl27VFpaqoMHDyotLc1zLCwsTD179qzVlqhRUFCg0NBQXX311XWOe+fOnTpx4oSuvfZar/1Op1OXXHKJJGnbtm1ecUhSenp6ne9RY8GCBZo1a5Z27dql48ePq7q6Wg6Hw2tM27Zt1bp1a6/7uN1u7dixQzExMdq1a5eGDBmiYcOGecZUV1crNjbW73gAnLtIGNBo9e7dWy+++KLCw8OVmJiosDDvv+7R0dFen48fP67U1FS9+eabta513nnnnVEMkZGRfp9z/PhxSdKyZcu8fqilU/MyAiUvL0+DBg3S1KlTlZmZqdjYWL311lt6+umn/Y715ZdfrpXAhIaGBixWAMFHwoBGKzo6WsnJyXUe36NHDy1YsECtWrWq9a/sGueff74+/fRTXXXVVZJO/Us6Pz9fPXr0OO34Ll26yO12a+3atcrIyKh1vKbC4XK5PPtSUlJkt9u1d+/en6xMdO7c2TOBs8Ynn3zi+0v+yIYNG9SuXTs9/PDDnn3fffddrXF79+5VYWGhEhMTPfcJCQlRx44dFR8fr8TERH377bcaNGiQX/cH0LAw6RH4waBBg9SyZUv169dP69at0+7du7VmzRr94Q9/0P79+yVJo0aN0pNPPqnFixdr+/btuv/++392DYX27dsrKytL99xzjxYvXuy55sKFCyVJ7dq1k81m09KlS3X48GEdP35cMTExevDBBzVmzBi9/vrr2rVrlzZv3qznn3/eM5Hwvvvu0zfffKOxY8dqx44dmj9/vnJzc/36vhdeeKH27t2rt956S7t27dKsWbNOO4EzIiJCWVlZ+uKLL7Ru3Tr94Q9/0G233aaEhARJ0tSpUzV9+nTNmjVLX3/9tbZs2aLXXntNzzzzjF/xADi3kTAAP4iKitJHH32ktm3b6pZbblHnzp01ZMgQVVRUeCoODzzwgO666y5lZWUpPT1dMTExuvnmm3/2ui+++KJuvfVW3X///erUqZOGDRum8vJySVLr1q01depUjR8/XvHx8RoxYoQkadq0aZo4caKmT5+uzp0767rrrtOyZcvUoUMHSafmFbz77rtavHixunXrprlz5+qJJ57w6/vedNNNGjNmjEaMGKHu3btrw4YNmjhxYq1xycnJuuWWW3TDDTeoT58+6tq1q9djk0OHDtW8efP02muvqUuXLrr66quVm5vriRVA42Azfmq2FgAAwA+oMAAAAJ9IGAAAgE8kDAAAwCcSBgAA4BMJAwAA8ImEAQAA+ETCAAAAfCJhAAAAPpEwAAAAn0gYAACATyQMAADAp/8P8X4wlhb6mGQAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_results(y_test,cnn_cust_test_preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "id": "kGr3CbYYiEND",
        "outputId": "c97626f1-f40d-4d1b-a061-45cd160ccf28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     56659\n",
            "           1       0.68      0.78      0.73        87\n",
            "\n",
            "    accuracy                           1.00     56746\n",
            "   macro avg       0.84      0.89      0.86     56746\n",
            "weighted avg       1.00      1.00      1.00     56746\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "cnn + rmsprop + weighted loss"
      ],
      "metadata": {
        "id": "VxNx80boN7Tu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w=0.02\n",
        "validation_preds_rms=[]\n",
        "while(w<=0.2): \n",
        "  print(\"******************************for w = \",w)\n",
        "  cnn_cust=create_cnn_cust()\n",
        "  cnn_cust.summary()\n",
        "  cnn_cust.fit(X_train_norm,y_train, batch_size=2048,epochs=20, verbose=1, validation_data=(X_val_norm,y_val), class_weight={0: w, 1: 1-w})\n",
        "  cnn_cust_val_preds = cnn_cust.predict(X_val_norm)>0.5\n",
        "  #cnn_cust_test_preds= cnn_cust.predict(X_test_norm)>0.5\n",
        "  validation_preds_rms.append(cnn_cust_val_preds)\n",
        "  w+=0.01"
      ],
      "metadata": {
        "id": "8PxgJHjYiR9M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0917b446-f988-467d-8c40-34da314833ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "******************************for w =  0.02\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 3712)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89/89 [==============================] - ETA: 0s - loss: 0.0042 - accuracy: 0.9928 - precision: 0.1583 - recall: 0.6997 - auc: 0.8947 - prc: 0.5722"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 11s 16ms/step - loss: 0.0042 - accuracy: 0.9928 - precision: 0.1583 - recall: 0.6997 - auc: 0.8947 - prc: 0.5722 - val_loss: 0.0363 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9621 - val_prc: 0.6986\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.8069 - recall: 0.8019 - auc: 0.9445 - prc: 0.7406 - val_loss: 0.0232 - val_accuracy: 0.9993 - val_precision: 0.6986 - val_recall: 0.8095 - val_auc: 0.9634 - val_prc: 0.7198\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0014 - accuracy: 0.9992 - precision: 0.7642 - recall: 0.8328 - auc: 0.9615 - prc: 0.7511 - val_loss: 0.0228 - val_accuracy: 0.9991 - val_precision: 0.6265 - val_recall: 0.8254 - val_auc: 0.9716 - val_prc: 0.7097\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0013 - accuracy: 0.9991 - precision: 0.7139 - recall: 0.8421 - auc: 0.9714 - prc: 0.7546 - val_loss: 0.0220 - val_accuracy: 0.9989 - val_precision: 0.5714 - val_recall: 0.8254 - val_auc: 0.9743 - val_prc: 0.7131\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0013 - accuracy: 0.9990 - precision: 0.6851 - recall: 0.8421 - auc: 0.9737 - prc: 0.7603 - val_loss: 0.0206 - val_accuracy: 0.9988 - val_precision: 0.5361 - val_recall: 0.8254 - val_auc: 0.9742 - val_prc: 0.7130\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 0.0012 - accuracy: 0.9990 - precision: 0.6610 - recall: 0.8452 - auc: 0.9818 - prc: 0.7620 - val_loss: 0.0160 - val_accuracy: 0.9989 - val_precision: 0.5730 - val_recall: 0.8095 - val_auc: 0.9734 - val_prc: 0.7499\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 0.0012 - accuracy: 0.9989 - precision: 0.6480 - recall: 0.8607 - auc: 0.9840 - prc: 0.7629 - val_loss: 0.0153 - val_accuracy: 0.9988 - val_precision: 0.5484 - val_recall: 0.8095 - val_auc: 0.9750 - val_prc: 0.7505\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 0.9989 - precision: 0.6419 - recall: 0.8545 - auc: 0.9833 - prc: 0.7664 - val_loss: 0.0183 - val_accuracy: 0.9985 - val_precision: 0.4815 - val_recall: 0.8254 - val_auc: 0.9767 - val_prc: 0.7383\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 0.9988 - precision: 0.6066 - recall: 0.8545 - auc: 0.9843 - prc: 0.7651 - val_loss: 0.0134 - val_accuracy: 0.9988 - val_precision: 0.5426 - val_recall: 0.8095 - val_auc: 0.9777 - val_prc: 0.7645\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 0.9987 - precision: 0.5974 - recall: 0.8545 - auc: 0.9836 - prc: 0.7583 - val_loss: 0.0166 - val_accuracy: 0.9984 - val_precision: 0.4643 - val_recall: 0.8254 - val_auc: 0.9758 - val_prc: 0.7513\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 0.9987 - precision: 0.5809 - recall: 0.8669 - auc: 0.9841 - prc: 0.7573 - val_loss: 0.0164 - val_accuracy: 0.9984 - val_precision: 0.4595 - val_recall: 0.8095 - val_auc: 0.9759 - val_prc: 0.7401\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 0.9987 - precision: 0.5949 - recall: 0.8638 - auc: 0.9840 - prc: 0.7606 - val_loss: 0.0156 - val_accuracy: 0.9985 - val_precision: 0.4679 - val_recall: 0.8095 - val_auc: 0.9765 - val_prc: 0.7406\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 0.9987 - precision: 0.5865 - recall: 0.8607 - auc: 0.9840 - prc: 0.7598 - val_loss: 0.0153 - val_accuracy: 0.9985 - val_precision: 0.4722 - val_recall: 0.8095 - val_auc: 0.9770 - val_prc: 0.7409\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0011 - accuracy: 0.9985 - precision: 0.5589 - recall: 0.8514 - auc: 0.9854 - prc: 0.7596 - val_loss: 0.0154 - val_accuracy: 0.9984 - val_precision: 0.4602 - val_recall: 0.8254 - val_auc: 0.9772 - val_prc: 0.7524\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 0.9986 - precision: 0.5611 - recall: 0.8669 - auc: 0.9843 - prc: 0.7671 - val_loss: 0.0157 - val_accuracy: 0.9984 - val_precision: 0.4554 - val_recall: 0.8095 - val_auc: 0.9770 - val_prc: 0.7529\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 0.9985 - precision: 0.5462 - recall: 0.8607 - auc: 0.9849 - prc: 0.7666 - val_loss: 0.0119 - val_accuracy: 0.9987 - val_precision: 0.5312 - val_recall: 0.8095 - val_auc: 0.9793 - val_prc: 0.7678\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0011 - accuracy: 0.9985 - precision: 0.5601 - recall: 0.8514 - auc: 0.9846 - prc: 0.7688 - val_loss: 0.0191 - val_accuracy: 0.9980 - val_precision: 0.4000 - val_recall: 0.8254 - val_auc: 0.9753 - val_prc: 0.7406\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0011 - accuracy: 0.9985 - precision: 0.5458 - recall: 0.8669 - auc: 0.9849 - prc: 0.7667 - val_loss: 0.0144 - val_accuracy: 0.9985 - val_precision: 0.4722 - val_recall: 0.8095 - val_auc: 0.9776 - val_prc: 0.7537\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 0.9985 - precision: 0.5547 - recall: 0.8638 - auc: 0.9845 - prc: 0.7675 - val_loss: 0.0167 - val_accuracy: 0.9983 - val_precision: 0.4370 - val_recall: 0.8254 - val_auc: 0.9772 - val_prc: 0.7421\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 0.9985 - precision: 0.5416 - recall: 0.8669 - auc: 0.9857 - prc: 0.7695 - val_loss: 0.0131 - val_accuracy: 0.9986 - val_precision: 0.5050 - val_recall: 0.8095 - val_auc: 0.9784 - val_prc: 0.7535\n",
            "1419/1419 [==============================] - 2s 2ms/step\n",
            "******************************for w =  0.03\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_1 (Conv1D)           (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87/89 [============================>.] - ETA: 0s - loss: 0.0050 - accuracy: 0.9948 - precision: 0.1931 - recall: 0.6544 - auc: 0.9056 - prc: 0.5427"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 7s 18ms/step - loss: 0.0049 - accuracy: 0.9949 - precision: 0.1969 - recall: 0.6580 - auc: 0.9069 - prc: 0.5499 - val_loss: 0.0209 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9539 - val_prc: 0.6974\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0018 - accuracy: 0.9993 - precision: 0.8199 - recall: 0.7895 - auc: 0.9490 - prc: 0.7363 - val_loss: 0.0181 - val_accuracy: 0.9993 - val_precision: 0.6986 - val_recall: 0.8095 - val_auc: 0.9636 - val_prc: 0.7154\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 2s 19ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7536 - recall: 0.8142 - auc: 0.9639 - prc: 0.7576 - val_loss: 0.0154 - val_accuracy: 0.9991 - val_precision: 0.6375 - val_recall: 0.8095 - val_auc: 0.9661 - val_prc: 0.7210\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 2s 18ms/step - loss: 0.0014 - accuracy: 0.9991 - precision: 0.7236 - recall: 0.8266 - auc: 0.9715 - prc: 0.7597 - val_loss: 0.0101 - val_accuracy: 0.9992 - val_precision: 0.6892 - val_recall: 0.8095 - val_auc: 0.9757 - val_prc: 0.7441\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 2s 20ms/step - loss: 0.0014 - accuracy: 0.9991 - precision: 0.7185 - recall: 0.8297 - auc: 0.9765 - prc: 0.7611 - val_loss: 0.0139 - val_accuracy: 0.9990 - val_precision: 0.6000 - val_recall: 0.8095 - val_auc: 0.9747 - val_prc: 0.7471\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 0.0013 - accuracy: 0.9991 - precision: 0.6931 - recall: 0.8390 - auc: 0.9781 - prc: 0.7565 - val_loss: 0.0121 - val_accuracy: 0.9990 - val_precision: 0.5930 - val_recall: 0.8095 - val_auc: 0.9775 - val_prc: 0.7586\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 0.0013 - accuracy: 0.9990 - precision: 0.6818 - recall: 0.8359 - auc: 0.9817 - prc: 0.7645 - val_loss: 0.0138 - val_accuracy: 0.9988 - val_precision: 0.5484 - val_recall: 0.8095 - val_auc: 0.9771 - val_prc: 0.7585\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0013 - accuracy: 0.9990 - precision: 0.6683 - recall: 0.8421 - auc: 0.9833 - prc: 0.7645 - val_loss: 0.0142 - val_accuracy: 0.9987 - val_precision: 0.5152 - val_recall: 0.8095 - val_auc: 0.9773 - val_prc: 0.7368\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0013 - accuracy: 0.9989 - precision: 0.6424 - recall: 0.8452 - auc: 0.9843 - prc: 0.7626 - val_loss: 0.0105 - val_accuracy: 0.9989 - val_precision: 0.5862 - val_recall: 0.8095 - val_auc: 0.9736 - val_prc: 0.7601\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0012 - accuracy: 0.9989 - precision: 0.6432 - recall: 0.8483 - auc: 0.9841 - prc: 0.7617 - val_loss: 0.0113 - val_accuracy: 0.9989 - val_precision: 0.5667 - val_recall: 0.8095 - val_auc: 0.9729 - val_prc: 0.7605\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 0.0012 - accuracy: 0.9989 - precision: 0.6478 - recall: 0.8483 - auc: 0.9832 - prc: 0.7661 - val_loss: 0.0112 - val_accuracy: 0.9988 - val_precision: 0.5543 - val_recall: 0.8095 - val_auc: 0.9735 - val_prc: 0.7597\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0012 - accuracy: 0.9989 - precision: 0.6485 - recall: 0.8452 - auc: 0.9845 - prc: 0.7616 - val_loss: 0.0123 - val_accuracy: 0.9986 - val_precision: 0.5050 - val_recall: 0.8095 - val_auc: 0.9730 - val_prc: 0.7529\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0012 - accuracy: 0.9989 - precision: 0.6322 - recall: 0.8514 - auc: 0.9859 - prc: 0.7620 - val_loss: 0.0099 - val_accuracy: 0.9989 - val_precision: 0.5862 - val_recall: 0.8095 - val_auc: 0.9730 - val_prc: 0.7614\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0012 - accuracy: 0.9989 - precision: 0.6485 - recall: 0.8452 - auc: 0.9853 - prc: 0.7629 - val_loss: 0.0134 - val_accuracy: 0.9985 - val_precision: 0.4857 - val_recall: 0.8095 - val_auc: 0.9791 - val_prc: 0.7391\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 17ms/step - loss: 0.0012 - accuracy: 0.9988 - precision: 0.6143 - recall: 0.8483 - auc: 0.9871 - prc: 0.7621 - val_loss: 0.0089 - val_accuracy: 0.9991 - val_precision: 0.6220 - val_recall: 0.8095 - val_auc: 0.9742 - val_prc: 0.7627\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 17ms/step - loss: 0.0012 - accuracy: 0.9989 - precision: 0.6387 - recall: 0.8483 - auc: 0.9835 - prc: 0.7705 - val_loss: 0.0096 - val_accuracy: 0.9989 - val_precision: 0.5795 - val_recall: 0.8095 - val_auc: 0.9736 - val_prc: 0.7635\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 0.0012 - accuracy: 0.9989 - precision: 0.6462 - recall: 0.8483 - auc: 0.9860 - prc: 0.7686 - val_loss: 0.0124 - val_accuracy: 0.9986 - val_precision: 0.4904 - val_recall: 0.8095 - val_auc: 0.9732 - val_prc: 0.7525\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 2s 25ms/step - loss: 0.0012 - accuracy: 0.9988 - precision: 0.6273 - recall: 0.8545 - auc: 0.9865 - prc: 0.7636 - val_loss: 0.0132 - val_accuracy: 0.9985 - val_precision: 0.4811 - val_recall: 0.8095 - val_auc: 0.9734 - val_prc: 0.7416\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 0.0012 - accuracy: 0.9988 - precision: 0.6241 - recall: 0.8483 - auc: 0.9867 - prc: 0.7685 - val_loss: 0.0109 - val_accuracy: 0.9987 - val_precision: 0.5152 - val_recall: 0.8095 - val_auc: 0.9728 - val_prc: 0.7628\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 0.0012 - accuracy: 0.9989 - precision: 0.6330 - recall: 0.8545 - auc: 0.9850 - prc: 0.7686 - val_loss: 0.0115 - val_accuracy: 0.9986 - val_precision: 0.5000 - val_recall: 0.8095 - val_auc: 0.9741 - val_prc: 0.7662\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "******************************for w =  0.04\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_2 (Conv1D)           (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87/89 [============================>.] - ETA: 0s - loss: 0.0057 - accuracy: 0.9948 - precision: 0.1983 - recall: 0.6702 - auc: 0.9039 - prc: 0.5457"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 5s 39ms/step - loss: 0.0056 - accuracy: 0.9949 - precision: 0.2002 - recall: 0.6710 - auc: 0.9042 - prc: 0.5485 - val_loss: 0.0178 - val_accuracy: 0.9994 - val_precision: 0.8033 - val_recall: 0.7778 - val_auc: 0.9577 - val_prc: 0.6741\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0019 - accuracy: 0.9994 - precision: 0.8528 - recall: 0.7895 - auc: 0.9477 - prc: 0.7395 - val_loss: 0.0107 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9662 - val_prc: 0.7100\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.7821 - recall: 0.8111 - auc: 0.9600 - prc: 0.7518 - val_loss: 0.0111 - val_accuracy: 0.9993 - val_precision: 0.6986 - val_recall: 0.8095 - val_auc: 0.9686 - val_prc: 0.7180\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9992 - precision: 0.7659 - recall: 0.8204 - auc: 0.9681 - prc: 0.7566 - val_loss: 0.0129 - val_accuracy: 0.9991 - val_precision: 0.6375 - val_recall: 0.8095 - val_auc: 0.9738 - val_prc: 0.7086\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7563 - recall: 0.8359 - auc: 0.9731 - prc: 0.7622 - val_loss: 0.0093 - val_accuracy: 0.9993 - val_precision: 0.6986 - val_recall: 0.8095 - val_auc: 0.9799 - val_prc: 0.7453\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0014 - accuracy: 0.9991 - precision: 0.7197 - recall: 0.8266 - auc: 0.9782 - prc: 0.7673 - val_loss: 0.0090 - val_accuracy: 0.9992 - val_precision: 0.6623 - val_recall: 0.8095 - val_auc: 0.9731 - val_prc: 0.7619\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0014 - accuracy: 0.9991 - precision: 0.7188 - recall: 0.8390 - auc: 0.9761 - prc: 0.7624 - val_loss: 0.0082 - val_accuracy: 0.9993 - val_precision: 0.7083 - val_recall: 0.8095 - val_auc: 0.9722 - val_prc: 0.7633\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0014 - accuracy: 0.9991 - precision: 0.7219 - recall: 0.8359 - auc: 0.9797 - prc: 0.7633 - val_loss: 0.0089 - val_accuracy: 0.9991 - val_precision: 0.6456 - val_recall: 0.8095 - val_auc: 0.9731 - val_prc: 0.7629\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0014 - accuracy: 0.9991 - precision: 0.7128 - recall: 0.8297 - auc: 0.9791 - prc: 0.7663 - val_loss: 0.0107 - val_accuracy: 0.9989 - val_precision: 0.5862 - val_recall: 0.8095 - val_auc: 0.9798 - val_prc: 0.7374\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0013 - accuracy: 0.9991 - precision: 0.6949 - recall: 0.8390 - auc: 0.9804 - prc: 0.7623 - val_loss: 0.0114 - val_accuracy: 0.9988 - val_precision: 0.5484 - val_recall: 0.8095 - val_auc: 0.9794 - val_prc: 0.7657\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0013 - accuracy: 0.9991 - precision: 0.6941 - recall: 0.8359 - auc: 0.9800 - prc: 0.7669 - val_loss: 0.0087 - val_accuracy: 0.9991 - val_precision: 0.6296 - val_recall: 0.8095 - val_auc: 0.9738 - val_prc: 0.7628\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0013 - accuracy: 0.9990 - precision: 0.6683 - recall: 0.8359 - auc: 0.9813 - prc: 0.7731 - val_loss: 0.0088 - val_accuracy: 0.9991 - val_precision: 0.6296 - val_recall: 0.8095 - val_auc: 0.9737 - val_prc: 0.7629\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0013 - accuracy: 0.9991 - precision: 0.7042 - recall: 0.8328 - auc: 0.9789 - prc: 0.7676 - val_loss: 0.0086 - val_accuracy: 0.9991 - val_precision: 0.6456 - val_recall: 0.8095 - val_auc: 0.9739 - val_prc: 0.7618\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0013 - accuracy: 0.9990 - precision: 0.6791 - recall: 0.8452 - auc: 0.9840 - prc: 0.7680 - val_loss: 0.0069 - val_accuracy: 0.9993 - val_precision: 0.6986 - val_recall: 0.8095 - val_auc: 0.9748 - val_prc: 0.7627\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0013 - accuracy: 0.9991 - precision: 0.7120 - recall: 0.8421 - auc: 0.9806 - prc: 0.7727 - val_loss: 0.0129 - val_accuracy: 0.9985 - val_precision: 0.4766 - val_recall: 0.8095 - val_auc: 0.9794 - val_prc: 0.7400\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0013 - accuracy: 0.9990 - precision: 0.6834 - recall: 0.8421 - auc: 0.9818 - prc: 0.7719 - val_loss: 0.0115 - val_accuracy: 0.9987 - val_precision: 0.5152 - val_recall: 0.8095 - val_auc: 0.9791 - val_prc: 0.7406\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0013 - accuracy: 0.9990 - precision: 0.6826 - recall: 0.8390 - auc: 0.9827 - prc: 0.7658 - val_loss: 0.0118 - val_accuracy: 0.9986 - val_precision: 0.4951 - val_recall: 0.8095 - val_auc: 0.9801 - val_prc: 0.7668\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0013 - accuracy: 0.9990 - precision: 0.6733 - recall: 0.8421 - auc: 0.9827 - prc: 0.7655 - val_loss: 0.0100 - val_accuracy: 0.9989 - val_precision: 0.5667 - val_recall: 0.8095 - val_auc: 0.9733 - val_prc: 0.7648\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0013 - accuracy: 0.9990 - precision: 0.6642 - recall: 0.8452 - auc: 0.9841 - prc: 0.7686 - val_loss: 0.0083 - val_accuracy: 0.9990 - val_precision: 0.6071 - val_recall: 0.8095 - val_auc: 0.9748 - val_prc: 0.7631\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0013 - accuracy: 0.9990 - precision: 0.6783 - recall: 0.8421 - auc: 0.9833 - prc: 0.7664 - val_loss: 0.0084 - val_accuracy: 0.9990 - val_precision: 0.6000 - val_recall: 0.8095 - val_auc: 0.9747 - val_prc: 0.7628\n",
            "1419/1419 [==============================] - 4s 3ms/step\n",
            "******************************for w =  0.05\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_3 (Conv1D)           (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88/89 [============================>.] - ETA: 0s - loss: 0.0065 - accuracy: 0.9932 - precision: 0.1464 - recall: 0.6257 - auc: 0.8838 - prc: 0.5335"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 4s 17ms/step - loss: 0.0065 - accuracy: 0.9932 - precision: 0.1479 - recall: 0.6269 - auc: 0.8846 - prc: 0.5365 - val_loss: 0.0148 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9580 - val_prc: 0.6878\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0020 - accuracy: 0.9994 - precision: 0.8505 - recall: 0.7926 - auc: 0.9402 - prc: 0.7420 - val_loss: 0.0097 - val_accuracy: 0.9994 - val_precision: 0.7812 - val_recall: 0.7937 - val_auc: 0.9687 - val_prc: 0.7163\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9993 - precision: 0.7892 - recall: 0.8111 - auc: 0.9583 - prc: 0.7516 - val_loss: 0.0110 - val_accuracy: 0.9993 - val_precision: 0.6986 - val_recall: 0.8095 - val_auc: 0.9695 - val_prc: 0.7078\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0016 - accuracy: 0.9992 - precision: 0.7615 - recall: 0.8204 - auc: 0.9697 - prc: 0.7541 - val_loss: 0.0099 - val_accuracy: 0.9992 - val_precision: 0.6800 - val_recall: 0.8095 - val_auc: 0.9721 - val_prc: 0.7340\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0016 - accuracy: 0.9992 - precision: 0.7659 - recall: 0.8204 - auc: 0.9729 - prc: 0.7603 - val_loss: 0.0076 - val_accuracy: 0.9993 - val_precision: 0.6986 - val_recall: 0.8095 - val_auc: 0.9672 - val_prc: 0.7588\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7688 - recall: 0.8235 - auc: 0.9743 - prc: 0.7646 - val_loss: 0.0085 - val_accuracy: 0.9992 - val_precision: 0.6800 - val_recall: 0.8095 - val_auc: 0.9724 - val_prc: 0.7463\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7521 - recall: 0.8266 - auc: 0.9769 - prc: 0.7691 - val_loss: 0.0085 - val_accuracy: 0.9991 - val_precision: 0.6538 - val_recall: 0.8095 - val_auc: 0.9722 - val_prc: 0.7613\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0014 - accuracy: 0.9992 - precision: 0.7620 - recall: 0.8328 - auc: 0.9797 - prc: 0.7655 - val_loss: 0.0101 - val_accuracy: 0.9990 - val_precision: 0.6145 - val_recall: 0.8095 - val_auc: 0.9790 - val_prc: 0.7608\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 2s 26ms/step - loss: 0.0014 - accuracy: 0.9992 - precision: 0.7397 - recall: 0.8359 - auc: 0.9820 - prc: 0.7683 - val_loss: 0.0065 - val_accuracy: 0.9993 - val_precision: 0.7391 - val_recall: 0.8095 - val_auc: 0.9753 - val_prc: 0.7594\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 2s 23ms/step - loss: 0.0014 - accuracy: 0.9991 - precision: 0.7251 - recall: 0.8328 - auc: 0.9800 - prc: 0.7659 - val_loss: 0.0088 - val_accuracy: 0.9991 - val_precision: 0.6296 - val_recall: 0.8095 - val_auc: 0.9736 - val_prc: 0.7610\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 2s 22ms/step - loss: 0.0014 - accuracy: 0.9991 - precision: 0.7270 - recall: 0.8328 - auc: 0.9817 - prc: 0.7707 - val_loss: 0.0083 - val_accuracy: 0.9991 - val_precision: 0.6296 - val_recall: 0.8095 - val_auc: 0.9744 - val_prc: 0.7612\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 2s 25ms/step - loss: 0.0014 - accuracy: 0.9992 - precision: 0.7445 - recall: 0.8390 - auc: 0.9791 - prc: 0.7679 - val_loss: 0.0095 - val_accuracy: 0.9989 - val_precision: 0.5730 - val_recall: 0.8095 - val_auc: 0.9739 - val_prc: 0.7623\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 2s 18ms/step - loss: 0.0014 - accuracy: 0.9991 - precision: 0.7060 - recall: 0.8328 - auc: 0.9815 - prc: 0.7638 - val_loss: 0.0073 - val_accuracy: 0.9991 - val_precision: 0.6538 - val_recall: 0.8095 - val_auc: 0.9755 - val_prc: 0.7621\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 2s 19ms/step - loss: 0.0014 - accuracy: 0.9991 - precision: 0.7060 - recall: 0.8328 - auc: 0.9810 - prc: 0.7706 - val_loss: 0.0067 - val_accuracy: 0.9993 - val_precision: 0.6986 - val_recall: 0.8095 - val_auc: 0.9683 - val_prc: 0.7604\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 3s 34ms/step - loss: 0.0014 - accuracy: 0.9992 - precision: 0.7410 - recall: 0.8328 - auc: 0.9809 - prc: 0.7632 - val_loss: 0.0095 - val_accuracy: 0.9989 - val_precision: 0.5604 - val_recall: 0.8095 - val_auc: 0.9741 - val_prc: 0.7637\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 2s 18ms/step - loss: 0.0014 - accuracy: 0.9991 - precision: 0.7113 - recall: 0.8390 - auc: 0.9808 - prc: 0.7695 - val_loss: 0.0079 - val_accuracy: 0.9991 - val_precision: 0.6296 - val_recall: 0.8095 - val_auc: 0.9754 - val_prc: 0.7596\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 2s 18ms/step - loss: 0.0014 - accuracy: 0.9991 - precision: 0.7177 - recall: 0.8421 - auc: 0.9837 - prc: 0.7667 - val_loss: 0.0071 - val_accuracy: 0.9992 - val_precision: 0.6623 - val_recall: 0.8095 - val_auc: 0.9684 - val_prc: 0.7601\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 0.0014 - accuracy: 0.9991 - precision: 0.7021 - recall: 0.8390 - auc: 0.9835 - prc: 0.7732 - val_loss: 0.0088 - val_accuracy: 0.9989 - val_precision: 0.5862 - val_recall: 0.8095 - val_auc: 0.9745 - val_prc: 0.7591\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0013 - accuracy: 0.9992 - precision: 0.7439 - recall: 0.8452 - auc: 0.9837 - prc: 0.7766 - val_loss: 0.0099 - val_accuracy: 0.9988 - val_precision: 0.5426 - val_recall: 0.8095 - val_auc: 0.9739 - val_prc: 0.7624\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0014 - accuracy: 0.9991 - precision: 0.7143 - recall: 0.8359 - auc: 0.9831 - prc: 0.7673 - val_loss: 0.0085 - val_accuracy: 0.9990 - val_precision: 0.5930 - val_recall: 0.8095 - val_auc: 0.9752 - val_prc: 0.7423\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "******************************for w =  0.060000000000000005\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_4 (Conv1D)           (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86/89 [===========================>..] - ETA: 0s - loss: 0.0075 - accuracy: 0.9924 - precision: 0.1255 - recall: 0.5829 - auc: 0.8637 - prc: 0.4761"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 16ms/step - loss: 0.0073 - accuracy: 0.9926 - precision: 0.1304 - recall: 0.5907 - auc: 0.8675 - prc: 0.4838 - val_loss: 0.0124 - val_accuracy: 0.9994 - val_precision: 0.7903 - val_recall: 0.7778 - val_auc: 0.9524 - val_prc: 0.6650\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0020 - accuracy: 0.9994 - precision: 0.8557 - recall: 0.7895 - auc: 0.9434 - prc: 0.7354 - val_loss: 0.0115 - val_accuracy: 0.9993 - val_precision: 0.7286 - val_recall: 0.8095 - val_auc: 0.9659 - val_prc: 0.7167\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9993 - precision: 0.7927 - recall: 0.8050 - auc: 0.9643 - prc: 0.7548 - val_loss: 0.0086 - val_accuracy: 0.9993 - val_precision: 0.6986 - val_recall: 0.8095 - val_auc: 0.9669 - val_prc: 0.7158\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.7781 - recall: 0.8142 - auc: 0.9710 - prc: 0.7623 - val_loss: 0.0078 - val_accuracy: 0.9993 - val_precision: 0.6986 - val_recall: 0.8095 - val_auc: 0.9670 - val_prc: 0.7413\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7840 - recall: 0.8204 - auc: 0.9723 - prc: 0.7611 - val_loss: 0.0081 - val_accuracy: 0.9993 - val_precision: 0.6986 - val_recall: 0.8095 - val_auc: 0.9736 - val_prc: 0.7547\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7657 - recall: 0.8297 - auc: 0.9746 - prc: 0.7637 - val_loss: 0.0073 - val_accuracy: 0.9993 - val_precision: 0.6986 - val_recall: 0.8095 - val_auc: 0.9748 - val_prc: 0.7583\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7672 - recall: 0.8266 - auc: 0.9772 - prc: 0.7630 - val_loss: 0.0087 - val_accuracy: 0.9991 - val_precision: 0.6538 - val_recall: 0.8095 - val_auc: 0.9741 - val_prc: 0.7427\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7486 - recall: 0.8297 - auc: 0.9793 - prc: 0.7656 - val_loss: 0.0086 - val_accuracy: 0.9991 - val_precision: 0.6296 - val_recall: 0.8095 - val_auc: 0.9741 - val_prc: 0.7583\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7486 - recall: 0.8297 - auc: 0.9795 - prc: 0.7671 - val_loss: 0.0071 - val_accuracy: 0.9992 - val_precision: 0.6892 - val_recall: 0.8095 - val_auc: 0.9754 - val_prc: 0.7588\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7458 - recall: 0.8266 - auc: 0.9811 - prc: 0.7660 - val_loss: 0.0084 - val_accuracy: 0.9991 - val_precision: 0.6220 - val_recall: 0.8095 - val_auc: 0.9748 - val_prc: 0.7583\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7383 - recall: 0.8297 - auc: 0.9803 - prc: 0.7701 - val_loss: 0.0065 - val_accuracy: 0.9992 - val_precision: 0.6800 - val_recall: 0.8095 - val_auc: 0.9690 - val_prc: 0.7595\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 0.0014 - accuracy: 0.9992 - precision: 0.7417 - recall: 0.8266 - auc: 0.9796 - prc: 0.7656 - val_loss: 0.0067 - val_accuracy: 0.9992 - val_precision: 0.6711 - val_recall: 0.8095 - val_auc: 0.9687 - val_prc: 0.7585\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0014 - accuracy: 0.9992 - precision: 0.7418 - recall: 0.8359 - auc: 0.9813 - prc: 0.7692 - val_loss: 0.0072 - val_accuracy: 0.9991 - val_precision: 0.6538 - val_recall: 0.8095 - val_auc: 0.9759 - val_prc: 0.7597\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0014 - accuracy: 0.9992 - precision: 0.7403 - recall: 0.8297 - auc: 0.9788 - prc: 0.7660 - val_loss: 0.0064 - val_accuracy: 0.9992 - val_precision: 0.6800 - val_recall: 0.8095 - val_auc: 0.9695 - val_prc: 0.7590\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0014 - accuracy: 0.9992 - precision: 0.7302 - recall: 0.8297 - auc: 0.9828 - prc: 0.7714 - val_loss: 0.0066 - val_accuracy: 0.9992 - val_precision: 0.6892 - val_recall: 0.8095 - val_auc: 0.9688 - val_prc: 0.7572\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0014 - accuracy: 0.9992 - precision: 0.7337 - recall: 0.8359 - auc: 0.9781 - prc: 0.7650 - val_loss: 0.0058 - val_accuracy: 0.9993 - val_precision: 0.7246 - val_recall: 0.7937 - val_auc: 0.9696 - val_prc: 0.7592\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0014 - accuracy: 0.9992 - precision: 0.7486 - recall: 0.8297 - auc: 0.9777 - prc: 0.7666 - val_loss: 0.0075 - val_accuracy: 0.9990 - val_precision: 0.6145 - val_recall: 0.8095 - val_auc: 0.9752 - val_prc: 0.7575\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0014 - accuracy: 0.9991 - precision: 0.7212 - recall: 0.8328 - auc: 0.9797 - prc: 0.7659 - val_loss: 0.0060 - val_accuracy: 0.9993 - val_precision: 0.7143 - val_recall: 0.7937 - val_auc: 0.9699 - val_prc: 0.7578\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0014 - accuracy: 0.9992 - precision: 0.7431 - recall: 0.8328 - auc: 0.9816 - prc: 0.7663 - val_loss: 0.0067 - val_accuracy: 0.9992 - val_precision: 0.6711 - val_recall: 0.8095 - val_auc: 0.9688 - val_prc: 0.7584\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0014 - accuracy: 0.9992 - precision: 0.7438 - recall: 0.8359 - auc: 0.9815 - prc: 0.7703 - val_loss: 0.0073 - val_accuracy: 0.9991 - val_precision: 0.6296 - val_recall: 0.8095 - val_auc: 0.9755 - val_prc: 0.7565\n",
            "1419/1419 [==============================] - 2s 2ms/step\n",
            "******************************for w =  0.07\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_5 (Conv1D)           (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88/89 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9974 - precision: 0.3476 - recall: 0.5942 - auc: 0.8718 - prc: 0.5362"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 15ms/step - loss: 0.0072 - accuracy: 0.9974 - precision: 0.3506 - recall: 0.5959 - auc: 0.8725 - prc: 0.5388 - val_loss: 0.0124 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9516 - val_prc: 0.6766\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0022 - accuracy: 0.9994 - precision: 0.8467 - recall: 0.7864 - auc: 0.9409 - prc: 0.7280 - val_loss: 0.0106 - val_accuracy: 0.9994 - val_precision: 0.7812 - val_recall: 0.7937 - val_auc: 0.9676 - val_prc: 0.7078\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0019 - accuracy: 0.9993 - precision: 0.8137 - recall: 0.8111 - auc: 0.9569 - prc: 0.7532 - val_loss: 0.0075 - val_accuracy: 0.9994 - val_precision: 0.7500 - val_recall: 0.8095 - val_auc: 0.9688 - val_prc: 0.7417\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8067 - recall: 0.8142 - auc: 0.9660 - prc: 0.7614 - val_loss: 0.0079 - val_accuracy: 0.9993 - val_precision: 0.7083 - val_recall: 0.8095 - val_auc: 0.9683 - val_prc: 0.7448\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.7958 - recall: 0.8204 - auc: 0.9694 - prc: 0.7659 - val_loss: 0.0063 - val_accuracy: 0.9993 - val_precision: 0.7246 - val_recall: 0.7937 - val_auc: 0.9686 - val_prc: 0.7606\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7811 - recall: 0.8173 - auc: 0.9745 - prc: 0.7632 - val_loss: 0.0063 - val_accuracy: 0.9993 - val_precision: 0.7083 - val_recall: 0.8095 - val_auc: 0.9686 - val_prc: 0.7577\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7929 - recall: 0.8297 - auc: 0.9729 - prc: 0.7645 - val_loss: 0.0070 - val_accuracy: 0.9993 - val_precision: 0.6986 - val_recall: 0.8095 - val_auc: 0.9742 - val_prc: 0.7598\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7681 - recall: 0.8204 - auc: 0.9801 - prc: 0.7626 - val_loss: 0.0083 - val_accuracy: 0.9991 - val_precision: 0.6456 - val_recall: 0.8095 - val_auc: 0.9736 - val_prc: 0.7443\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7622 - recall: 0.8235 - auc: 0.9770 - prc: 0.7692 - val_loss: 0.0072 - val_accuracy: 0.9992 - val_precision: 0.6892 - val_recall: 0.8095 - val_auc: 0.9742 - val_prc: 0.7589\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0015 - accuracy: 0.9993 - precision: 0.7733 - recall: 0.8235 - auc: 0.9765 - prc: 0.7653 - val_loss: 0.0071 - val_accuracy: 0.9992 - val_precision: 0.6800 - val_recall: 0.8095 - val_auc: 0.9746 - val_prc: 0.7582\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7465 - recall: 0.8297 - auc: 0.9795 - prc: 0.7662 - val_loss: 0.0061 - val_accuracy: 0.9993 - val_precision: 0.7083 - val_recall: 0.8095 - val_auc: 0.9691 - val_prc: 0.7570\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7614 - recall: 0.8297 - auc: 0.9718 - prc: 0.7712 - val_loss: 0.0063 - val_accuracy: 0.9993 - val_precision: 0.6986 - val_recall: 0.8095 - val_auc: 0.9762 - val_prc: 0.7597\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7535 - recall: 0.8235 - auc: 0.9804 - prc: 0.7670 - val_loss: 0.0079 - val_accuracy: 0.9990 - val_precision: 0.6071 - val_recall: 0.8095 - val_auc: 0.9748 - val_prc: 0.7423\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7500 - recall: 0.8266 - auc: 0.9763 - prc: 0.7719 - val_loss: 0.0081 - val_accuracy: 0.9990 - val_precision: 0.5930 - val_recall: 0.8095 - val_auc: 0.9738 - val_prc: 0.7553\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7564 - recall: 0.8266 - auc: 0.9786 - prc: 0.7674 - val_loss: 0.0065 - val_accuracy: 0.9992 - val_precision: 0.6711 - val_recall: 0.8095 - val_auc: 0.9691 - val_prc: 0.7583\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7607 - recall: 0.8266 - auc: 0.9776 - prc: 0.7671 - val_loss: 0.0062 - val_accuracy: 0.9993 - val_precision: 0.7083 - val_recall: 0.8095 - val_auc: 0.9696 - val_prc: 0.7579\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7635 - recall: 0.8297 - auc: 0.9802 - prc: 0.7725 - val_loss: 0.0062 - val_accuracy: 0.9992 - val_precision: 0.6944 - val_recall: 0.7937 - val_auc: 0.9697 - val_prc: 0.7575\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0014 - accuracy: 0.9993 - precision: 0.7759 - recall: 0.8359 - auc: 0.9795 - prc: 0.7718 - val_loss: 0.0080 - val_accuracy: 0.9989 - val_precision: 0.5795 - val_recall: 0.8095 - val_auc: 0.9758 - val_prc: 0.7542\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7542 - recall: 0.8359 - auc: 0.9826 - prc: 0.7721 - val_loss: 0.0068 - val_accuracy: 0.9991 - val_precision: 0.6296 - val_recall: 0.8095 - val_auc: 0.9766 - val_prc: 0.7414\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0014 - accuracy: 0.9992 - precision: 0.7556 - recall: 0.8328 - auc: 0.9821 - prc: 0.7746 - val_loss: 0.0066 - val_accuracy: 0.9991 - val_precision: 0.6533 - val_recall: 0.7778 - val_auc: 0.9762 - val_prc: 0.7536\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "******************************for w =  0.08\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_6 (Conv1D)           (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88/89 [============================>.] - ETA: 0s - loss: 0.0084 - accuracy: 0.9934 - precision: 0.1426 - recall: 0.5729 - auc: 0.8676 - prc: 0.4690"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 15ms/step - loss: 0.0083 - accuracy: 0.9934 - precision: 0.1437 - recall: 0.5751 - auc: 0.8687 - prc: 0.4719 - val_loss: 0.0112 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9504 - val_prc: 0.6813\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0022 - accuracy: 0.9994 - precision: 0.8542 - recall: 0.7802 - auc: 0.9442 - prc: 0.7346 - val_loss: 0.0077 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9684 - val_prc: 0.7163\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 0.0019 - accuracy: 0.9993 - precision: 0.8125 - recall: 0.8050 - auc: 0.9574 - prc: 0.7464 - val_loss: 0.0068 - val_accuracy: 0.9993 - val_precision: 0.7391 - val_recall: 0.8095 - val_auc: 0.9696 - val_prc: 0.7299\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0018 - accuracy: 0.9993 - precision: 0.8162 - recall: 0.8111 - auc: 0.9690 - prc: 0.7583 - val_loss: 0.0074 - val_accuracy: 0.9993 - val_precision: 0.7083 - val_recall: 0.8095 - val_auc: 0.9684 - val_prc: 0.7439\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.7970 - recall: 0.8142 - auc: 0.9714 - prc: 0.7615 - val_loss: 0.0067 - val_accuracy: 0.9993 - val_precision: 0.7083 - val_recall: 0.8095 - val_auc: 0.9691 - val_prc: 0.7570\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.7910 - recall: 0.8204 - auc: 0.9749 - prc: 0.7591 - val_loss: 0.0063 - val_accuracy: 0.9993 - val_precision: 0.7083 - val_recall: 0.8095 - val_auc: 0.9690 - val_prc: 0.7573\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7881 - recall: 0.8173 - auc: 0.9738 - prc: 0.7602 - val_loss: 0.0060 - val_accuracy: 0.9993 - val_precision: 0.6986 - val_recall: 0.8095 - val_auc: 0.9622 - val_prc: 0.7582\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7940 - recall: 0.8235 - auc: 0.9758 - prc: 0.7653 - val_loss: 0.0060 - val_accuracy: 0.9993 - val_precision: 0.7286 - val_recall: 0.8095 - val_auc: 0.9616 - val_prc: 0.7601\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7847 - recall: 0.8235 - auc: 0.9747 - prc: 0.7663 - val_loss: 0.0046 - val_accuracy: 0.9995 - val_precision: 0.8421 - val_recall: 0.7619 - val_auc: 0.9633 - val_prc: 0.7542\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7830 - recall: 0.8266 - auc: 0.9783 - prc: 0.7638 - val_loss: 0.0083 - val_accuracy: 0.9991 - val_precision: 0.6375 - val_recall: 0.8095 - val_auc: 0.9739 - val_prc: 0.7440\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9993 - precision: 0.7813 - recall: 0.8297 - auc: 0.9788 - prc: 0.7694 - val_loss: 0.0049 - val_accuracy: 0.9994 - val_precision: 0.7778 - val_recall: 0.7778 - val_auc: 0.9635 - val_prc: 0.7575\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9993 - precision: 0.7788 - recall: 0.8173 - auc: 0.9732 - prc: 0.7672 - val_loss: 0.0062 - val_accuracy: 0.9993 - val_precision: 0.6986 - val_recall: 0.8095 - val_auc: 0.9689 - val_prc: 0.7567\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0015 - accuracy: 0.9993 - precision: 0.7746 - recall: 0.8297 - auc: 0.9739 - prc: 0.7695 - val_loss: 0.0059 - val_accuracy: 0.9993 - val_precision: 0.7143 - val_recall: 0.7937 - val_auc: 0.9696 - val_prc: 0.7428\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9993 - precision: 0.7859 - recall: 0.8297 - auc: 0.9738 - prc: 0.7695 - val_loss: 0.0061 - val_accuracy: 0.9993 - val_precision: 0.7143 - val_recall: 0.7937 - val_auc: 0.9692 - val_prc: 0.7562\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7635 - recall: 0.8297 - auc: 0.9779 - prc: 0.7705 - val_loss: 0.0057 - val_accuracy: 0.9993 - val_precision: 0.7042 - val_recall: 0.7937 - val_auc: 0.9698 - val_prc: 0.7560\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0015 - accuracy: 0.9993 - precision: 0.7778 - recall: 0.8235 - auc: 0.9714 - prc: 0.7714 - val_loss: 0.0079 - val_accuracy: 0.9990 - val_precision: 0.6000 - val_recall: 0.8095 - val_auc: 0.9758 - val_prc: 0.7528\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7599 - recall: 0.8328 - auc: 0.9742 - prc: 0.7712 - val_loss: 0.0048 - val_accuracy: 0.9994 - val_precision: 0.8276 - val_recall: 0.7619 - val_auc: 0.9709 - val_prc: 0.7533\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7637 - recall: 0.8204 - auc: 0.9727 - prc: 0.7675 - val_loss: 0.0054 - val_accuracy: 0.9993 - val_precision: 0.7353 - val_recall: 0.7937 - val_auc: 0.9699 - val_prc: 0.7565\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9993 - precision: 0.7649 - recall: 0.8359 - auc: 0.9731 - prc: 0.7685 - val_loss: 0.0066 - val_accuracy: 0.9992 - val_precision: 0.6757 - val_recall: 0.7937 - val_auc: 0.9683 - val_prc: 0.7554\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0015 - accuracy: 0.9992 - precision: 0.7607 - recall: 0.8266 - auc: 0.9806 - prc: 0.7698 - val_loss: 0.0052 - val_accuracy: 0.9993 - val_precision: 0.7246 - val_recall: 0.7937 - val_auc: 0.9706 - val_prc: 0.7558\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "******************************for w =  0.09\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_7 (Conv1D)           (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85/89 [===========================>..] - ETA: 0s - loss: 0.0090 - accuracy: 0.9950 - precision: 0.1731 - recall: 0.5257 - auc: 0.8507 - prc: 0.4364"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 16ms/step - loss: 0.0088 - accuracy: 0.9951 - precision: 0.1824 - recall: 0.5363 - auc: 0.8531 - prc: 0.4484 - val_loss: 0.0093 - val_accuracy: 0.9994 - val_precision: 0.7833 - val_recall: 0.7460 - val_auc: 0.9468 - val_prc: 0.6548\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0023 - accuracy: 0.9994 - precision: 0.8542 - recall: 0.7802 - auc: 0.9375 - prc: 0.7248 - val_loss: 0.0086 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9661 - val_prc: 0.7170\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0019 - accuracy: 0.9993 - precision: 0.8290 - recall: 0.7957 - auc: 0.9566 - prc: 0.7538 - val_loss: 0.0070 - val_accuracy: 0.9993 - val_precision: 0.7083 - val_recall: 0.8095 - val_auc: 0.9702 - val_prc: 0.7417\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9993 - precision: 0.8208 - recall: 0.8080 - auc: 0.9596 - prc: 0.7560 - val_loss: 0.0060 - val_accuracy: 0.9993 - val_precision: 0.7391 - val_recall: 0.8095 - val_auc: 0.9702 - val_prc: 0.7431\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.7970 - recall: 0.8142 - auc: 0.9695 - prc: 0.7608 - val_loss: 0.0058 - val_accuracy: 0.9993 - val_precision: 0.7183 - val_recall: 0.8095 - val_auc: 0.9702 - val_prc: 0.7572\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8055 - recall: 0.8204 - auc: 0.9738 - prc: 0.7606 - val_loss: 0.0054 - val_accuracy: 0.9993 - val_precision: 0.7463 - val_recall: 0.7937 - val_auc: 0.9703 - val_prc: 0.7581\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8024 - recall: 0.8173 - auc: 0.9714 - prc: 0.7592 - val_loss: 0.0061 - val_accuracy: 0.9993 - val_precision: 0.6986 - val_recall: 0.8095 - val_auc: 0.9694 - val_prc: 0.7584\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7899 - recall: 0.8266 - auc: 0.9774 - prc: 0.7624 - val_loss: 0.0057 - val_accuracy: 0.9993 - val_precision: 0.7183 - val_recall: 0.8095 - val_auc: 0.9623 - val_prc: 0.7600\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7857 - recall: 0.8173 - auc: 0.9777 - prc: 0.7646 - val_loss: 0.0050 - val_accuracy: 0.9993 - val_precision: 0.7424 - val_recall: 0.7778 - val_auc: 0.9634 - val_prc: 0.7589\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 0.0016 - accuracy: 0.9992 - precision: 0.7697 - recall: 0.8173 - auc: 0.9725 - prc: 0.7651 - val_loss: 0.0057 - val_accuracy: 0.9993 - val_precision: 0.7463 - val_recall: 0.7937 - val_auc: 0.9696 - val_prc: 0.7569\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7778 - recall: 0.8235 - auc: 0.9682 - prc: 0.7639 - val_loss: 0.0060 - val_accuracy: 0.9993 - val_precision: 0.7042 - val_recall: 0.7937 - val_auc: 0.9694 - val_prc: 0.7591\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7847 - recall: 0.8235 - auc: 0.9765 - prc: 0.7620 - val_loss: 0.0045 - val_accuracy: 0.9994 - val_precision: 0.8167 - val_recall: 0.7778 - val_auc: 0.9638 - val_prc: 0.7539\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7840 - recall: 0.8204 - auc: 0.9682 - prc: 0.7668 - val_loss: 0.0048 - val_accuracy: 0.9994 - val_precision: 0.7778 - val_recall: 0.7778 - val_auc: 0.9636 - val_prc: 0.7567\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7840 - recall: 0.8204 - auc: 0.9710 - prc: 0.7647 - val_loss: 0.0062 - val_accuracy: 0.9992 - val_precision: 0.6849 - val_recall: 0.7937 - val_auc: 0.9693 - val_prc: 0.7557\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0015 - accuracy: 0.9993 - precision: 0.7859 - recall: 0.8297 - auc: 0.9720 - prc: 0.7697 - val_loss: 0.0047 - val_accuracy: 0.9994 - val_precision: 0.7778 - val_recall: 0.7778 - val_auc: 0.9638 - val_prc: 0.7546\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9993 - precision: 0.7953 - recall: 0.8297 - auc: 0.9708 - prc: 0.7682 - val_loss: 0.0046 - val_accuracy: 0.9995 - val_precision: 0.8305 - val_recall: 0.7778 - val_auc: 0.9715 - val_prc: 0.7547\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9993 - precision: 0.7730 - recall: 0.8328 - auc: 0.9699 - prc: 0.7688 - val_loss: 0.0053 - val_accuracy: 0.9994 - val_precision: 0.7656 - val_recall: 0.7778 - val_auc: 0.9701 - val_prc: 0.7576\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9993 - precision: 0.7813 - recall: 0.8297 - auc: 0.9711 - prc: 0.7689 - val_loss: 0.0067 - val_accuracy: 0.9991 - val_precision: 0.6410 - val_recall: 0.7937 - val_auc: 0.9693 - val_prc: 0.7546\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9993 - precision: 0.7749 - recall: 0.8204 - auc: 0.9744 - prc: 0.7697 - val_loss: 0.0059 - val_accuracy: 0.9992 - val_precision: 0.6757 - val_recall: 0.7937 - val_auc: 0.9695 - val_prc: 0.7556\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9993 - precision: 0.7784 - recall: 0.8266 - auc: 0.9726 - prc: 0.7668 - val_loss: 0.0047 - val_accuracy: 0.9995 - val_precision: 0.8305 - val_recall: 0.7778 - val_auc: 0.9710 - val_prc: 0.7676\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "******************************for w =  0.09999999999999999\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_8 (Conv1D)           (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86/89 [===========================>..] - ETA: 0s - loss: 0.0090 - accuracy: 0.9980 - precision: 0.4265 - recall: 0.5464 - auc: 0.8601 - prc: 0.4719"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 15ms/step - loss: 0.0088 - accuracy: 0.9980 - precision: 0.4318 - recall: 0.5492 - auc: 0.8619 - prc: 0.4738 - val_loss: 0.0077 - val_accuracy: 0.9994 - val_precision: 0.8246 - val_recall: 0.7460 - val_auc: 0.9504 - val_prc: 0.6938\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0024 - accuracy: 0.9994 - precision: 0.8794 - recall: 0.7678 - auc: 0.9398 - prc: 0.7146 - val_loss: 0.0070 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9691 - val_prc: 0.7132\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0020 - accuracy: 0.9994 - precision: 0.8421 - recall: 0.7926 - auc: 0.9521 - prc: 0.7520 - val_loss: 0.0069 - val_accuracy: 0.9993 - val_precision: 0.7183 - val_recall: 0.8095 - val_auc: 0.9704 - val_prc: 0.7433\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0019 - accuracy: 0.9993 - precision: 0.8086 - recall: 0.8111 - auc: 0.9577 - prc: 0.7573 - val_loss: 0.0058 - val_accuracy: 0.9993 - val_precision: 0.7143 - val_recall: 0.7937 - val_auc: 0.9702 - val_prc: 0.7585\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0018 - accuracy: 0.9993 - precision: 0.7970 - recall: 0.8142 - auc: 0.9658 - prc: 0.7619 - val_loss: 0.0054 - val_accuracy: 0.9993 - val_precision: 0.7463 - val_recall: 0.7937 - val_auc: 0.9706 - val_prc: 0.7604\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8049 - recall: 0.8173 - auc: 0.9730 - prc: 0.7584 - val_loss: 0.0045 - val_accuracy: 0.9994 - val_precision: 0.8167 - val_recall: 0.7778 - val_auc: 0.9640 - val_prc: 0.7584\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8092 - recall: 0.8142 - auc: 0.9706 - prc: 0.7610 - val_loss: 0.0045 - val_accuracy: 0.9994 - val_precision: 0.8136 - val_recall: 0.7619 - val_auc: 0.9638 - val_prc: 0.7557\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8024 - recall: 0.8173 - auc: 0.9712 - prc: 0.7619 - val_loss: 0.0057 - val_accuracy: 0.9993 - val_precision: 0.7183 - val_recall: 0.8095 - val_auc: 0.9699 - val_prc: 0.7597\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.8123 - recall: 0.8173 - auc: 0.9755 - prc: 0.7668 - val_loss: 0.0076 - val_accuracy: 0.9991 - val_precision: 0.6296 - val_recall: 0.8095 - val_auc: 0.9744 - val_prc: 0.7564\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7857 - recall: 0.8173 - auc: 0.9709 - prc: 0.7632 - val_loss: 0.0057 - val_accuracy: 0.9993 - val_precision: 0.6986 - val_recall: 0.8095 - val_auc: 0.9698 - val_prc: 0.7592\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7923 - recall: 0.8266 - auc: 0.9744 - prc: 0.7637 - val_loss: 0.0057 - val_accuracy: 0.9993 - val_precision: 0.7246 - val_recall: 0.7937 - val_auc: 0.9697 - val_prc: 0.7581\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7946 - recall: 0.8266 - auc: 0.9756 - prc: 0.7643 - val_loss: 0.0046 - val_accuracy: 0.9994 - val_precision: 0.7778 - val_recall: 0.7778 - val_auc: 0.9558 - val_prc: 0.7564\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.8055 - recall: 0.8204 - auc: 0.9702 - prc: 0.7654 - val_loss: 0.0056 - val_accuracy: 0.9993 - val_precision: 0.7042 - val_recall: 0.7937 - val_auc: 0.9702 - val_prc: 0.7556\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7801 - recall: 0.8235 - auc: 0.9757 - prc: 0.7676 - val_loss: 0.0048 - val_accuracy: 0.9994 - val_precision: 0.7778 - val_recall: 0.7778 - val_auc: 0.9712 - val_prc: 0.7534\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7853 - recall: 0.8266 - auc: 0.9688 - prc: 0.7689 - val_loss: 0.0048 - val_accuracy: 0.9994 - val_precision: 0.7778 - val_recall: 0.7778 - val_auc: 0.9638 - val_prc: 0.7540\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7840 - recall: 0.8204 - auc: 0.9705 - prc: 0.7654 - val_loss: 0.0064 - val_accuracy: 0.9991 - val_precision: 0.6494 - val_recall: 0.7937 - val_auc: 0.9690 - val_prc: 0.7517\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7859 - recall: 0.8297 - auc: 0.9732 - prc: 0.7655 - val_loss: 0.0055 - val_accuracy: 0.9993 - val_precision: 0.7042 - val_recall: 0.7937 - val_auc: 0.9704 - val_prc: 0.7532\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7870 - recall: 0.8235 - auc: 0.9732 - prc: 0.7681 - val_loss: 0.0047 - val_accuracy: 0.9994 - val_precision: 0.7903 - val_recall: 0.7778 - val_auc: 0.9713 - val_prc: 0.7533\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0015 - accuracy: 0.9993 - precision: 0.7946 - recall: 0.8266 - auc: 0.9749 - prc: 0.7646 - val_loss: 0.0065 - val_accuracy: 0.9991 - val_precision: 0.6375 - val_recall: 0.8095 - val_auc: 0.9689 - val_prc: 0.7523\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0015 - accuracy: 0.9993 - precision: 0.8000 - recall: 0.8297 - auc: 0.9734 - prc: 0.7654 - val_loss: 0.0050 - val_accuracy: 0.9994 - val_precision: 0.7576 - val_recall: 0.7937 - val_auc: 0.9712 - val_prc: 0.7552\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "******************************for w =  0.10999999999999999\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_9 (Conv1D)           (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_9 (Flatten)         (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86/89 [===========================>..] - ETA: 0s - loss: 0.0095 - accuracy: 0.9959 - precision: 0.2119 - recall: 0.5079 - auc: 0.8437 - prc: 0.4385"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 15ms/step - loss: 0.0093 - accuracy: 0.9960 - precision: 0.2180 - recall: 0.5155 - auc: 0.8478 - prc: 0.4489 - val_loss: 0.0089 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9458 - val_prc: 0.6672\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0023 - accuracy: 0.9994 - precision: 0.8635 - recall: 0.7833 - auc: 0.9421 - prc: 0.7269 - val_loss: 0.0068 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9688 - val_prc: 0.7094\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0020 - accuracy: 0.9994 - precision: 0.8505 - recall: 0.7926 - auc: 0.9540 - prc: 0.7478 - val_loss: 0.0056 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9704 - val_prc: 0.7357\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0019 - accuracy: 0.9994 - precision: 0.8371 - recall: 0.7957 - auc: 0.9595 - prc: 0.7541 - val_loss: 0.0055 - val_accuracy: 0.9993 - val_precision: 0.7353 - val_recall: 0.7937 - val_auc: 0.9705 - val_prc: 0.7558\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9993 - precision: 0.8094 - recall: 0.8019 - auc: 0.9659 - prc: 0.7544 - val_loss: 0.0049 - val_accuracy: 0.9994 - val_precision: 0.7812 - val_recall: 0.7937 - val_auc: 0.9635 - val_prc: 0.7546\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.7946 - recall: 0.8142 - auc: 0.9719 - prc: 0.7606 - val_loss: 0.0050 - val_accuracy: 0.9994 - val_precision: 0.7576 - val_recall: 0.7937 - val_auc: 0.9634 - val_prc: 0.7546\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8137 - recall: 0.8111 - auc: 0.9714 - prc: 0.7632 - val_loss: 0.0060 - val_accuracy: 0.9992 - val_precision: 0.6892 - val_recall: 0.8095 - val_auc: 0.9692 - val_prc: 0.7564\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8168 - recall: 0.8142 - auc: 0.9726 - prc: 0.7602 - val_loss: 0.0047 - val_accuracy: 0.9994 - val_precision: 0.7812 - val_recall: 0.7937 - val_auc: 0.9561 - val_prc: 0.7564\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.7976 - recall: 0.8173 - auc: 0.9703 - prc: 0.7577 - val_loss: 0.0054 - val_accuracy: 0.9993 - val_precision: 0.7246 - val_recall: 0.7937 - val_auc: 0.9632 - val_prc: 0.7425\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7964 - recall: 0.8235 - auc: 0.9701 - prc: 0.7627 - val_loss: 0.0045 - val_accuracy: 0.9994 - val_precision: 0.7692 - val_recall: 0.7937 - val_auc: 0.9562 - val_prc: 0.7540\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.8168 - recall: 0.8142 - auc: 0.9660 - prc: 0.7614 - val_loss: 0.0059 - val_accuracy: 0.9992 - val_precision: 0.6711 - val_recall: 0.8095 - val_auc: 0.9700 - val_prc: 0.7418\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7982 - recall: 0.8204 - auc: 0.9691 - prc: 0.7648 - val_loss: 0.0065 - val_accuracy: 0.9991 - val_precision: 0.6494 - val_recall: 0.7937 - val_auc: 0.9688 - val_prc: 0.7536\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7853 - recall: 0.8266 - auc: 0.9700 - prc: 0.7629 - val_loss: 0.0045 - val_accuracy: 0.9994 - val_precision: 0.7812 - val_recall: 0.7937 - val_auc: 0.9560 - val_prc: 0.7530\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.8012 - recall: 0.8235 - auc: 0.9719 - prc: 0.7633 - val_loss: 0.0047 - val_accuracy: 0.9994 - val_precision: 0.7692 - val_recall: 0.7937 - val_auc: 0.9560 - val_prc: 0.7549\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.8085 - recall: 0.8235 - auc: 0.9690 - prc: 0.7646 - val_loss: 0.0058 - val_accuracy: 0.9992 - val_precision: 0.6667 - val_recall: 0.7937 - val_auc: 0.9702 - val_prc: 0.7517\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7940 - recall: 0.8235 - auc: 0.9705 - prc: 0.7626 - val_loss: 0.0049 - val_accuracy: 0.9993 - val_precision: 0.7538 - val_recall: 0.7778 - val_auc: 0.9635 - val_prc: 0.7528\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.8042 - recall: 0.8266 - auc: 0.9704 - prc: 0.7726 - val_loss: 0.0042 - val_accuracy: 0.9995 - val_precision: 0.8448 - val_recall: 0.7778 - val_auc: 0.9565 - val_prc: 0.7672\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7904 - recall: 0.8173 - auc: 0.9662 - prc: 0.7664 - val_loss: 0.0042 - val_accuracy: 0.9995 - val_precision: 0.8421 - val_recall: 0.7619 - val_auc: 0.9564 - val_prc: 0.7672\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.8006 - recall: 0.8328 - auc: 0.9692 - prc: 0.7682 - val_loss: 0.0047 - val_accuracy: 0.9994 - val_precision: 0.7778 - val_recall: 0.7778 - val_auc: 0.9712 - val_prc: 0.7539\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7923 - recall: 0.8266 - auc: 0.9736 - prc: 0.7680 - val_loss: 0.0047 - val_accuracy: 0.9994 - val_precision: 0.7778 - val_recall: 0.7778 - val_auc: 0.9638 - val_prc: 0.7532\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "******************************for w =  0.11999999999999998\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_10 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_10 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85/89 [===========================>..] - ETA: 0s - loss: 0.0117 - accuracy: 0.9901 - precision: 0.0860 - recall: 0.5040 - auc: 0.8225 - prc: 0.4042"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 15ms/step - loss: 0.0113 - accuracy: 0.9904 - precision: 0.0906 - recall: 0.5130 - auc: 0.8279 - prc: 0.4161 - val_loss: 0.0082 - val_accuracy: 0.9994 - val_precision: 0.7966 - val_recall: 0.7460 - val_auc: 0.9465 - val_prc: 0.6624\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0024 - accuracy: 0.9994 - precision: 0.8768 - recall: 0.7709 - auc: 0.9374 - prc: 0.7244 - val_loss: 0.0066 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9690 - val_prc: 0.7132\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0020 - accuracy: 0.9994 - precision: 0.8487 - recall: 0.7988 - auc: 0.9563 - prc: 0.7445 - val_loss: 0.0052 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9712 - val_prc: 0.7521\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0019 - accuracy: 0.9994 - precision: 0.8275 - recall: 0.8019 - auc: 0.9529 - prc: 0.7512 - val_loss: 0.0049 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9717 - val_prc: 0.7560\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8452 - recall: 0.8111 - auc: 0.9632 - prc: 0.7553 - val_loss: 0.0049 - val_accuracy: 0.9994 - val_precision: 0.7727 - val_recall: 0.8095 - val_auc: 0.9714 - val_prc: 0.7592\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9993 - precision: 0.8168 - recall: 0.8142 - auc: 0.9654 - prc: 0.7570 - val_loss: 0.0050 - val_accuracy: 0.9994 - val_precision: 0.7969 - val_recall: 0.8095 - val_auc: 0.9634 - val_prc: 0.7595\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8265 - recall: 0.8111 - auc: 0.9636 - prc: 0.7592 - val_loss: 0.0050 - val_accuracy: 0.9994 - val_precision: 0.7612 - val_recall: 0.8095 - val_auc: 0.9636 - val_prc: 0.7559\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8067 - recall: 0.8142 - auc: 0.9683 - prc: 0.7576 - val_loss: 0.0050 - val_accuracy: 0.9993 - val_precision: 0.7391 - val_recall: 0.8095 - val_auc: 0.9639 - val_prc: 0.7572\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8230 - recall: 0.8204 - auc: 0.9674 - prc: 0.7635 - val_loss: 0.0046 - val_accuracy: 0.9995 - val_precision: 0.8197 - val_recall: 0.7937 - val_auc: 0.9638 - val_prc: 0.7554\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8049 - recall: 0.8173 - auc: 0.9644 - prc: 0.7591 - val_loss: 0.0050 - val_accuracy: 0.9994 - val_precision: 0.7812 - val_recall: 0.7937 - val_auc: 0.9706 - val_prc: 0.7537\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8117 - recall: 0.8142 - auc: 0.9657 - prc: 0.7647 - val_loss: 0.0049 - val_accuracy: 0.9993 - val_precision: 0.7353 - val_recall: 0.7937 - val_auc: 0.9710 - val_prc: 0.7532\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8204 - recall: 0.8204 - auc: 0.9632 - prc: 0.7659 - val_loss: 0.0060 - val_accuracy: 0.9992 - val_precision: 0.6711 - val_recall: 0.8095 - val_auc: 0.9698 - val_prc: 0.7546\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8043 - recall: 0.8142 - auc: 0.9688 - prc: 0.7674 - val_loss: 0.0049 - val_accuracy: 0.9993 - val_precision: 0.7463 - val_recall: 0.7937 - val_auc: 0.9636 - val_prc: 0.7553\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.8030 - recall: 0.8204 - auc: 0.9633 - prc: 0.7606 - val_loss: 0.0049 - val_accuracy: 0.9994 - val_precision: 0.7576 - val_recall: 0.7937 - val_auc: 0.9712 - val_prc: 0.7561\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9994 - precision: 0.8230 - recall: 0.8204 - auc: 0.9679 - prc: 0.7634 - val_loss: 0.0043 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9570 - val_prc: 0.7515\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.8079 - recall: 0.8204 - auc: 0.9681 - prc: 0.7656 - val_loss: 0.0054 - val_accuracy: 0.9993 - val_precision: 0.7353 - val_recall: 0.7937 - val_auc: 0.9705 - val_prc: 0.7529\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.8006 - recall: 0.8204 - auc: 0.9664 - prc: 0.7669 - val_loss: 0.0052 - val_accuracy: 0.9993 - val_precision: 0.7463 - val_recall: 0.7937 - val_auc: 0.9708 - val_prc: 0.7489\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9994 - precision: 0.8360 - recall: 0.8204 - auc: 0.9691 - prc: 0.7640 - val_loss: 0.0052 - val_accuracy: 0.9994 - val_precision: 0.7576 - val_recall: 0.7937 - val_auc: 0.9708 - val_prc: 0.7513\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9994 - precision: 0.8173 - recall: 0.8173 - auc: 0.9676 - prc: 0.7670 - val_loss: 0.0051 - val_accuracy: 0.9994 - val_precision: 0.7576 - val_recall: 0.7937 - val_auc: 0.9710 - val_prc: 0.7499\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7988 - recall: 0.8111 - auc: 0.9692 - prc: 0.7669 - val_loss: 0.0048 - val_accuracy: 0.9994 - val_precision: 0.7656 - val_recall: 0.7778 - val_auc: 0.9715 - val_prc: 0.7507\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "******************************for w =  0.12999999999999998\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_11 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_11 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88/89 [============================>.] - ETA: 0s - loss: 0.0114 - accuracy: 0.9947 - precision: 0.1619 - recall: 0.5092 - auc: 0.8099 - prc: 0.4595"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 15ms/step - loss: 0.0114 - accuracy: 0.9947 - precision: 0.1625 - recall: 0.5052 - auc: 0.8055 - prc: 0.4508 - val_loss: 0.0097 - val_accuracy: 0.9994 - val_precision: 0.8000 - val_recall: 0.7619 - val_auc: 0.9578 - val_prc: 0.6703\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0024 - accuracy: 0.9994 - precision: 0.8552 - recall: 0.7678 - auc: 0.9473 - prc: 0.7260 - val_loss: 0.0055 - val_accuracy: 0.9994 - val_precision: 0.8167 - val_recall: 0.7778 - val_auc: 0.9704 - val_prc: 0.7038\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0021 - accuracy: 0.9994 - precision: 0.8576 - recall: 0.7833 - auc: 0.9495 - prc: 0.7464 - val_loss: 0.0059 - val_accuracy: 0.9994 - val_precision: 0.7612 - val_recall: 0.8095 - val_auc: 0.9703 - val_prc: 0.7146\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0019 - accuracy: 0.9994 - precision: 0.8350 - recall: 0.7988 - auc: 0.9624 - prc: 0.7528 - val_loss: 0.0046 - val_accuracy: 0.9994 - val_precision: 0.8065 - val_recall: 0.7937 - val_auc: 0.9645 - val_prc: 0.7544\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8355 - recall: 0.8019 - auc: 0.9686 - prc: 0.7506 - val_loss: 0.0051 - val_accuracy: 0.9993 - val_precision: 0.7391 - val_recall: 0.8095 - val_auc: 0.9718 - val_prc: 0.7564\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0018 - accuracy: 0.9993 - precision: 0.8182 - recall: 0.8080 - auc: 0.9629 - prc: 0.7544 - val_loss: 0.0047 - val_accuracy: 0.9994 - val_precision: 0.7778 - val_recall: 0.7778 - val_auc: 0.9642 - val_prc: 0.7551\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8245 - recall: 0.8142 - auc: 0.9659 - prc: 0.7528 - val_loss: 0.0056 - val_accuracy: 0.9993 - val_precision: 0.7183 - val_recall: 0.8095 - val_auc: 0.9627 - val_prc: 0.7540\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8086 - recall: 0.8111 - auc: 0.9671 - prc: 0.7629 - val_loss: 0.0046 - val_accuracy: 0.9994 - val_precision: 0.7778 - val_recall: 0.7778 - val_auc: 0.9565 - val_prc: 0.7530\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8265 - recall: 0.8111 - auc: 0.9702 - prc: 0.7586 - val_loss: 0.0042 - val_accuracy: 0.9994 - val_precision: 0.8033 - val_recall: 0.7778 - val_auc: 0.9570 - val_prc: 0.7514\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8276 - recall: 0.8173 - auc: 0.9632 - prc: 0.7609 - val_loss: 0.0038 - val_accuracy: 0.9994 - val_precision: 0.8276 - val_recall: 0.7619 - val_auc: 0.9576 - val_prc: 0.7548\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8086 - recall: 0.8111 - auc: 0.9649 - prc: 0.7564 - val_loss: 0.0045 - val_accuracy: 0.9994 - val_precision: 0.7656 - val_recall: 0.7778 - val_auc: 0.9565 - val_prc: 0.7547\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8125 - recall: 0.8050 - auc: 0.9695 - prc: 0.7611 - val_loss: 0.0050 - val_accuracy: 0.9993 - val_precision: 0.7463 - val_recall: 0.7937 - val_auc: 0.9561 - val_prc: 0.7522\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8146 - recall: 0.8297 - auc: 0.9664 - prc: 0.7596 - val_loss: 0.0043 - val_accuracy: 0.9994 - val_precision: 0.7778 - val_recall: 0.7778 - val_auc: 0.9566 - val_prc: 0.7531\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8179 - recall: 0.8204 - auc: 0.9665 - prc: 0.7650 - val_loss: 0.0048 - val_accuracy: 0.9993 - val_precision: 0.7353 - val_recall: 0.7937 - val_auc: 0.9563 - val_prc: 0.7505\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8150 - recall: 0.8050 - auc: 0.9637 - prc: 0.7626 - val_loss: 0.0048 - val_accuracy: 0.9993 - val_precision: 0.7246 - val_recall: 0.7937 - val_auc: 0.9564 - val_prc: 0.7503\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8098 - recall: 0.8173 - auc: 0.9620 - prc: 0.7606 - val_loss: 0.0044 - val_accuracy: 0.9994 - val_precision: 0.7692 - val_recall: 0.7937 - val_auc: 0.9567 - val_prc: 0.7504\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9994 - precision: 0.8297 - recall: 0.8142 - auc: 0.9624 - prc: 0.7661 - val_loss: 0.0061 - val_accuracy: 0.9991 - val_precision: 0.6579 - val_recall: 0.7937 - val_auc: 0.9696 - val_prc: 0.7457\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.7915 - recall: 0.8111 - auc: 0.9725 - prc: 0.7727 - val_loss: 0.0039 - val_accuracy: 0.9995 - val_precision: 0.8421 - val_recall: 0.7619 - val_auc: 0.9568 - val_prc: 0.7655\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 0.0016 - accuracy: 0.9994 - precision: 0.8245 - recall: 0.8142 - auc: 0.9681 - prc: 0.7690 - val_loss: 0.0041 - val_accuracy: 0.9995 - val_precision: 0.8305 - val_recall: 0.7778 - val_auc: 0.9570 - val_prc: 0.7662\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.8079 - recall: 0.8204 - auc: 0.9650 - prc: 0.7618 - val_loss: 0.0043 - val_accuracy: 0.9994 - val_precision: 0.7656 - val_recall: 0.7778 - val_auc: 0.9568 - val_prc: 0.7498\n",
            "1419/1419 [==============================] - 2s 2ms/step\n",
            "******************************for w =  0.13999999999999999\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_12 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_12 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88/89 [============================>.] - ETA: 0s - loss: 0.0125 - accuracy: 0.9905 - precision: 0.0877 - recall: 0.4856 - auc: 0.8213 - prc: 0.4012"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 16ms/step - loss: 0.0124 - accuracy: 0.9906 - precision: 0.0885 - recall: 0.4870 - auc: 0.8232 - prc: 0.4028 - val_loss: 0.0080 - val_accuracy: 0.9994 - val_precision: 0.8000 - val_recall: 0.7619 - val_auc: 0.9418 - val_prc: 0.6778\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0025 - accuracy: 0.9994 - precision: 0.8702 - recall: 0.7678 - auc: 0.9402 - prc: 0.7248 - val_loss: 0.0058 - val_accuracy: 0.9994 - val_precision: 0.8065 - val_recall: 0.7937 - val_auc: 0.9705 - val_prc: 0.7149\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 0.0021 - accuracy: 0.9994 - precision: 0.8789 - recall: 0.7864 - auc: 0.9534 - prc: 0.7508 - val_loss: 0.0056 - val_accuracy: 0.9994 - val_precision: 0.7576 - val_recall: 0.7937 - val_auc: 0.9710 - val_prc: 0.7539\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0020 - accuracy: 0.9993 - precision: 0.8333 - recall: 0.7895 - auc: 0.9549 - prc: 0.7520 - val_loss: 0.0044 - val_accuracy: 0.9995 - val_precision: 0.8475 - val_recall: 0.7937 - val_auc: 0.9646 - val_prc: 0.7565\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0019 - accuracy: 0.9994 - precision: 0.8360 - recall: 0.8050 - auc: 0.9573 - prc: 0.7564 - val_loss: 0.0053 - val_accuracy: 0.9993 - val_precision: 0.7183 - val_recall: 0.8095 - val_auc: 0.9711 - val_prc: 0.7579\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0019 - accuracy: 0.9994 - precision: 0.8397 - recall: 0.8111 - auc: 0.9628 - prc: 0.7565 - val_loss: 0.0050 - val_accuracy: 0.9994 - val_precision: 0.7576 - val_recall: 0.7937 - val_auc: 0.9638 - val_prc: 0.7554\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9993 - precision: 0.8188 - recall: 0.8111 - auc: 0.9632 - prc: 0.7591 - val_loss: 0.0040 - val_accuracy: 0.9995 - val_precision: 0.8545 - val_recall: 0.7460 - val_auc: 0.9570 - val_prc: 0.7659\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8419 - recall: 0.8080 - auc: 0.9663 - prc: 0.7563 - val_loss: 0.0043 - val_accuracy: 0.9995 - val_precision: 0.8305 - val_recall: 0.7778 - val_auc: 0.9566 - val_prc: 0.7540\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8387 - recall: 0.8050 - auc: 0.9632 - prc: 0.7568 - val_loss: 0.0046 - val_accuracy: 0.9994 - val_precision: 0.7656 - val_recall: 0.7778 - val_auc: 0.9562 - val_prc: 0.7515\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8245 - recall: 0.8142 - auc: 0.9619 - prc: 0.7609 - val_loss: 0.0042 - val_accuracy: 0.9995 - val_precision: 0.8305 - val_recall: 0.7778 - val_auc: 0.9567 - val_prc: 0.7533\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8239 - recall: 0.8111 - auc: 0.9637 - prc: 0.7613 - val_loss: 0.0047 - val_accuracy: 0.9993 - val_precision: 0.7424 - val_recall: 0.7778 - val_auc: 0.9561 - val_prc: 0.7527\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8333 - recall: 0.8050 - auc: 0.9680 - prc: 0.7592 - val_loss: 0.0052 - val_accuracy: 0.9993 - val_precision: 0.7246 - val_recall: 0.7937 - val_auc: 0.9634 - val_prc: 0.7531\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8208 - recall: 0.8080 - auc: 0.9664 - prc: 0.7631 - val_loss: 0.0039 - val_accuracy: 0.9995 - val_precision: 0.8421 - val_recall: 0.7619 - val_auc: 0.9571 - val_prc: 0.7678\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8111 - recall: 0.8111 - auc: 0.9669 - prc: 0.7643 - val_loss: 0.0038 - val_accuracy: 0.9994 - val_precision: 0.8136 - val_recall: 0.7619 - val_auc: 0.9575 - val_prc: 0.7542\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8219 - recall: 0.8142 - auc: 0.9637 - prc: 0.7660 - val_loss: 0.0035 - val_accuracy: 0.9995 - val_precision: 0.8727 - val_recall: 0.7619 - val_auc: 0.9578 - val_prc: 0.7678\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0016 - accuracy: 0.9994 - precision: 0.8219 - recall: 0.8142 - auc: 0.9608 - prc: 0.7681 - val_loss: 0.0037 - val_accuracy: 0.9995 - val_precision: 0.8421 - val_recall: 0.7619 - val_auc: 0.9572 - val_prc: 0.7636\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 0.0016 - accuracy: 0.9993 - precision: 0.8188 - recall: 0.8111 - auc: 0.9684 - prc: 0.7620 - val_loss: 0.0046 - val_accuracy: 0.9993 - val_precision: 0.7538 - val_recall: 0.7778 - val_auc: 0.9565 - val_prc: 0.7496\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0016 - accuracy: 0.9994 - precision: 0.8297 - recall: 0.8142 - auc: 0.9655 - prc: 0.7660 - val_loss: 0.0046 - val_accuracy: 0.9994 - val_precision: 0.7778 - val_recall: 0.7778 - val_auc: 0.9562 - val_prc: 0.7511\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0016 - accuracy: 0.9994 - precision: 0.8435 - recall: 0.8173 - auc: 0.9665 - prc: 0.7683 - val_loss: 0.0048 - val_accuracy: 0.9993 - val_precision: 0.7424 - val_recall: 0.7778 - val_auc: 0.9563 - val_prc: 0.7509\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9994 - precision: 0.8179 - recall: 0.8204 - auc: 0.9683 - prc: 0.7712 - val_loss: 0.0035 - val_accuracy: 0.9995 - val_precision: 0.8571 - val_recall: 0.7619 - val_auc: 0.9578 - val_prc: 0.7641\n",
            "1419/1419 [==============================] - 2s 2ms/step\n",
            "******************************for w =  0.15\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_13 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_13 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_13 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88/89 [============================>.] - ETA: 0s - loss: 0.0123 - accuracy: 0.9967 - precision: 0.2459 - recall: 0.4611 - auc: 0.7982 - prc: 0.3926"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 15ms/step - loss: 0.0122 - accuracy: 0.9967 - precision: 0.2459 - recall: 0.4611 - auc: 0.7987 - prc: 0.3926 - val_loss: 0.0074 - val_accuracy: 0.9994 - val_precision: 0.8103 - val_recall: 0.7460 - val_auc: 0.9384 - val_prc: 0.6766\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0025 - accuracy: 0.9994 - precision: 0.8794 - recall: 0.7678 - auc: 0.9399 - prc: 0.7119 - val_loss: 0.0048 - val_accuracy: 0.9994 - val_precision: 0.8167 - val_recall: 0.7778 - val_auc: 0.9714 - val_prc: 0.7432\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0021 - accuracy: 0.9994 - precision: 0.8819 - recall: 0.7864 - auc: 0.9529 - prc: 0.7435 - val_loss: 0.0045 - val_accuracy: 0.9996 - val_precision: 0.8909 - val_recall: 0.7778 - val_auc: 0.9723 - val_prc: 0.7564\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0020 - accuracy: 0.9994 - precision: 0.8591 - recall: 0.7926 - auc: 0.9553 - prc: 0.7505 - val_loss: 0.0045 - val_accuracy: 0.9995 - val_precision: 0.8333 - val_recall: 0.7937 - val_auc: 0.9725 - val_prc: 0.7573\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0019 - accuracy: 0.9994 - precision: 0.8404 - recall: 0.7988 - auc: 0.9527 - prc: 0.7532 - val_loss: 0.0045 - val_accuracy: 0.9994 - val_precision: 0.7812 - val_recall: 0.7937 - val_auc: 0.9642 - val_prc: 0.7556\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8419 - recall: 0.8080 - auc: 0.9529 - prc: 0.7579 - val_loss: 0.0048 - val_accuracy: 0.9994 - val_precision: 0.7576 - val_recall: 0.7937 - val_auc: 0.9642 - val_prc: 0.7563\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8360 - recall: 0.8050 - auc: 0.9618 - prc: 0.7565 - val_loss: 0.0042 - val_accuracy: 0.9995 - val_precision: 0.8448 - val_recall: 0.7778 - val_auc: 0.9568 - val_prc: 0.7544\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8317 - recall: 0.8111 - auc: 0.9680 - prc: 0.7633 - val_loss: 0.0040 - val_accuracy: 0.9995 - val_precision: 0.8448 - val_recall: 0.7778 - val_auc: 0.9571 - val_prc: 0.7551\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8339 - recall: 0.8080 - auc: 0.9576 - prc: 0.7600 - val_loss: 0.0043 - val_accuracy: 0.9994 - val_precision: 0.7656 - val_recall: 0.7778 - val_auc: 0.9565 - val_prc: 0.7541\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8265 - recall: 0.8111 - auc: 0.9636 - prc: 0.7591 - val_loss: 0.0039 - val_accuracy: 0.9995 - val_precision: 0.8305 - val_recall: 0.7778 - val_auc: 0.9573 - val_prc: 0.7550\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8317 - recall: 0.8111 - auc: 0.9609 - prc: 0.7645 - val_loss: 0.0040 - val_accuracy: 0.9995 - val_precision: 0.8305 - val_recall: 0.7778 - val_auc: 0.9568 - val_prc: 0.7696\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8323 - recall: 0.8142 - auc: 0.9636 - prc: 0.7605 - val_loss: 0.0038 - val_accuracy: 0.9994 - val_precision: 0.8136 - val_recall: 0.7619 - val_auc: 0.9574 - val_prc: 0.7554\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8086 - recall: 0.8111 - auc: 0.9563 - prc: 0.7647 - val_loss: 0.0040 - val_accuracy: 0.9994 - val_precision: 0.8033 - val_recall: 0.7778 - val_auc: 0.9569 - val_prc: 0.7534\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8328 - recall: 0.8173 - auc: 0.9637 - prc: 0.7636 - val_loss: 0.0038 - val_accuracy: 0.9994 - val_precision: 0.8167 - val_recall: 0.7778 - val_auc: 0.9575 - val_prc: 0.7552\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8275 - recall: 0.8019 - auc: 0.9643 - prc: 0.7639 - val_loss: 0.0053 - val_accuracy: 0.9993 - val_precision: 0.7286 - val_recall: 0.8095 - val_auc: 0.9632 - val_prc: 0.7537\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0016 - accuracy: 0.9994 - precision: 0.8261 - recall: 0.8235 - auc: 0.9669 - prc: 0.7692 - val_loss: 0.0036 - val_accuracy: 0.9995 - val_precision: 0.8421 - val_recall: 0.7619 - val_auc: 0.9576 - val_prc: 0.7677\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0016 - accuracy: 0.9994 - precision: 0.8328 - recall: 0.8019 - auc: 0.9701 - prc: 0.7703 - val_loss: 0.0038 - val_accuracy: 0.9995 - val_precision: 0.8305 - val_recall: 0.7778 - val_auc: 0.9570 - val_prc: 0.7673\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8208 - recall: 0.8080 - auc: 0.9640 - prc: 0.7695 - val_loss: 0.0043 - val_accuracy: 0.9994 - val_precision: 0.8033 - val_recall: 0.7778 - val_auc: 0.9568 - val_prc: 0.7513\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0016 - accuracy: 0.9994 - precision: 0.8333 - recall: 0.8050 - auc: 0.9642 - prc: 0.7686 - val_loss: 0.0050 - val_accuracy: 0.9994 - val_precision: 0.7692 - val_recall: 0.7937 - val_auc: 0.9636 - val_prc: 0.7496\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0016 - accuracy: 0.9994 - precision: 0.8245 - recall: 0.8142 - auc: 0.9641 - prc: 0.7704 - val_loss: 0.0041 - val_accuracy: 0.9994 - val_precision: 0.8033 - val_recall: 0.7778 - val_auc: 0.9570 - val_prc: 0.7650\n",
            "1419/1419 [==============================] - 2s 2ms/step\n",
            "******************************for w =  0.16\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_14 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_14 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_14 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "84/89 [===========================>..] - ETA: 0s - loss: 0.0137 - accuracy: 0.9948 - precision: 0.1419 - recall: 0.4227 - auc: 0.7717 - prc: 0.3642"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 15ms/step - loss: 0.0131 - accuracy: 0.9950 - precision: 0.1596 - recall: 0.4560 - auc: 0.7887 - prc: 0.3982 - val_loss: 0.0067 - val_accuracy: 0.9994 - val_precision: 0.8214 - val_recall: 0.7302 - val_auc: 0.9323 - val_prc: 0.6660\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0025 - accuracy: 0.9994 - precision: 0.8759 - recall: 0.7647 - auc: 0.9380 - prc: 0.7182 - val_loss: 0.0055 - val_accuracy: 0.9994 - val_precision: 0.8033 - val_recall: 0.7778 - val_auc: 0.9708 - val_prc: 0.7334\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0022 - accuracy: 0.9994 - precision: 0.8746 - recall: 0.7771 - auc: 0.9516 - prc: 0.7421 - val_loss: 0.0043 - val_accuracy: 0.9995 - val_precision: 0.8750 - val_recall: 0.7778 - val_auc: 0.9723 - val_prc: 0.7541\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0020 - accuracy: 0.9994 - precision: 0.8707 - recall: 0.7926 - auc: 0.9528 - prc: 0.7499 - val_loss: 0.0044 - val_accuracy: 0.9995 - val_precision: 0.8197 - val_recall: 0.7937 - val_auc: 0.9646 - val_prc: 0.7549\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0019 - accuracy: 0.9994 - precision: 0.8436 - recall: 0.8019 - auc: 0.9485 - prc: 0.7519 - val_loss: 0.0042 - val_accuracy: 0.9994 - val_precision: 0.8276 - val_recall: 0.7619 - val_auc: 0.9570 - val_prc: 0.7668\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0019 - accuracy: 0.9994 - precision: 0.8404 - recall: 0.7988 - auc: 0.9575 - prc: 0.7530 - val_loss: 0.0050 - val_accuracy: 0.9993 - val_precision: 0.7391 - val_recall: 0.8095 - val_auc: 0.9638 - val_prc: 0.7573\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8409 - recall: 0.8019 - auc: 0.9608 - prc: 0.7567 - val_loss: 0.0046 - val_accuracy: 0.9993 - val_precision: 0.7538 - val_recall: 0.7778 - val_auc: 0.9562 - val_prc: 0.7531\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8360 - recall: 0.8050 - auc: 0.9621 - prc: 0.7551 - val_loss: 0.0040 - val_accuracy: 0.9994 - val_precision: 0.8167 - val_recall: 0.7778 - val_auc: 0.9573 - val_prc: 0.7508\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8392 - recall: 0.8080 - auc: 0.9610 - prc: 0.7632 - val_loss: 0.0046 - val_accuracy: 0.9994 - val_precision: 0.7778 - val_recall: 0.7778 - val_auc: 0.9563 - val_prc: 0.7505\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8429 - recall: 0.8142 - auc: 0.9593 - prc: 0.7569 - val_loss: 0.0043 - val_accuracy: 0.9994 - val_precision: 0.7903 - val_recall: 0.7778 - val_auc: 0.9566 - val_prc: 0.7535\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8344 - recall: 0.8111 - auc: 0.9596 - prc: 0.7607 - val_loss: 0.0041 - val_accuracy: 0.9994 - val_precision: 0.8136 - val_recall: 0.7619 - val_auc: 0.9568 - val_prc: 0.7655\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8339 - recall: 0.8080 - auc: 0.9595 - prc: 0.7620 - val_loss: 0.0041 - val_accuracy: 0.9994 - val_precision: 0.7869 - val_recall: 0.7619 - val_auc: 0.9568 - val_prc: 0.7650\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8182 - recall: 0.8080 - auc: 0.9610 - prc: 0.7659 - val_loss: 0.0038 - val_accuracy: 0.9994 - val_precision: 0.8167 - val_recall: 0.7778 - val_auc: 0.9575 - val_prc: 0.7530\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8349 - recall: 0.8142 - auc: 0.9641 - prc: 0.7598 - val_loss: 0.0036 - val_accuracy: 0.9995 - val_precision: 0.8421 - val_recall: 0.7619 - val_auc: 0.9576 - val_prc: 0.7663\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8217 - recall: 0.7988 - auc: 0.9641 - prc: 0.7620 - val_loss: 0.0034 - val_accuracy: 0.9995 - val_precision: 0.8704 - val_recall: 0.7460 - val_auc: 0.9579 - val_prc: 0.7642\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8469 - recall: 0.8050 - auc: 0.9625 - prc: 0.7577 - val_loss: 0.0039 - val_accuracy: 0.9994 - val_precision: 0.8136 - val_recall: 0.7619 - val_auc: 0.9570 - val_prc: 0.7661\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8484 - recall: 0.8142 - auc: 0.9638 - prc: 0.7776 - val_loss: 0.0040 - val_accuracy: 0.9994 - val_precision: 0.7903 - val_recall: 0.7778 - val_auc: 0.9574 - val_prc: 0.7510\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8429 - recall: 0.8142 - auc: 0.9628 - prc: 0.7772 - val_loss: 0.0039 - val_accuracy: 0.9994 - val_precision: 0.8033 - val_recall: 0.7778 - val_auc: 0.9575 - val_prc: 0.7502\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8529 - recall: 0.8080 - auc: 0.9642 - prc: 0.7781 - val_loss: 0.0038 - val_accuracy: 0.9994 - val_precision: 0.8136 - val_recall: 0.7619 - val_auc: 0.9574 - val_prc: 0.7670\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0016 - accuracy: 0.9994 - precision: 0.8381 - recall: 0.8173 - auc: 0.9614 - prc: 0.7825 - val_loss: 0.0042 - val_accuracy: 0.9994 - val_precision: 0.7903 - val_recall: 0.7778 - val_auc: 0.9569 - val_prc: 0.7515\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "******************************for w =  0.17\n",
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_15 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_15 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86/89 [===========================>..] - ETA: 0s - loss: 0.0135 - accuracy: 0.9949 - precision: 0.1605 - recall: 0.4735 - auc: 0.7825 - prc: 0.3900"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 16ms/step - loss: 0.0131 - accuracy: 0.9950 - precision: 0.1643 - recall: 0.4767 - auc: 0.7877 - prc: 0.3952 - val_loss: 0.0083 - val_accuracy: 0.9994 - val_precision: 0.7966 - val_recall: 0.7460 - val_auc: 0.9313 - val_prc: 0.6477\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0026 - accuracy: 0.9994 - precision: 0.8804 - recall: 0.7523 - auc: 0.9443 - prc: 0.7149 - val_loss: 0.0057 - val_accuracy: 0.9994 - val_precision: 0.7903 - val_recall: 0.7778 - val_auc: 0.9698 - val_prc: 0.7074\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0022 - accuracy: 0.9994 - precision: 0.8785 - recall: 0.7833 - auc: 0.9502 - prc: 0.7432 - val_loss: 0.0044 - val_accuracy: 0.9995 - val_precision: 0.8596 - val_recall: 0.7778 - val_auc: 0.9641 - val_prc: 0.7451\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0020 - accuracy: 0.9994 - precision: 0.8552 - recall: 0.7864 - auc: 0.9511 - prc: 0.7438 - val_loss: 0.0039 - val_accuracy: 0.9995 - val_precision: 0.8727 - val_recall: 0.7619 - val_auc: 0.9574 - val_prc: 0.7491\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0019 - accuracy: 0.9994 - precision: 0.8635 - recall: 0.7833 - auc: 0.9499 - prc: 0.7549 - val_loss: 0.0040 - val_accuracy: 0.9995 - val_precision: 0.8448 - val_recall: 0.7778 - val_auc: 0.9576 - val_prc: 0.7489\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0019 - accuracy: 0.9994 - precision: 0.8682 - recall: 0.7957 - auc: 0.9519 - prc: 0.7527 - val_loss: 0.0044 - val_accuracy: 0.9995 - val_precision: 0.8305 - val_recall: 0.7778 - val_auc: 0.9642 - val_prc: 0.7529\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8431 - recall: 0.7988 - auc: 0.9622 - prc: 0.7521 - val_loss: 0.0041 - val_accuracy: 0.9995 - val_precision: 0.8305 - val_recall: 0.7778 - val_auc: 0.9572 - val_prc: 0.7491\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8382 - recall: 0.8019 - auc: 0.9654 - prc: 0.7555 - val_loss: 0.0038 - val_accuracy: 0.9995 - val_precision: 0.8448 - val_recall: 0.7778 - val_auc: 0.9576 - val_prc: 0.7511\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8469 - recall: 0.8050 - auc: 0.9655 - prc: 0.7583 - val_loss: 0.0039 - val_accuracy: 0.9995 - val_precision: 0.8305 - val_recall: 0.7778 - val_auc: 0.9573 - val_prc: 0.7521\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8581 - recall: 0.8050 - auc: 0.9564 - prc: 0.7629 - val_loss: 0.0042 - val_accuracy: 0.9994 - val_precision: 0.7903 - val_recall: 0.7778 - val_auc: 0.9568 - val_prc: 0.7502\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8442 - recall: 0.8050 - auc: 0.9612 - prc: 0.7616 - val_loss: 0.0041 - val_accuracy: 0.9994 - val_precision: 0.8167 - val_recall: 0.7778 - val_auc: 0.9565 - val_prc: 0.7642\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8269 - recall: 0.7988 - auc: 0.9621 - prc: 0.7674 - val_loss: 0.0036 - val_accuracy: 0.9995 - val_precision: 0.8448 - val_recall: 0.7778 - val_auc: 0.9580 - val_prc: 0.7513\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8447 - recall: 0.8080 - auc: 0.9644 - prc: 0.7658 - val_loss: 0.0038 - val_accuracy: 0.9994 - val_precision: 0.8136 - val_recall: 0.7619 - val_auc: 0.9573 - val_prc: 0.7637\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8419 - recall: 0.8080 - auc: 0.9597 - prc: 0.7683 - val_loss: 0.0037 - val_accuracy: 0.9994 - val_precision: 0.8276 - val_recall: 0.7619 - val_auc: 0.9574 - val_prc: 0.7643\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8534 - recall: 0.8111 - auc: 0.9598 - prc: 0.7643 - val_loss: 0.0046 - val_accuracy: 0.9994 - val_precision: 0.7778 - val_recall: 0.7778 - val_auc: 0.9564 - val_prc: 0.7497\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8511 - recall: 0.8142 - auc: 0.9612 - prc: 0.7630 - val_loss: 0.0041 - val_accuracy: 0.9994 - val_precision: 0.8167 - val_recall: 0.7778 - val_auc: 0.9569 - val_prc: 0.7496\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8506 - recall: 0.8111 - auc: 0.9643 - prc: 0.7708 - val_loss: 0.0044 - val_accuracy: 0.9994 - val_precision: 0.7778 - val_recall: 0.7778 - val_auc: 0.9566 - val_prc: 0.7489\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0016 - accuracy: 0.9994 - precision: 0.8344 - recall: 0.8111 - auc: 0.9628 - prc: 0.7793 - val_loss: 0.0039 - val_accuracy: 0.9994 - val_precision: 0.8136 - val_recall: 0.7619 - val_auc: 0.9568 - val_prc: 0.7623\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8397 - recall: 0.8111 - auc: 0.9613 - prc: 0.7812 - val_loss: 0.0032 - val_accuracy: 0.9995 - val_precision: 0.8704 - val_recall: 0.7460 - val_auc: 0.9582 - val_prc: 0.7628\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8515 - recall: 0.7988 - auc: 0.9631 - prc: 0.7695 - val_loss: 0.0041 - val_accuracy: 0.9994 - val_precision: 0.8136 - val_recall: 0.7619 - val_auc: 0.9568 - val_prc: 0.7605\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "******************************for w =  0.18000000000000002\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_16 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_16 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88/89 [============================>.] - ETA: 0s - loss: 0.0132 - accuracy: 0.9976 - precision: 0.3396 - recall: 0.4230 - auc: 0.7857 - prc: 0.3528"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 18ms/step - loss: 0.0131 - accuracy: 0.9976 - precision: 0.3438 - recall: 0.4275 - auc: 0.7880 - prc: 0.3587 - val_loss: 0.0062 - val_accuracy: 0.9993 - val_precision: 0.8113 - val_recall: 0.6825 - val_auc: 0.9100 - val_prc: 0.6651\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0026 - accuracy: 0.9994 - precision: 0.8683 - recall: 0.7554 - auc: 0.9311 - prc: 0.7148 - val_loss: 0.0044 - val_accuracy: 0.9995 - val_precision: 0.8448 - val_recall: 0.7778 - val_auc: 0.9565 - val_prc: 0.7348\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0022 - accuracy: 0.9994 - precision: 0.8790 - recall: 0.7647 - auc: 0.9507 - prc: 0.7374 - val_loss: 0.0048 - val_accuracy: 0.9994 - val_precision: 0.8167 - val_recall: 0.7778 - val_auc: 0.9719 - val_prc: 0.7530\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0020 - accuracy: 0.9994 - precision: 0.8616 - recall: 0.7709 - auc: 0.9528 - prc: 0.7474 - val_loss: 0.0040 - val_accuracy: 0.9995 - val_precision: 0.8596 - val_recall: 0.7778 - val_auc: 0.9576 - val_prc: 0.7535\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0020 - accuracy: 0.9994 - precision: 0.8547 - recall: 0.7833 - auc: 0.9534 - prc: 0.7521 - val_loss: 0.0041 - val_accuracy: 0.9994 - val_precision: 0.8167 - val_recall: 0.7778 - val_auc: 0.9575 - val_prc: 0.7517\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0019 - accuracy: 0.9994 - precision: 0.8523 - recall: 0.7864 - auc: 0.9533 - prc: 0.7526 - val_loss: 0.0038 - val_accuracy: 0.9995 - val_precision: 0.8448 - val_recall: 0.7778 - val_auc: 0.9577 - val_prc: 0.7531\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8600 - recall: 0.7988 - auc: 0.9596 - prc: 0.7594 - val_loss: 0.0039 - val_accuracy: 0.9995 - val_precision: 0.8448 - val_recall: 0.7778 - val_auc: 0.9573 - val_prc: 0.7535\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8404 - recall: 0.7988 - auc: 0.9552 - prc: 0.7638 - val_loss: 0.0040 - val_accuracy: 0.9994 - val_precision: 0.8033 - val_recall: 0.7778 - val_auc: 0.9573 - val_prc: 0.7504\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8431 - recall: 0.7988 - auc: 0.9596 - prc: 0.7584 - val_loss: 0.0037 - val_accuracy: 0.9995 - val_precision: 0.8421 - val_recall: 0.7619 - val_auc: 0.9576 - val_prc: 0.7656\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8323 - recall: 0.7988 - auc: 0.9584 - prc: 0.7586 - val_loss: 0.0040 - val_accuracy: 0.9994 - val_precision: 0.8033 - val_recall: 0.7778 - val_auc: 0.9570 - val_prc: 0.7503\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8265 - recall: 0.8111 - auc: 0.9612 - prc: 0.7646 - val_loss: 0.0034 - val_accuracy: 0.9995 - val_precision: 0.8704 - val_recall: 0.7460 - val_auc: 0.9579 - val_prc: 0.7629\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8609 - recall: 0.8050 - auc: 0.9645 - prc: 0.7621 - val_loss: 0.0037 - val_accuracy: 0.9994 - val_precision: 0.8136 - val_recall: 0.7619 - val_auc: 0.9576 - val_prc: 0.7625\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0017 - accuracy: 0.9993 - precision: 0.8248 - recall: 0.8019 - auc: 0.9582 - prc: 0.7624 - val_loss: 0.0036 - val_accuracy: 0.9994 - val_precision: 0.8276 - val_recall: 0.7619 - val_auc: 0.9579 - val_prc: 0.7497\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8497 - recall: 0.8050 - auc: 0.9571 - prc: 0.7726 - val_loss: 0.0039 - val_accuracy: 0.9994 - val_precision: 0.8136 - val_recall: 0.7619 - val_auc: 0.9572 - val_prc: 0.7615\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8605 - recall: 0.8019 - auc: 0.9628 - prc: 0.7680 - val_loss: 0.0036 - val_accuracy: 0.9994 - val_precision: 0.8276 - val_recall: 0.7619 - val_auc: 0.9579 - val_prc: 0.7460\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8502 - recall: 0.8080 - auc: 0.9615 - prc: 0.7666 - val_loss: 0.0042 - val_accuracy: 0.9994 - val_precision: 0.7812 - val_recall: 0.7937 - val_auc: 0.9572 - val_prc: 0.7286\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8452 - recall: 0.8111 - auc: 0.9601 - prc: 0.7719 - val_loss: 0.0038 - val_accuracy: 0.9994 - val_precision: 0.8000 - val_recall: 0.7619 - val_auc: 0.9573 - val_prc: 0.7450\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8409 - recall: 0.8019 - auc: 0.9646 - prc: 0.7630 - val_loss: 0.0043 - val_accuracy: 0.9993 - val_precision: 0.7619 - val_recall: 0.7619 - val_auc: 0.9567 - val_prc: 0.7439\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8459 - recall: 0.7988 - auc: 0.9615 - prc: 0.7771 - val_loss: 0.0035 - val_accuracy: 0.9994 - val_precision: 0.8136 - val_recall: 0.7619 - val_auc: 0.9577 - val_prc: 0.7618\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8323 - recall: 0.7988 - auc: 0.9645 - prc: 0.7699 - val_loss: 0.0043 - val_accuracy: 0.9994 - val_precision: 0.7656 - val_recall: 0.7778 - val_auc: 0.9571 - val_prc: 0.7415\n",
            "1419/1419 [==============================] - 2s 2ms/step\n",
            "******************************for w =  0.19000000000000003\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_17 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_17 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_17 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87/89 [============================>.] - ETA: 0s - loss: 0.0150 - accuracy: 0.9932 - precision: 0.1085 - recall: 0.4153 - auc: 0.7603 - prc: 0.3645"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 19ms/step - loss: 0.0148 - accuracy: 0.9933 - precision: 0.1110 - recall: 0.4171 - auc: 0.7636 - prc: 0.3671 - val_loss: 0.0063 - val_accuracy: 0.9994 - val_precision: 0.8214 - val_recall: 0.7302 - val_auc: 0.9262 - val_prc: 0.6593\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0026 - accuracy: 0.9994 - precision: 0.8781 - recall: 0.7585 - auc: 0.9405 - prc: 0.7168 - val_loss: 0.0046 - val_accuracy: 0.9995 - val_precision: 0.8305 - val_recall: 0.7778 - val_auc: 0.9648 - val_prc: 0.7322\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0022 - accuracy: 0.9994 - precision: 0.8834 - recall: 0.7740 - auc: 0.9453 - prc: 0.7396 - val_loss: 0.0042 - val_accuracy: 0.9995 - val_precision: 0.8448 - val_recall: 0.7778 - val_auc: 0.9648 - val_prc: 0.7546\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0020 - accuracy: 0.9994 - precision: 0.8720 - recall: 0.7802 - auc: 0.9475 - prc: 0.7510 - val_loss: 0.0043 - val_accuracy: 0.9995 - val_precision: 0.8197 - val_recall: 0.7937 - val_auc: 0.9647 - val_prc: 0.7517\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0020 - accuracy: 0.9994 - precision: 0.8571 - recall: 0.7802 - auc: 0.9504 - prc: 0.7543 - val_loss: 0.0047 - val_accuracy: 0.9994 - val_precision: 0.7846 - val_recall: 0.8095 - val_auc: 0.9645 - val_prc: 0.7417\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0019 - accuracy: 0.9994 - precision: 0.8649 - recall: 0.7926 - auc: 0.9551 - prc: 0.7545 - val_loss: 0.0039 - val_accuracy: 0.9995 - val_precision: 0.8305 - val_recall: 0.7778 - val_auc: 0.9576 - val_prc: 0.7549\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8543 - recall: 0.7988 - auc: 0.9566 - prc: 0.7542 - val_loss: 0.0038 - val_accuracy: 0.9995 - val_precision: 0.8448 - val_recall: 0.7778 - val_auc: 0.9578 - val_prc: 0.7509\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8454 - recall: 0.7957 - auc: 0.9553 - prc: 0.7606 - val_loss: 0.0037 - val_accuracy: 0.9995 - val_precision: 0.8305 - val_recall: 0.7778 - val_auc: 0.9578 - val_prc: 0.7510\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8502 - recall: 0.8080 - auc: 0.9539 - prc: 0.7579 - val_loss: 0.0036 - val_accuracy: 0.9995 - val_precision: 0.8545 - val_recall: 0.7460 - val_auc: 0.9574 - val_prc: 0.7654\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8487 - recall: 0.7988 - auc: 0.9597 - prc: 0.7582 - val_loss: 0.0034 - val_accuracy: 0.9995 - val_precision: 0.8571 - val_recall: 0.7619 - val_auc: 0.9581 - val_prc: 0.7645\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8571 - recall: 0.7988 - auc: 0.9587 - prc: 0.7631 - val_loss: 0.0049 - val_accuracy: 0.9993 - val_precision: 0.7353 - val_recall: 0.7937 - val_auc: 0.9562 - val_prc: 0.7470\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 0.9994 - precision: 0.8520 - recall: 0.8019 - auc: 0.9599 - prc: 0.7544 - val_loss: 0.0039 - val_accuracy: 0.9995 - val_precision: 0.8421 - val_recall: 0.7619 - val_auc: 0.9569 - val_prc: 0.7644\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8678 - recall: 0.7926 - auc: 0.9599 - prc: 0.7764 - val_loss: 0.0051 - val_accuracy: 0.9993 - val_precision: 0.7286 - val_recall: 0.8095 - val_auc: 0.9564 - val_prc: 0.7232\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8414 - recall: 0.8050 - auc: 0.9571 - prc: 0.7681 - val_loss: 0.0037 - val_accuracy: 0.9994 - val_precision: 0.8136 - val_recall: 0.7619 - val_auc: 0.9575 - val_prc: 0.7616\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8680 - recall: 0.8142 - auc: 0.9585 - prc: 0.7719 - val_loss: 0.0037 - val_accuracy: 0.9994 - val_precision: 0.8136 - val_recall: 0.7619 - val_auc: 0.9576 - val_prc: 0.7636\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8492 - recall: 0.8019 - auc: 0.9585 - prc: 0.7832 - val_loss: 0.0035 - val_accuracy: 0.9994 - val_precision: 0.8276 - val_recall: 0.7619 - val_auc: 0.9579 - val_prc: 0.7623\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8576 - recall: 0.8019 - auc: 0.9586 - prc: 0.7760 - val_loss: 0.0035 - val_accuracy: 0.9994 - val_precision: 0.8276 - val_recall: 0.7619 - val_auc: 0.9577 - val_prc: 0.7652\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8462 - recall: 0.8173 - auc: 0.9586 - prc: 0.7651 - val_loss: 0.0034 - val_accuracy: 0.9995 - val_precision: 0.8421 - val_recall: 0.7619 - val_auc: 0.9578 - val_prc: 0.7604\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8581 - recall: 0.8050 - auc: 0.9588 - prc: 0.7880 - val_loss: 0.0044 - val_accuracy: 0.9994 - val_precision: 0.7692 - val_recall: 0.7937 - val_auc: 0.9569 - val_prc: 0.7438\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 0.9994 - precision: 0.8360 - recall: 0.8050 - auc: 0.9599 - prc: 0.7851 - val_loss: 0.0033 - val_accuracy: 0.9995 - val_precision: 0.8421 - val_recall: 0.7619 - val_auc: 0.9580 - val_prc: 0.7636\n",
            "1419/1419 [==============================] - 2s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(validation_preds)):\n",
        "  print(\"CLASSIFICATION REPORT FOR W=\", 0.01 +i*(0.01))\n",
        "  print(classification_report(y_val, validation_preds_rms[i]))"
      ],
      "metadata": {
        "id": "lXVGV6HGO3MI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w=0.01\n",
        "val_preds_rms_gamma=[]\n",
        "parameters=[]\n",
        "for gamma in range(2,6):\n",
        "  w=0.01\n",
        "  while(w<=0.2): \n",
        "      print(\"****************************** w = \",w)\n",
        "      print(\"****************************** w = \",gamma)\n",
        "      cnn_cust=create_cnn_cust()\n",
        "      cnn_cust.summary()\n",
        "      cnn_cust.fit(X_train_norm,y_train, batch_size=2048,epochs=20, verbose=1, validation_data=(X_val_norm,y_val), class_weight={0: w**gamma, 1: (1-w)**gamma})\n",
        "      cnn_cust_val_preds = cnn_cust.predict(X_val_norm)>0.5\n",
        "      #cnn_cust_test_preds= cnn_cust.predict(X_test_norm)>0.5\n",
        "      val_preds_rms_gamma.append(cnn_cust_val_preds)\n",
        "      parameters.append([w,gamma])\n",
        "      w+=0.01"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAZIr5E0OKQR",
        "outputId": "133cf3de-7433-4f9b-b494-ac26a78a8e3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****************************** w =  0.01\n",
            "****************************** w =  2\n",
            "Model: \"sequential_29\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_29 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_29 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_29 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87/89 [============================>.] - ETA: 0s - loss: 2.9913e-04 - accuracy: 0.0054 - precision: 0.0018 - recall: 0.9983 - auc: 0.9177 - prc: 0.1984"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 20ms/step - loss: 2.9560e-04 - accuracy: 0.0054 - precision: 0.0018 - recall: 0.9983 - auc: 0.9188 - prc: 0.2010 - val_loss: 1.0467 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9653 - val_prc: 0.4768\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 2.1204e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9674 - prc: 0.5119 - val_loss: 1.1339 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9665 - val_prc: 0.4609\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 2.0196e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9712 - prc: 0.4979 - val_loss: 1.1847 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9676 - val_prc: 0.4268\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.9531e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9745 - prc: 0.4725 - val_loss: 1.2080 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9683 - val_prc: 0.4065\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.9172e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9753 - prc: 0.4598 - val_loss: 1.2177 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9690 - val_prc: 0.3931\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.8820e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9775 - prc: 0.4418 - val_loss: 1.2194 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9694 - val_prc: 0.3751\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.8680e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9765 - prc: 0.4408 - val_loss: 1.2167 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9696 - val_prc: 0.3689\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.8468e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9784 - prc: 0.4392 - val_loss: 1.2104 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9703 - val_prc: 0.3622\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 1.8250e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9789 - prc: 0.4449 - val_loss: 1.2015 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9703 - val_prc: 0.3626\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.8027e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9808 - prc: 0.4471 - val_loss: 1.1915 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9707 - val_prc: 0.3647\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.7723e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9819 - prc: 0.4439 - val_loss: 1.1772 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9707 - val_prc: 0.3694\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.7725e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9809 - prc: 0.4550 - val_loss: 1.1680 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9709 - val_prc: 0.3704\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.7586e-04 - accuracy: 0.0019 - precision: 0.0018 - recall: 1.0000 - auc: 0.9812 - prc: 0.4472 - val_loss: 1.1555 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9711 - val_prc: 0.3714\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 1.7422e-04 - accuracy: 0.0022 - precision: 0.0018 - recall: 1.0000 - auc: 0.9818 - prc: 0.4487 - val_loss: 1.1432 - val_accuracy: 0.0017 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9715 - val_prc: 0.3756\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.7220e-04 - accuracy: 0.0029 - precision: 0.0018 - recall: 1.0000 - auc: 0.9822 - prc: 0.4524 - val_loss: 1.1302 - val_accuracy: 0.0025 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9714 - val_prc: 0.3800\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 1.7087e-04 - accuracy: 0.0045 - precision: 0.0018 - recall: 1.0000 - auc: 0.9829 - prc: 0.4539 - val_loss: 1.1170 - val_accuracy: 0.0039 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9712 - val_prc: 0.3848\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.6888e-04 - accuracy: 0.0063 - precision: 0.0018 - recall: 1.0000 - auc: 0.9836 - prc: 0.4584 - val_loss: 1.1036 - val_accuracy: 0.0065 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9715 - val_prc: 0.3847\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.6822e-04 - accuracy: 0.0108 - precision: 0.0018 - recall: 1.0000 - auc: 0.9834 - prc: 0.4585 - val_loss: 1.0939 - val_accuracy: 0.0095 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9721 - val_prc: 0.3810\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.6689e-04 - accuracy: 0.0148 - precision: 0.0018 - recall: 1.0000 - auc: 0.9834 - prc: 0.4662 - val_loss: 1.0825 - val_accuracy: 0.0131 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9720 - val_prc: 0.3888\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.6434e-04 - accuracy: 0.0199 - precision: 0.0018 - recall: 1.0000 - auc: 0.9849 - prc: 0.4701 - val_loss: 1.0705 - val_accuracy: 0.0197 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9719 - val_prc: 0.3896\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.02\n",
            "****************************** w =  2\n",
            "Model: \"sequential_30\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_30 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_30 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_30 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "83/89 [==========================>...] - ETA: 0s - loss: 5.9224e-04 - accuracy: 0.0175 - precision: 0.0017 - recall: 0.9891 - auc: 0.9364 - prc: 0.3707"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 15ms/step - loss: 5.8658e-04 - accuracy: 0.0170 - precision: 0.0017 - recall: 0.9896 - auc: 0.9391 - prc: 0.3671 - val_loss: 0.8603 - val_accuracy: 0.0021 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9768 - val_prc: 0.6013\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 4.8805e-04 - accuracy: 0.0350 - precision: 0.0018 - recall: 1.0000 - auc: 0.9726 - prc: 0.6741 - val_loss: 0.7990 - val_accuracy: 0.0900 - val_precision: 0.0015 - val_recall: 1.0000 - val_auc: 0.9787 - val_prc: 0.6512\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 4.5601e-04 - accuracy: 0.3006 - precision: 0.0025 - recall: 0.9969 - auc: 0.9769 - prc: 0.7168 - val_loss: 0.7414 - val_accuracy: 0.4318 - val_precision: 0.0024 - val_recall: 0.9841 - val_auc: 0.9795 - val_prc: 0.6733\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 4.3105e-04 - accuracy: 0.6041 - precision: 0.0044 - recall: 0.9845 - auc: 0.9798 - prc: 0.7347 - val_loss: 0.6869 - val_accuracy: 0.6916 - val_precision: 0.0044 - val_recall: 0.9841 - val_auc: 0.9783 - val_prc: 0.6898\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 4.1380e-04 - accuracy: 0.7307 - precision: 0.0064 - recall: 0.9783 - auc: 0.9804 - prc: 0.7397 - val_loss: 0.6348 - val_accuracy: 0.7986 - val_precision: 0.0066 - val_recall: 0.9683 - val_auc: 0.9782 - val_prc: 0.6932\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.9814e-04 - accuracy: 0.8117 - precision: 0.0091 - recall: 0.9752 - auc: 0.9826 - prc: 0.7397 - val_loss: 0.5947 - val_accuracy: 0.8452 - val_precision: 0.0086 - val_recall: 0.9683 - val_auc: 0.9770 - val_prc: 0.6948\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 3.8434e-04 - accuracy: 0.8609 - precision: 0.0121 - recall: 0.9567 - auc: 0.9821 - prc: 0.7470 - val_loss: 0.5690 - val_accuracy: 0.8594 - val_precision: 0.0095 - val_recall: 0.9683 - val_auc: 0.9767 - val_prc: 0.7002\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.7203e-04 - accuracy: 0.8614 - precision: 0.0121 - recall: 0.9567 - auc: 0.9831 - prc: 0.7399 - val_loss: 0.5361 - val_accuracy: 0.8814 - val_precision: 0.0112 - val_recall: 0.9683 - val_auc: 0.9764 - val_prc: 0.6979\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.5987e-04 - accuracy: 0.8769 - precision: 0.0136 - recall: 0.9536 - auc: 0.9842 - prc: 0.7381 - val_loss: 0.5078 - val_accuracy: 0.8937 - val_precision: 0.0125 - val_recall: 0.9683 - val_auc: 0.9764 - val_prc: 0.6965\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 3.5421e-04 - accuracy: 0.8914 - precision: 0.0154 - recall: 0.9536 - auc: 0.9831 - prc: 0.7300 - val_loss: 0.4912 - val_accuracy: 0.8958 - val_precision: 0.0127 - val_recall: 0.9683 - val_auc: 0.9759 - val_prc: 0.6992\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 3.4322e-04 - accuracy: 0.8906 - precision: 0.0153 - recall: 0.9536 - auc: 0.9837 - prc: 0.7290 - val_loss: 0.4675 - val_accuracy: 0.9025 - val_precision: 0.0134 - val_recall: 0.9524 - val_auc: 0.9757 - val_prc: 0.6757\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.3610e-04 - accuracy: 0.9066 - precision: 0.0178 - recall: 0.9505 - auc: 0.9847 - prc: 0.7180 - val_loss: 0.4627 - val_accuracy: 0.8971 - val_precision: 0.0127 - val_recall: 0.9524 - val_auc: 0.9756 - val_prc: 0.6217\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.2724e-04 - accuracy: 0.8981 - precision: 0.0163 - recall: 0.9505 - auc: 0.9847 - prc: 0.6884 - val_loss: 0.4431 - val_accuracy: 0.9026 - val_precision: 0.0132 - val_recall: 0.9365 - val_auc: 0.9751 - val_prc: 0.6038\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 3.2000e-04 - accuracy: 0.9057 - precision: 0.0177 - recall: 0.9536 - auc: 0.9858 - prc: 0.6969 - val_loss: 0.4302 - val_accuracy: 0.9042 - val_precision: 0.0134 - val_recall: 0.9365 - val_auc: 0.9750 - val_prc: 0.5946\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.1388e-04 - accuracy: 0.9049 - precision: 0.0176 - recall: 0.9598 - auc: 0.9854 - prc: 0.6876 - val_loss: 0.4156 - val_accuracy: 0.9072 - val_precision: 0.0138 - val_recall: 0.9365 - val_auc: 0.9750 - val_prc: 0.6021\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.0937e-04 - accuracy: 0.9083 - precision: 0.0181 - recall: 0.9505 - auc: 0.9858 - prc: 0.6824 - val_loss: 0.4085 - val_accuracy: 0.9059 - val_precision: 0.0136 - val_recall: 0.9365 - val_auc: 0.9748 - val_prc: 0.5961\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.0263e-04 - accuracy: 0.9105 - precision: 0.0186 - recall: 0.9505 - auc: 0.9862 - prc: 0.6894 - val_loss: 0.4050 - val_accuracy: 0.9027 - val_precision: 0.0132 - val_recall: 0.9365 - val_auc: 0.9744 - val_prc: 0.6101\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.0180e-04 - accuracy: 0.9043 - precision: 0.0174 - recall: 0.9536 - auc: 0.9858 - prc: 0.6650 - val_loss: 0.3965 - val_accuracy: 0.9032 - val_precision: 0.0133 - val_recall: 0.9365 - val_auc: 0.9745 - val_prc: 0.6005\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 2.9580e-04 - accuracy: 0.9012 - precision: 0.0169 - recall: 0.9567 - auc: 0.9860 - prc: 0.6671 - val_loss: 0.3814 - val_accuracy: 0.9078 - val_precision: 0.0139 - val_recall: 0.9365 - val_auc: 0.9743 - val_prc: 0.6002\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 2.9263e-04 - accuracy: 0.9050 - precision: 0.0176 - recall: 0.9567 - auc: 0.9858 - prc: 0.6581 - val_loss: 0.3715 - val_accuracy: 0.9093 - val_precision: 0.0141 - val_recall: 0.9365 - val_auc: 0.9741 - val_prc: 0.6007\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.03\n",
            "****************************** w =  2\n",
            "Model: \"sequential_31\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_31 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_31 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_31 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "83/89 [==========================>...] - ETA: 0s - loss: 9.4246e-04 - accuracy: 0.4333 - precision: 0.0028 - recall: 0.9613 - auc: 0.9496 - prc: 0.5229"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 15ms/step - loss: 9.3707e-04 - accuracy: 0.4502 - precision: 0.0030 - recall: 0.9637 - auc: 0.9510 - prc: 0.5350 - val_loss: 0.6556 - val_accuracy: 0.8028 - val_precision: 0.0068 - val_recall: 0.9683 - val_auc: 0.9677 - val_prc: 0.6738\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.5399e-04 - accuracy: 0.9126 - precision: 0.0183 - recall: 0.9164 - auc: 0.9641 - prc: 0.7265 - val_loss: 0.5191 - val_accuracy: 0.9547 - val_precision: 0.0275 - val_recall: 0.9206 - val_auc: 0.9673 - val_prc: 0.6858\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 6.6762e-04 - accuracy: 0.9600 - precision: 0.0386 - recall: 0.9009 - auc: 0.9633 - prc: 0.7373 - val_loss: 0.4213 - val_accuracy: 0.9756 - val_precision: 0.0484 - val_recall: 0.8889 - val_auc: 0.9677 - val_prc: 0.7143\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 6.0898e-04 - accuracy: 0.9747 - precision: 0.0591 - recall: 0.8854 - auc: 0.9704 - prc: 0.7430 - val_loss: 0.3653 - val_accuracy: 0.9791 - val_precision: 0.0560 - val_recall: 0.8889 - val_auc: 0.9699 - val_prc: 0.7247\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 5.6250e-04 - accuracy: 0.9784 - precision: 0.0685 - recall: 0.8854 - auc: 0.9724 - prc: 0.7526 - val_loss: 0.3254 - val_accuracy: 0.9800 - val_precision: 0.0594 - val_recall: 0.9048 - val_auc: 0.9727 - val_prc: 0.7308\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 5.2893e-04 - accuracy: 0.9799 - precision: 0.0735 - recall: 0.8885 - auc: 0.9774 - prc: 0.7572 - val_loss: 0.2989 - val_accuracy: 0.9792 - val_precision: 0.0572 - val_recall: 0.9048 - val_auc: 0.9746 - val_prc: 0.7302\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 5.0196e-04 - accuracy: 0.9797 - precision: 0.0730 - recall: 0.8916 - auc: 0.9795 - prc: 0.7525 - val_loss: 0.2802 - val_accuracy: 0.9778 - val_precision: 0.0538 - val_recall: 0.9048 - val_auc: 0.9753 - val_prc: 0.7346\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 4.8133e-04 - accuracy: 0.9767 - precision: 0.0644 - recall: 0.8947 - auc: 0.9815 - prc: 0.7558 - val_loss: 0.2544 - val_accuracy: 0.9784 - val_precision: 0.0553 - val_recall: 0.9048 - val_auc: 0.9755 - val_prc: 0.7373\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 4.6126e-04 - accuracy: 0.9792 - precision: 0.0718 - recall: 0.8978 - auc: 0.9839 - prc: 0.7563 - val_loss: 0.2485 - val_accuracy: 0.9759 - val_precision: 0.0498 - val_recall: 0.9048 - val_auc: 0.9756 - val_prc: 0.7389\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 4.4495e-04 - accuracy: 0.9743 - precision: 0.0593 - recall: 0.9040 - auc: 0.9849 - prc: 0.7373 - val_loss: 0.2229 - val_accuracy: 0.9781 - val_precision: 0.0544 - val_recall: 0.9048 - val_auc: 0.9755 - val_prc: 0.7391\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.3388e-04 - accuracy: 0.9780 - precision: 0.0683 - recall: 0.9009 - auc: 0.9847 - prc: 0.7379 - val_loss: 0.2233 - val_accuracy: 0.9746 - val_precision: 0.0473 - val_recall: 0.9048 - val_auc: 0.9753 - val_prc: 0.7070\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 4.2250e-04 - accuracy: 0.9734 - precision: 0.0571 - recall: 0.9009 - auc: 0.9855 - prc: 0.7170 - val_loss: 0.2102 - val_accuracy: 0.9748 - val_precision: 0.0477 - val_recall: 0.9048 - val_auc: 0.9752 - val_prc: 0.6540\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 4.1750e-04 - accuracy: 0.9738 - precision: 0.0582 - recall: 0.9040 - auc: 0.9852 - prc: 0.7103 - val_loss: 0.2140 - val_accuracy: 0.9708 - val_precision: 0.0414 - val_recall: 0.9048 - val_auc: 0.9750 - val_prc: 0.6454\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 4.1123e-04 - accuracy: 0.9726 - precision: 0.0559 - recall: 0.9071 - auc: 0.9853 - prc: 0.7041 - val_loss: 0.2152 - val_accuracy: 0.9678 - val_precision: 0.0382 - val_recall: 0.9206 - val_auc: 0.9747 - val_prc: 0.6342\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 4.0545e-04 - accuracy: 0.9694 - precision: 0.0502 - recall: 0.9040 - auc: 0.9855 - prc: 0.6950 - val_loss: 0.2058 - val_accuracy: 0.9680 - val_precision: 0.0385 - val_recall: 0.9206 - val_auc: 0.9745 - val_prc: 0.6247\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.9661e-04 - accuracy: 0.9671 - precision: 0.0476 - recall: 0.9195 - auc: 0.9857 - prc: 0.6882 - val_loss: 0.1997 - val_accuracy: 0.9676 - val_precision: 0.0375 - val_recall: 0.9048 - val_auc: 0.9744 - val_prc: 0.6191\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.9028e-04 - accuracy: 0.9702 - precision: 0.0517 - recall: 0.9102 - auc: 0.9867 - prc: 0.6967 - val_loss: 0.2070 - val_accuracy: 0.9632 - val_precision: 0.0337 - val_recall: 0.9206 - val_auc: 0.9740 - val_prc: 0.6086\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.8870e-04 - accuracy: 0.9640 - precision: 0.0435 - recall: 0.9164 - auc: 0.9865 - prc: 0.6742 - val_loss: 0.1896 - val_accuracy: 0.9670 - val_precision: 0.0368 - val_recall: 0.9048 - val_auc: 0.9742 - val_prc: 0.6147\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.8832e-04 - accuracy: 0.9633 - precision: 0.0426 - recall: 0.9133 - auc: 0.9859 - prc: 0.6712 - val_loss: 0.1789 - val_accuracy: 0.9687 - val_precision: 0.0387 - val_recall: 0.9048 - val_auc: 0.9739 - val_prc: 0.6146\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.7962e-04 - accuracy: 0.9678 - precision: 0.0485 - recall: 0.9164 - auc: 0.9871 - prc: 0.6776 - val_loss: 0.1843 - val_accuracy: 0.9654 - val_precision: 0.0352 - val_recall: 0.9048 - val_auc: 0.9740 - val_prc: 0.6145\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.04\n",
            "****************************** w =  2\n",
            "Model: \"sequential_32\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_32 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_32 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_32 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89/89 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.8170 - precision: 0.0083 - recall: 0.8964 - auc: 0.9410 - prc: 0.5586"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 16ms/step - loss: 0.0013 - accuracy: 0.8170 - precision: 0.0083 - recall: 0.8964 - auc: 0.9410 - prc: 0.5586 - val_loss: 0.4783 - val_accuracy: 0.9841 - val_precision: 0.0725 - val_recall: 0.8889 - val_auc: 0.9625 - val_prc: 0.6953\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.4035e-04 - accuracy: 0.9887 - precision: 0.1221 - recall: 0.8638 - auc: 0.9500 - prc: 0.7268 - val_loss: 0.3129 - val_accuracy: 0.9919 - val_precision: 0.1311 - val_recall: 0.8571 - val_auc: 0.9657 - val_prc: 0.7085\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.7126e-04 - accuracy: 0.9908 - precision: 0.1465 - recall: 0.8638 - auc: 0.9667 - prc: 0.7409 - val_loss: 0.2223 - val_accuracy: 0.9933 - val_precision: 0.1519 - val_recall: 0.8413 - val_auc: 0.9681 - val_prc: 0.7203\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 6.8901e-04 - accuracy: 0.9924 - precision: 0.1728 - recall: 0.8638 - auc: 0.9681 - prc: 0.7460 - val_loss: 0.2054 - val_accuracy: 0.9884 - val_precision: 0.0962 - val_recall: 0.8730 - val_auc: 0.9721 - val_prc: 0.7315\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 6.3433e-04 - accuracy: 0.9897 - precision: 0.1331 - recall: 0.8700 - auc: 0.9765 - prc: 0.7580 - val_loss: 0.1753 - val_accuracy: 0.9886 - val_precision: 0.0972 - val_recall: 0.8730 - val_auc: 0.9735 - val_prc: 0.7354\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 5.9805e-04 - accuracy: 0.9883 - precision: 0.1190 - recall: 0.8700 - auc: 0.9795 - prc: 0.7552 - val_loss: 0.1527 - val_accuracy: 0.9889 - val_precision: 0.0998 - val_recall: 0.8730 - val_auc: 0.9735 - val_prc: 0.7383\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 5.7353e-04 - accuracy: 0.9883 - precision: 0.1199 - recall: 0.8762 - auc: 0.9813 - prc: 0.7606 - val_loss: 0.1478 - val_accuracy: 0.9871 - val_precision: 0.0895 - val_recall: 0.9048 - val_auc: 0.9740 - val_prc: 0.7399\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 5.4119e-04 - accuracy: 0.9875 - precision: 0.1131 - recall: 0.8824 - auc: 0.9838 - prc: 0.7546 - val_loss: 0.1380 - val_accuracy: 0.9869 - val_precision: 0.0882 - val_recall: 0.9048 - val_auc: 0.9739 - val_prc: 0.7297\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.2811e-04 - accuracy: 0.9873 - precision: 0.1118 - recall: 0.8854 - auc: 0.9845 - prc: 0.7568 - val_loss: 0.1376 - val_accuracy: 0.9854 - val_precision: 0.0799 - val_recall: 0.9048 - val_auc: 0.9735 - val_prc: 0.7136\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 5.1654e-04 - accuracy: 0.9856 - precision: 0.1005 - recall: 0.8916 - auc: 0.9846 - prc: 0.7350 - val_loss: 0.1319 - val_accuracy: 0.9848 - val_precision: 0.0769 - val_recall: 0.9048 - val_auc: 0.9733 - val_prc: 0.6916\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 5.0529e-04 - accuracy: 0.9849 - precision: 0.0959 - recall: 0.8885 - auc: 0.9848 - prc: 0.7203 - val_loss: 0.1284 - val_accuracy: 0.9840 - val_precision: 0.0734 - val_recall: 0.9048 - val_auc: 0.9736 - val_prc: 0.6755\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 5.0027e-04 - accuracy: 0.9845 - precision: 0.0942 - recall: 0.8947 - auc: 0.9852 - prc: 0.7137 - val_loss: 0.1226 - val_accuracy: 0.9840 - val_precision: 0.0735 - val_recall: 0.9048 - val_auc: 0.9730 - val_prc: 0.6545\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 4.9446e-04 - accuracy: 0.9841 - precision: 0.0924 - recall: 0.8978 - auc: 0.9851 - prc: 0.7063 - val_loss: 0.1224 - val_accuracy: 0.9832 - val_precision: 0.0700 - val_recall: 0.9048 - val_auc: 0.9729 - val_prc: 0.6344\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 4.7574e-04 - accuracy: 0.9827 - precision: 0.0853 - recall: 0.8947 - auc: 0.9866 - prc: 0.7062 - val_loss: 0.1101 - val_accuracy: 0.9852 - val_precision: 0.0791 - val_recall: 0.9048 - val_auc: 0.9733 - val_prc: 0.6445\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 4.8271e-04 - accuracy: 0.9849 - precision: 0.0965 - recall: 0.8947 - auc: 0.9855 - prc: 0.7117 - val_loss: 0.1248 - val_accuracy: 0.9813 - val_precision: 0.0632 - val_recall: 0.9048 - val_auc: 0.9726 - val_prc: 0.6242\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 4.7279e-04 - accuracy: 0.9821 - precision: 0.0827 - recall: 0.8978 - auc: 0.9861 - prc: 0.7021 - val_loss: 0.1195 - val_accuracy: 0.9817 - val_precision: 0.0648 - val_recall: 0.9048 - val_auc: 0.9724 - val_prc: 0.6282\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 4.7728e-04 - accuracy: 0.9813 - precision: 0.0798 - recall: 0.9040 - auc: 0.9853 - prc: 0.7018 - val_loss: 0.1106 - val_accuracy: 0.9834 - val_precision: 0.0707 - val_recall: 0.9048 - val_auc: 0.9728 - val_prc: 0.6247\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 4.6195e-04 - accuracy: 0.9840 - precision: 0.0913 - recall: 0.8947 - auc: 0.9877 - prc: 0.7009 - val_loss: 0.1297 - val_accuracy: 0.9776 - val_precision: 0.0534 - val_recall: 0.9048 - val_auc: 0.9724 - val_prc: 0.6208\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 4.6287e-04 - accuracy: 0.9791 - precision: 0.0723 - recall: 0.9071 - auc: 0.9868 - prc: 0.6971 - val_loss: 0.1153 - val_accuracy: 0.9811 - val_precision: 0.0628 - val_recall: 0.9048 - val_auc: 0.9729 - val_prc: 0.6275\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 4.5139e-04 - accuracy: 0.9821 - precision: 0.0830 - recall: 0.9040 - auc: 0.9874 - prc: 0.7009 - val_loss: 0.1187 - val_accuracy: 0.9796 - val_precision: 0.0583 - val_recall: 0.9048 - val_auc: 0.9728 - val_prc: 0.6185\n",
            "1419/1419 [==============================] - 2s 2ms/step\n",
            "****************************** w =  0.05\n",
            "****************************** w =  2\n",
            "Model: \"sequential_33\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_33 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_33 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_33 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "83/89 [==========================>...] - ETA: 0s - loss: 0.0017 - accuracy: 0.9624 - precision: 0.0382 - recall: 0.8602 - auc: 0.9415 - prc: 0.5824"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 15ms/step - loss: 0.0016 - accuracy: 0.9642 - precision: 0.0396 - recall: 0.8627 - auc: 0.9430 - prc: 0.5961 - val_loss: 0.3167 - val_accuracy: 0.9983 - val_precision: 0.4444 - val_recall: 0.8254 - val_auc: 0.9661 - val_prc: 0.7145\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0010 - accuracy: 0.9973 - precision: 0.3848 - recall: 0.8483 - auc: 0.9469 - prc: 0.7338 - val_loss: 0.1943 - val_accuracy: 0.9967 - val_precision: 0.2694 - val_recall: 0.8254 - val_auc: 0.9680 - val_prc: 0.7233\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.4339e-04 - accuracy: 0.9958 - precision: 0.2783 - recall: 0.8514 - auc: 0.9623 - prc: 0.7459 - val_loss: 0.1392 - val_accuracy: 0.9956 - val_precision: 0.2190 - val_recall: 0.8413 - val_auc: 0.9695 - val_prc: 0.7352\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 7.4247e-04 - accuracy: 0.9943 - precision: 0.2197 - recall: 0.8638 - auc: 0.9748 - prc: 0.7566 - val_loss: 0.1077 - val_accuracy: 0.9953 - val_precision: 0.2062 - val_recall: 0.8413 - val_auc: 0.9725 - val_prc: 0.7362\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 6.9715e-04 - accuracy: 0.9944 - precision: 0.2225 - recall: 0.8638 - auc: 0.9741 - prc: 0.7588 - val_loss: 0.1122 - val_accuracy: 0.9913 - val_precision: 0.1222 - val_recall: 0.8571 - val_auc: 0.9745 - val_prc: 0.7413\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 6.5636e-04 - accuracy: 0.9920 - precision: 0.1663 - recall: 0.8700 - auc: 0.9810 - prc: 0.7594 - val_loss: 0.0990 - val_accuracy: 0.9917 - val_precision: 0.1283 - val_recall: 0.8571 - val_auc: 0.9755 - val_prc: 0.7302\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 6.2505e-04 - accuracy: 0.9916 - precision: 0.1588 - recall: 0.8669 - auc: 0.9839 - prc: 0.7612 - val_loss: 0.0848 - val_accuracy: 0.9930 - val_precision: 0.1468 - val_recall: 0.8413 - val_auc: 0.9752 - val_prc: 0.7321\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 6.1033e-04 - accuracy: 0.9917 - precision: 0.1620 - recall: 0.8731 - auc: 0.9838 - prc: 0.7585 - val_loss: 0.0904 - val_accuracy: 0.9903 - val_precision: 0.1125 - val_recall: 0.8730 - val_auc: 0.9753 - val_prc: 0.7331\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 5.9379e-04 - accuracy: 0.9914 - precision: 0.1565 - recall: 0.8793 - auc: 0.9846 - prc: 0.7485 - val_loss: 0.0929 - val_accuracy: 0.9894 - val_precision: 0.1042 - val_recall: 0.8730 - val_auc: 0.9748 - val_prc: 0.7233\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 5.8335e-04 - accuracy: 0.9904 - precision: 0.1431 - recall: 0.8824 - auc: 0.9843 - prc: 0.7356 - val_loss: 0.0931 - val_accuracy: 0.9888 - val_precision: 0.1002 - val_recall: 0.8889 - val_auc: 0.9745 - val_prc: 0.7073\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 5.7829e-04 - accuracy: 0.9896 - precision: 0.1338 - recall: 0.8854 - auc: 0.9844 - prc: 0.7435 - val_loss: 0.0843 - val_accuracy: 0.9893 - val_precision: 0.1036 - val_recall: 0.8730 - val_auc: 0.9742 - val_prc: 0.7203\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 5.5936e-04 - accuracy: 0.9904 - precision: 0.1441 - recall: 0.8885 - auc: 0.9861 - prc: 0.7301 - val_loss: 0.0862 - val_accuracy: 0.9883 - val_precision: 0.0969 - val_recall: 0.8889 - val_auc: 0.9743 - val_prc: 0.6750\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 5.5020e-04 - accuracy: 0.9894 - precision: 0.1315 - recall: 0.8885 - auc: 0.9859 - prc: 0.7271 - val_loss: 0.0847 - val_accuracy: 0.9883 - val_precision: 0.0966 - val_recall: 0.8889 - val_auc: 0.9746 - val_prc: 0.6752\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 5.5677e-04 - accuracy: 0.9892 - precision: 0.1293 - recall: 0.8885 - auc: 0.9852 - prc: 0.7197 - val_loss: 0.0853 - val_accuracy: 0.9878 - val_precision: 0.0927 - val_recall: 0.8889 - val_auc: 0.9743 - val_prc: 0.6679\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 5.4294e-04 - accuracy: 0.9882 - precision: 0.1206 - recall: 0.8947 - auc: 0.9866 - prc: 0.7170 - val_loss: 0.0773 - val_accuracy: 0.9891 - val_precision: 0.1028 - val_recall: 0.8889 - val_auc: 0.9748 - val_prc: 0.6886\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 5.3652e-04 - accuracy: 0.9890 - precision: 0.1283 - recall: 0.8978 - auc: 0.9866 - prc: 0.7091 - val_loss: 0.0770 - val_accuracy: 0.9888 - val_precision: 0.1002 - val_recall: 0.8889 - val_auc: 0.9735 - val_prc: 0.6783\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.4021e-04 - accuracy: 0.9890 - precision: 0.1278 - recall: 0.8885 - auc: 0.9861 - prc: 0.7176 - val_loss: 0.0781 - val_accuracy: 0.9882 - val_precision: 0.0961 - val_recall: 0.8889 - val_auc: 0.9735 - val_prc: 0.6675\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 5.3551e-04 - accuracy: 0.9887 - precision: 0.1246 - recall: 0.8916 - auc: 0.9870 - prc: 0.7157 - val_loss: 0.0775 - val_accuracy: 0.9882 - val_precision: 0.0959 - val_recall: 0.8889 - val_auc: 0.9738 - val_prc: 0.6683\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 5.3168e-04 - accuracy: 0.9878 - precision: 0.1173 - recall: 0.8947 - auc: 0.9862 - prc: 0.7087 - val_loss: 0.0715 - val_accuracy: 0.9899 - val_precision: 0.1102 - val_recall: 0.8889 - val_auc: 0.9739 - val_prc: 0.6897\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 5.3271e-04 - accuracy: 0.9882 - precision: 0.1207 - recall: 0.8978 - auc: 0.9862 - prc: 0.7151 - val_loss: 0.0714 - val_accuracy: 0.9894 - val_precision: 0.1057 - val_recall: 0.8889 - val_auc: 0.9741 - val_prc: 0.6791\n",
            "1419/1419 [==============================] - 2s 2ms/step\n",
            "****************************** w =  0.060000000000000005\n",
            "****************************** w =  2\n",
            "Model: \"sequential_34\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_34 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_34 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_34 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "83/89 [==========================>...] - ETA: 0s - loss: 0.0021 - accuracy: 0.9622 - precision: 0.0360 - recall: 0.8229 - auc: 0.9341 - prc: 0.5906"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 14ms/step - loss: 0.0020 - accuracy: 0.9641 - precision: 0.0378 - recall: 0.8238 - auc: 0.9372 - prc: 0.5977 - val_loss: 0.2361 - val_accuracy: 0.9992 - val_precision: 0.6892 - val_recall: 0.8095 - val_auc: 0.9624 - val_prc: 0.7181\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0011 - accuracy: 0.9991 - precision: 0.7162 - recall: 0.8359 - auc: 0.9440 - prc: 0.7334 - val_loss: 0.1387 - val_accuracy: 0.9981 - val_precision: 0.4127 - val_recall: 0.8254 - val_auc: 0.9658 - val_prc: 0.7315\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 8.9594e-04 - accuracy: 0.9974 - precision: 0.3937 - recall: 0.8545 - auc: 0.9633 - prc: 0.7478 - val_loss: 0.0852 - val_accuracy: 0.9985 - val_precision: 0.4727 - val_recall: 0.8254 - val_auc: 0.9680 - val_prc: 0.7350\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 8.0208e-04 - accuracy: 0.9974 - precision: 0.3960 - recall: 0.8607 - auc: 0.9729 - prc: 0.7563 - val_loss: 0.0795 - val_accuracy: 0.9968 - val_precision: 0.2826 - val_recall: 0.8254 - val_auc: 0.9716 - val_prc: 0.7390\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.5063e-04 - accuracy: 0.9956 - precision: 0.2706 - recall: 0.8638 - auc: 0.9782 - prc: 0.7593 - val_loss: 0.0699 - val_accuracy: 0.9964 - val_precision: 0.2585 - val_recall: 0.8413 - val_auc: 0.9731 - val_prc: 0.7417\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 7.1777e-04 - accuracy: 0.9956 - precision: 0.2678 - recall: 0.8638 - auc: 0.9805 - prc: 0.7584 - val_loss: 0.0786 - val_accuracy: 0.9929 - val_precision: 0.1456 - val_recall: 0.8413 - val_auc: 0.9741 - val_prc: 0.7302\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 6.9799e-04 - accuracy: 0.9939 - precision: 0.2094 - recall: 0.8669 - auc: 0.9811 - prc: 0.7606 - val_loss: 0.0711 - val_accuracy: 0.9934 - val_precision: 0.1550 - val_recall: 0.8413 - val_auc: 0.9738 - val_prc: 0.7310\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 6.7428e-04 - accuracy: 0.9937 - precision: 0.2030 - recall: 0.8731 - auc: 0.9836 - prc: 0.7541 - val_loss: 0.0684 - val_accuracy: 0.9930 - val_precision: 0.1468 - val_recall: 0.8413 - val_auc: 0.9731 - val_prc: 0.7200\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 6.6466e-04 - accuracy: 0.9937 - precision: 0.2030 - recall: 0.8700 - auc: 0.9836 - prc: 0.7450 - val_loss: 0.0708 - val_accuracy: 0.9918 - val_precision: 0.1289 - val_recall: 0.8571 - val_auc: 0.9733 - val_prc: 0.7207\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 6.4465e-04 - accuracy: 0.9923 - precision: 0.1721 - recall: 0.8762 - auc: 0.9843 - prc: 0.7368 - val_loss: 0.0589 - val_accuracy: 0.9940 - val_precision: 0.1693 - val_recall: 0.8571 - val_auc: 0.9729 - val_prc: 0.7180\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 6.3726e-04 - accuracy: 0.9927 - precision: 0.1807 - recall: 0.8793 - auc: 0.9841 - prc: 0.7454 - val_loss: 0.0534 - val_accuracy: 0.9946 - val_precision: 0.1834 - val_recall: 0.8413 - val_auc: 0.9731 - val_prc: 0.7323\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 6.3988e-04 - accuracy: 0.9930 - precision: 0.1866 - recall: 0.8731 - auc: 0.9848 - prc: 0.7411 - val_loss: 0.0594 - val_accuracy: 0.9930 - val_precision: 0.1479 - val_recall: 0.8571 - val_auc: 0.9734 - val_prc: 0.7220\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 6.1728e-04 - accuracy: 0.9927 - precision: 0.1817 - recall: 0.8916 - auc: 0.9852 - prc: 0.7443 - val_loss: 0.0546 - val_accuracy: 0.9937 - val_precision: 0.1631 - val_recall: 0.8571 - val_auc: 0.9736 - val_prc: 0.7222\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 6.1895e-04 - accuracy: 0.9930 - precision: 0.1876 - recall: 0.8885 - auc: 0.9854 - prc: 0.7390 - val_loss: 0.0609 - val_accuracy: 0.9920 - val_precision: 0.1341 - val_recall: 0.8730 - val_auc: 0.9737 - val_prc: 0.7231\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 6.0942e-04 - accuracy: 0.9925 - precision: 0.1779 - recall: 0.8854 - auc: 0.9863 - prc: 0.7468 - val_loss: 0.0610 - val_accuracy: 0.9918 - val_precision: 0.1313 - val_recall: 0.8730 - val_auc: 0.9740 - val_prc: 0.7266\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 6.1149e-04 - accuracy: 0.9925 - precision: 0.1774 - recall: 0.8885 - auc: 0.9857 - prc: 0.7315 - val_loss: 0.0619 - val_accuracy: 0.9914 - val_precision: 0.1253 - val_recall: 0.8730 - val_auc: 0.9740 - val_prc: 0.7023\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 6.0467e-04 - accuracy: 0.9918 - precision: 0.1654 - recall: 0.8885 - auc: 0.9854 - prc: 0.7390 - val_loss: 0.0562 - val_accuracy: 0.9925 - val_precision: 0.1425 - val_recall: 0.8730 - val_auc: 0.9744 - val_prc: 0.7278\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 5.9393e-04 - accuracy: 0.9928 - precision: 0.1838 - recall: 0.8824 - auc: 0.9869 - prc: 0.7347 - val_loss: 0.0669 - val_accuracy: 0.9897 - val_precision: 0.1083 - val_recall: 0.8889 - val_auc: 0.9738 - val_prc: 0.6825\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 5.9993e-04 - accuracy: 0.9918 - precision: 0.1652 - recall: 0.8885 - auc: 0.9859 - prc: 0.7388 - val_loss: 0.0608 - val_accuracy: 0.9913 - val_precision: 0.1258 - val_recall: 0.8889 - val_auc: 0.9738 - val_prc: 0.7022\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 5.9154e-04 - accuracy: 0.9923 - precision: 0.1732 - recall: 0.8885 - auc: 0.9870 - prc: 0.7356 - val_loss: 0.0630 - val_accuracy: 0.9906 - val_precision: 0.1181 - val_recall: 0.8889 - val_auc: 0.9737 - val_prc: 0.7058\n",
            "1419/1419 [==============================] - 2s 2ms/step\n",
            "****************************** w =  0.07\n",
            "****************************** w =  2\n",
            "Model: \"sequential_35\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_35 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_35 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_35 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85/89 [===========================>..] - ETA: 0s - loss: 0.0024 - accuracy: 0.9893 - precision: 0.1143 - recall: 0.7913 - auc: 0.9194 - prc: 0.5787"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 14ms/step - loss: 0.0023 - accuracy: 0.9897 - precision: 0.1188 - recall: 0.7902 - auc: 0.9176 - prc: 0.5840 - val_loss: 0.1726 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9548 - val_prc: 0.7120\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0012 - accuracy: 0.9993 - precision: 0.8049 - recall: 0.8173 - auc: 0.9409 - prc: 0.7241 - val_loss: 0.0915 - val_accuracy: 0.9992 - val_precision: 0.6711 - val_recall: 0.8095 - val_auc: 0.9626 - val_prc: 0.7307\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 9.6530e-04 - accuracy: 0.9990 - precision: 0.6923 - recall: 0.8359 - auc: 0.9550 - prc: 0.7487 - val_loss: 0.0729 - val_accuracy: 0.9985 - val_precision: 0.4727 - val_recall: 0.8254 - val_auc: 0.9645 - val_prc: 0.7428\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 8.7101e-04 - accuracy: 0.9981 - precision: 0.4750 - recall: 0.8545 - auc: 0.9677 - prc: 0.7545 - val_loss: 0.0626 - val_accuracy: 0.9978 - val_precision: 0.3636 - val_recall: 0.8254 - val_auc: 0.9689 - val_prc: 0.7428\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.1624e-04 - accuracy: 0.9973 - precision: 0.3880 - recall: 0.8638 - auc: 0.9753 - prc: 0.7578 - val_loss: 0.0522 - val_accuracy: 0.9978 - val_precision: 0.3714 - val_recall: 0.8254 - val_auc: 0.9711 - val_prc: 0.7445\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.7161e-04 - accuracy: 0.9970 - precision: 0.3582 - recall: 0.8638 - auc: 0.9804 - prc: 0.7578 - val_loss: 0.0520 - val_accuracy: 0.9970 - val_precision: 0.2994 - val_recall: 0.8413 - val_auc: 0.9736 - val_prc: 0.7331\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.4285e-04 - accuracy: 0.9962 - precision: 0.2987 - recall: 0.8638 - auc: 0.9818 - prc: 0.7605 - val_loss: 0.0467 - val_accuracy: 0.9972 - val_precision: 0.3081 - val_recall: 0.8413 - val_auc: 0.9741 - val_prc: 0.7309\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.2584e-04 - accuracy: 0.9962 - precision: 0.3006 - recall: 0.8638 - auc: 0.9824 - prc: 0.7590 - val_loss: 0.0472 - val_accuracy: 0.9965 - val_precision: 0.2637 - val_recall: 0.8413 - val_auc: 0.9734 - val_prc: 0.7327\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.0379e-04 - accuracy: 0.9963 - precision: 0.3098 - recall: 0.8700 - auc: 0.9839 - prc: 0.7531 - val_loss: 0.0543 - val_accuracy: 0.9945 - val_precision: 0.1803 - val_recall: 0.8413 - val_auc: 0.9738 - val_prc: 0.7217\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 6.9402e-04 - accuracy: 0.9954 - precision: 0.2616 - recall: 0.8700 - auc: 0.9849 - prc: 0.7590 - val_loss: 0.0519 - val_accuracy: 0.9947 - val_precision: 0.1879 - val_recall: 0.8413 - val_auc: 0.9736 - val_prc: 0.7226\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 6.8804e-04 - accuracy: 0.9954 - precision: 0.2616 - recall: 0.8700 - auc: 0.9856 - prc: 0.7568 - val_loss: 0.0562 - val_accuracy: 0.9933 - val_precision: 0.1532 - val_recall: 0.8413 - val_auc: 0.9740 - val_prc: 0.7254\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 6.7448e-04 - accuracy: 0.9946 - precision: 0.2293 - recall: 0.8731 - auc: 0.9858 - prc: 0.7523 - val_loss: 0.0446 - val_accuracy: 0.9954 - val_precision: 0.2120 - val_recall: 0.8413 - val_auc: 0.9732 - val_prc: 0.7237\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 6.7923e-04 - accuracy: 0.9951 - precision: 0.2504 - recall: 0.8762 - auc: 0.9852 - prc: 0.7461 - val_loss: 0.0429 - val_accuracy: 0.9955 - val_precision: 0.2137 - val_recall: 0.8413 - val_auc: 0.9736 - val_prc: 0.7353\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 6.7919e-04 - accuracy: 0.9948 - precision: 0.2375 - recall: 0.8793 - auc: 0.9845 - prc: 0.7404 - val_loss: 0.0438 - val_accuracy: 0.9953 - val_precision: 0.2062 - val_recall: 0.8413 - val_auc: 0.9736 - val_prc: 0.7243\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 6.5621e-04 - accuracy: 0.9950 - precision: 0.2478 - recall: 0.8824 - auc: 0.9862 - prc: 0.7492 - val_loss: 0.0443 - val_accuracy: 0.9950 - val_precision: 0.1970 - val_recall: 0.8413 - val_auc: 0.9735 - val_prc: 0.7236\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 6.5641e-04 - accuracy: 0.9950 - precision: 0.2453 - recall: 0.8824 - auc: 0.9863 - prc: 0.7508 - val_loss: 0.0470 - val_accuracy: 0.9941 - val_precision: 0.1715 - val_recall: 0.8413 - val_auc: 0.9733 - val_prc: 0.7269\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 6.6364e-04 - accuracy: 0.9945 - precision: 0.2279 - recall: 0.8793 - auc: 0.9861 - prc: 0.7435 - val_loss: 0.0422 - val_accuracy: 0.9952 - val_precision: 0.2015 - val_recall: 0.8413 - val_auc: 0.9730 - val_prc: 0.7262\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 6.4971e-04 - accuracy: 0.9948 - precision: 0.2381 - recall: 0.8854 - auc: 0.9862 - prc: 0.7386 - val_loss: 0.0432 - val_accuracy: 0.9949 - val_precision: 0.1927 - val_recall: 0.8413 - val_auc: 0.9729 - val_prc: 0.7291\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 6.4323e-04 - accuracy: 0.9944 - precision: 0.2272 - recall: 0.8854 - auc: 0.9858 - prc: 0.7454 - val_loss: 0.0379 - val_accuracy: 0.9956 - val_precision: 0.2199 - val_recall: 0.8413 - val_auc: 0.9742 - val_prc: 0.7256\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 6.5641e-04 - accuracy: 0.9953 - precision: 0.2591 - recall: 0.8854 - auc: 0.9848 - prc: 0.7476 - val_loss: 0.0489 - val_accuracy: 0.9933 - val_precision: 0.1576 - val_recall: 0.8730 - val_auc: 0.9724 - val_prc: 0.7056\n",
            "1419/1419 [==============================] - 2s 2ms/step\n",
            "****************************** w =  0.08\n",
            "****************************** w =  2\n",
            "Model: \"sequential_36\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_36 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_36 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_36 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_36 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87/89 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9748 - precision: 0.0523 - recall: 0.8143 - auc: 0.9222 - prc: 0.5973"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 20ms/step - loss: 0.0026 - accuracy: 0.9752 - precision: 0.0537 - recall: 0.8187 - auc: 0.9245 - prc: 0.6003 - val_loss: 0.1248 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9588 - val_prc: 0.7033\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0012 - accuracy: 0.9993 - precision: 0.7838 - recall: 0.8080 - auc: 0.9423 - prc: 0.7326 - val_loss: 0.0654 - val_accuracy: 0.9991 - val_precision: 0.6456 - val_recall: 0.8095 - val_auc: 0.9637 - val_prc: 0.7062\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.9458e-04 - accuracy: 0.9990 - precision: 0.6758 - recall: 0.8390 - auc: 0.9586 - prc: 0.7488 - val_loss: 0.0486 - val_accuracy: 0.9990 - val_precision: 0.6118 - val_recall: 0.8254 - val_auc: 0.9654 - val_prc: 0.7133\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.9155e-04 - accuracy: 0.9985 - precision: 0.5567 - recall: 0.8514 - auc: 0.9710 - prc: 0.7567 - val_loss: 0.0419 - val_accuracy: 0.9987 - val_precision: 0.5253 - val_recall: 0.8254 - val_auc: 0.9700 - val_prc: 0.7286\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.5787e-04 - accuracy: 0.9983 - precision: 0.5082 - recall: 0.8607 - auc: 0.9753 - prc: 0.7608 - val_loss: 0.0420 - val_accuracy: 0.9982 - val_precision: 0.4262 - val_recall: 0.8254 - val_auc: 0.9727 - val_prc: 0.7319\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.1380e-04 - accuracy: 0.9980 - precision: 0.4626 - recall: 0.8607 - auc: 0.9796 - prc: 0.7585 - val_loss: 0.0429 - val_accuracy: 0.9972 - val_precision: 0.3155 - val_recall: 0.8413 - val_auc: 0.9736 - val_prc: 0.7207\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.8674e-04 - accuracy: 0.9972 - precision: 0.3796 - recall: 0.8638 - auc: 0.9826 - prc: 0.7605 - val_loss: 0.0399 - val_accuracy: 0.9972 - val_precision: 0.3155 - val_recall: 0.8413 - val_auc: 0.9737 - val_prc: 0.7335\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.7256e-04 - accuracy: 0.9971 - precision: 0.3690 - recall: 0.8638 - auc: 0.9820 - prc: 0.7588 - val_loss: 0.0386 - val_accuracy: 0.9970 - val_precision: 0.2944 - val_recall: 0.8413 - val_auc: 0.9746 - val_prc: 0.7359\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.5958e-04 - accuracy: 0.9968 - precision: 0.3386 - recall: 0.8669 - auc: 0.9823 - prc: 0.7578 - val_loss: 0.0374 - val_accuracy: 0.9968 - val_precision: 0.2849 - val_recall: 0.8413 - val_auc: 0.9724 - val_prc: 0.7348\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.4059e-04 - accuracy: 0.9972 - precision: 0.3714 - recall: 0.8669 - auc: 0.9842 - prc: 0.7627 - val_loss: 0.0371 - val_accuracy: 0.9966 - val_precision: 0.2690 - val_recall: 0.8413 - val_auc: 0.9730 - val_prc: 0.7343\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.3349e-04 - accuracy: 0.9966 - precision: 0.3298 - recall: 0.8669 - auc: 0.9852 - prc: 0.7560 - val_loss: 0.0360 - val_accuracy: 0.9965 - val_precision: 0.2624 - val_recall: 0.8413 - val_auc: 0.9734 - val_prc: 0.7334\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.3089e-04 - accuracy: 0.9964 - precision: 0.3164 - recall: 0.8669 - auc: 0.9847 - prc: 0.7575 - val_loss: 0.0346 - val_accuracy: 0.9966 - val_precision: 0.2663 - val_recall: 0.8413 - val_auc: 0.9725 - val_prc: 0.7369\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 7.2776e-04 - accuracy: 0.9967 - precision: 0.3337 - recall: 0.8762 - auc: 0.9841 - prc: 0.7617 - val_loss: 0.0398 - val_accuracy: 0.9955 - val_precision: 0.2129 - val_recall: 0.8413 - val_auc: 0.9732 - val_prc: 0.7223\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 7.1759e-04 - accuracy: 0.9961 - precision: 0.2969 - recall: 0.8824 - auc: 0.9851 - prc: 0.7572 - val_loss: 0.0387 - val_accuracy: 0.9956 - val_precision: 0.2172 - val_recall: 0.8413 - val_auc: 0.9734 - val_prc: 0.7243\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 7.1178e-04 - accuracy: 0.9962 - precision: 0.3059 - recall: 0.8762 - auc: 0.9854 - prc: 0.7627 - val_loss: 0.0412 - val_accuracy: 0.9952 - val_precision: 0.2015 - val_recall: 0.8413 - val_auc: 0.9732 - val_prc: 0.7273\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.0996e-04 - accuracy: 0.9961 - precision: 0.2990 - recall: 0.8731 - auc: 0.9857 - prc: 0.7471 - val_loss: 0.0365 - val_accuracy: 0.9957 - val_precision: 0.2218 - val_recall: 0.8413 - val_auc: 0.9727 - val_prc: 0.7248\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.0404e-04 - accuracy: 0.9960 - precision: 0.2948 - recall: 0.8854 - auc: 0.9852 - prc: 0.7555 - val_loss: 0.0341 - val_accuracy: 0.9961 - val_precision: 0.2398 - val_recall: 0.8413 - val_auc: 0.9736 - val_prc: 0.7260\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 7.0184e-04 - accuracy: 0.9962 - precision: 0.3067 - recall: 0.8793 - auc: 0.9860 - prc: 0.7524 - val_loss: 0.0359 - val_accuracy: 0.9956 - val_precision: 0.2199 - val_recall: 0.8413 - val_auc: 0.9732 - val_prc: 0.7263\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 7.0281e-04 - accuracy: 0.9960 - precision: 0.2932 - recall: 0.8824 - auc: 0.9857 - prc: 0.7548 - val_loss: 0.0347 - val_accuracy: 0.9960 - val_precision: 0.2356 - val_recall: 0.8413 - val_auc: 0.9737 - val_prc: 0.7281\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 6.9071e-04 - accuracy: 0.9962 - precision: 0.3067 - recall: 0.8793 - auc: 0.9859 - prc: 0.7571 - val_loss: 0.0351 - val_accuracy: 0.9957 - val_precision: 0.2208 - val_recall: 0.8413 - val_auc: 0.9737 - val_prc: 0.7255\n",
            "1419/1419 [==============================] - 2s 2ms/step\n",
            "****************************** w =  0.09\n",
            "****************************** w =  2\n",
            "Model: \"sequential_37\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_37 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_37 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_37 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85/89 [===========================>..] - ETA: 0s - loss: 0.0028 - accuracy: 0.9916 - precision: 0.1442 - recall: 0.7863 - auc: 0.9223 - prc: 0.6157"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 4s 20ms/step - loss: 0.0027 - accuracy: 0.9918 - precision: 0.1464 - recall: 0.7876 - auc: 0.9230 - prc: 0.6162 - val_loss: 0.0866 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9589 - val_prc: 0.7138\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0012 - accuracy: 0.9993 - precision: 0.7957 - recall: 0.8080 - auc: 0.9473 - prc: 0.7320 - val_loss: 0.0416 - val_accuracy: 0.9993 - val_precision: 0.7391 - val_recall: 0.8095 - val_auc: 0.9654 - val_prc: 0.7282\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0010 - accuracy: 0.9992 - precision: 0.7507 - recall: 0.8297 - auc: 0.9611 - prc: 0.7492 - val_loss: 0.0339 - val_accuracy: 0.9992 - val_precision: 0.6892 - val_recall: 0.8095 - val_auc: 0.9701 - val_prc: 0.7402\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.4790e-04 - accuracy: 0.9990 - precision: 0.6675 - recall: 0.8390 - auc: 0.9671 - prc: 0.7573 - val_loss: 0.0340 - val_accuracy: 0.9990 - val_precision: 0.6118 - val_recall: 0.8254 - val_auc: 0.9730 - val_prc: 0.7330\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.9477e-04 - accuracy: 0.9988 - precision: 0.6166 - recall: 0.8514 - auc: 0.9741 - prc: 0.7605 - val_loss: 0.0364 - val_accuracy: 0.9984 - val_precision: 0.4643 - val_recall: 0.8254 - val_auc: 0.9755 - val_prc: 0.7226\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.5803e-04 - accuracy: 0.9984 - precision: 0.5304 - recall: 0.8638 - auc: 0.9778 - prc: 0.7626 - val_loss: 0.0321 - val_accuracy: 0.9985 - val_precision: 0.4771 - val_recall: 0.8254 - val_auc: 0.9765 - val_prc: 0.7360\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.2636e-04 - accuracy: 0.9983 - precision: 0.5176 - recall: 0.8638 - auc: 0.9804 - prc: 0.7667 - val_loss: 0.0344 - val_accuracy: 0.9979 - val_precision: 0.3813 - val_recall: 0.8413 - val_auc: 0.9765 - val_prc: 0.7359\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.1144e-04 - accuracy: 0.9980 - precision: 0.4619 - recall: 0.8638 - auc: 0.9814 - prc: 0.7552 - val_loss: 0.0325 - val_accuracy: 0.9980 - val_precision: 0.3926 - val_recall: 0.8413 - val_auc: 0.9743 - val_prc: 0.7376\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.8751e-04 - accuracy: 0.9978 - precision: 0.4339 - recall: 0.8638 - auc: 0.9845 - prc: 0.7646 - val_loss: 0.0259 - val_accuracy: 0.9984 - val_precision: 0.4561 - val_recall: 0.8254 - val_auc: 0.9747 - val_prc: 0.7383\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.8981e-04 - accuracy: 0.9981 - precision: 0.4877 - recall: 0.8576 - auc: 0.9843 - prc: 0.7624 - val_loss: 0.0325 - val_accuracy: 0.9972 - val_precision: 0.3155 - val_recall: 0.8413 - val_auc: 0.9748 - val_prc: 0.7390\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.6752e-04 - accuracy: 0.9976 - precision: 0.4188 - recall: 0.8700 - auc: 0.9846 - prc: 0.7553 - val_loss: 0.0291 - val_accuracy: 0.9977 - val_precision: 0.3581 - val_recall: 0.8413 - val_auc: 0.9743 - val_prc: 0.7397\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.5189e-04 - accuracy: 0.9976 - precision: 0.4120 - recall: 0.8700 - auc: 0.9852 - prc: 0.7635 - val_loss: 0.0267 - val_accuracy: 0.9978 - val_precision: 0.3759 - val_recall: 0.8413 - val_auc: 0.9755 - val_prc: 0.7517\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 7.5747e-04 - accuracy: 0.9974 - precision: 0.3977 - recall: 0.8731 - auc: 0.9856 - prc: 0.7634 - val_loss: 0.0251 - val_accuracy: 0.9979 - val_precision: 0.3841 - val_recall: 0.8413 - val_auc: 0.9761 - val_prc: 0.7542\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 7.5018e-04 - accuracy: 0.9977 - precision: 0.4299 - recall: 0.8731 - auc: 0.9848 - prc: 0.7627 - val_loss: 0.0291 - val_accuracy: 0.9972 - val_precision: 0.3081 - val_recall: 0.8413 - val_auc: 0.9751 - val_prc: 0.7267\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 7.4658e-04 - accuracy: 0.9972 - precision: 0.3757 - recall: 0.8700 - auc: 0.9859 - prc: 0.7582 - val_loss: 0.0297 - val_accuracy: 0.9970 - val_precision: 0.2928 - val_recall: 0.8413 - val_auc: 0.9753 - val_prc: 0.7277\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.4622e-04 - accuracy: 0.9973 - precision: 0.3898 - recall: 0.8762 - auc: 0.9854 - prc: 0.7621 - val_loss: 0.0281 - val_accuracy: 0.9972 - val_precision: 0.3136 - val_recall: 0.8413 - val_auc: 0.9755 - val_prc: 0.7278\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.4184e-04 - accuracy: 0.9970 - precision: 0.3602 - recall: 0.8731 - auc: 0.9859 - prc: 0.7677 - val_loss: 0.0244 - val_accuracy: 0.9978 - val_precision: 0.3706 - val_recall: 0.8413 - val_auc: 0.9750 - val_prc: 0.7542\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.3661e-04 - accuracy: 0.9977 - precision: 0.4316 - recall: 0.8793 - auc: 0.9857 - prc: 0.7650 - val_loss: 0.0314 - val_accuracy: 0.9964 - val_precision: 0.2573 - val_recall: 0.8413 - val_auc: 0.9752 - val_prc: 0.7311\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.3482e-04 - accuracy: 0.9971 - precision: 0.3649 - recall: 0.8824 - auc: 0.9855 - prc: 0.7623 - val_loss: 0.0286 - val_accuracy: 0.9968 - val_precision: 0.2849 - val_recall: 0.8413 - val_auc: 0.9757 - val_prc: 0.7296\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.3205e-04 - accuracy: 0.9971 - precision: 0.3680 - recall: 0.8762 - auc: 0.9855 - prc: 0.7663 - val_loss: 0.0280 - val_accuracy: 0.9970 - val_precision: 0.2928 - val_recall: 0.8413 - val_auc: 0.9760 - val_prc: 0.7304\n",
            "1419/1419 [==============================] - 2s 2ms/step\n",
            "****************************** w =  0.09999999999999999\n",
            "****************************** w =  2\n",
            "Model: \"sequential_38\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_38 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_38 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_38 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_38 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86/89 [===========================>..] - ETA: 0s - loss: 0.0031 - accuracy: 0.9897 - precision: 0.1167 - recall: 0.7698 - auc: 0.9105 - prc: 0.6235"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 20ms/step - loss: 0.0031 - accuracy: 0.9899 - precision: 0.1187 - recall: 0.7694 - auc: 0.9102 - prc: 0.6219 - val_loss: 0.0720 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9576 - val_prc: 0.7078\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0013 - accuracy: 0.9993 - precision: 0.8107 - recall: 0.7957 - auc: 0.9420 - prc: 0.7276 - val_loss: 0.0364 - val_accuracy: 0.9993 - val_precision: 0.7083 - val_recall: 0.8095 - val_auc: 0.9635 - val_prc: 0.7221\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 0.9992 - precision: 0.7650 - recall: 0.8266 - auc: 0.9588 - prc: 0.7518 - val_loss: 0.0373 - val_accuracy: 0.9990 - val_precision: 0.6118 - val_recall: 0.8254 - val_auc: 0.9664 - val_prc: 0.7210\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.8166e-04 - accuracy: 0.9989 - precision: 0.6578 - recall: 0.8390 - auc: 0.9699 - prc: 0.7560 - val_loss: 0.0270 - val_accuracy: 0.9991 - val_precision: 0.6265 - val_recall: 0.8254 - val_auc: 0.9713 - val_prc: 0.7203\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.3660e-04 - accuracy: 0.9989 - precision: 0.6415 - recall: 0.8421 - auc: 0.9737 - prc: 0.7548 - val_loss: 0.0268 - val_accuracy: 0.9987 - val_precision: 0.5253 - val_recall: 0.8254 - val_auc: 0.9757 - val_prc: 0.7212\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.9526e-04 - accuracy: 0.9987 - precision: 0.5944 - recall: 0.8483 - auc: 0.9786 - prc: 0.7600 - val_loss: 0.0293 - val_accuracy: 0.9984 - val_precision: 0.4522 - val_recall: 0.8254 - val_auc: 0.9746 - val_prc: 0.7078\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.6937e-04 - accuracy: 0.9986 - precision: 0.5636 - recall: 0.8638 - auc: 0.9800 - prc: 0.7652 - val_loss: 0.0269 - val_accuracy: 0.9984 - val_precision: 0.4643 - val_recall: 0.8254 - val_auc: 0.9740 - val_prc: 0.7330\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.4999e-04 - accuracy: 0.9985 - precision: 0.5442 - recall: 0.8576 - auc: 0.9817 - prc: 0.7684 - val_loss: 0.0288 - val_accuracy: 0.9980 - val_precision: 0.3897 - val_recall: 0.8413 - val_auc: 0.9740 - val_prc: 0.7359\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.3553e-04 - accuracy: 0.9984 - precision: 0.5284 - recall: 0.8638 - auc: 0.9837 - prc: 0.7587 - val_loss: 0.0264 - val_accuracy: 0.9981 - val_precision: 0.4141 - val_recall: 0.8413 - val_auc: 0.9752 - val_prc: 0.7363\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 8.0875e-04 - accuracy: 0.9981 - precision: 0.4810 - recall: 0.8638 - auc: 0.9853 - prc: 0.7571 - val_loss: 0.0232 - val_accuracy: 0.9984 - val_precision: 0.4522 - val_recall: 0.8254 - val_auc: 0.9748 - val_prc: 0.7348\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.0572e-04 - accuracy: 0.9982 - precision: 0.5027 - recall: 0.8669 - auc: 0.9841 - prc: 0.7676 - val_loss: 0.0283 - val_accuracy: 0.9973 - val_precision: 0.3232 - val_recall: 0.8413 - val_auc: 0.9754 - val_prc: 0.7393\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 7.9552e-04 - accuracy: 0.9977 - precision: 0.4336 - recall: 0.8700 - auc: 0.9849 - prc: 0.7604 - val_loss: 0.0212 - val_accuracy: 0.9983 - val_precision: 0.4483 - val_recall: 0.8254 - val_auc: 0.9761 - val_prc: 0.7400\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 7.8802e-04 - accuracy: 0.9980 - precision: 0.4651 - recall: 0.8669 - auc: 0.9863 - prc: 0.7652 - val_loss: 0.0189 - val_accuracy: 0.9984 - val_precision: 0.4643 - val_recall: 0.8254 - val_auc: 0.9776 - val_prc: 0.7641\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 7.9657e-04 - accuracy: 0.9981 - precision: 0.4769 - recall: 0.8638 - auc: 0.9835 - prc: 0.7574 - val_loss: 0.0268 - val_accuracy: 0.9971 - val_precision: 0.3046 - val_recall: 0.8413 - val_auc: 0.9744 - val_prc: 0.7269\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 7.9515e-04 - accuracy: 0.9978 - precision: 0.4413 - recall: 0.8731 - auc: 0.9856 - prc: 0.7628 - val_loss: 0.0258 - val_accuracy: 0.9973 - val_precision: 0.3174 - val_recall: 0.8413 - val_auc: 0.9752 - val_prc: 0.7269\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.7624e-04 - accuracy: 0.9976 - precision: 0.4129 - recall: 0.8731 - auc: 0.9859 - prc: 0.7650 - val_loss: 0.0230 - val_accuracy: 0.9978 - val_precision: 0.3681 - val_recall: 0.8413 - val_auc: 0.9764 - val_prc: 0.7410\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.7656e-04 - accuracy: 0.9977 - precision: 0.4301 - recall: 0.8669 - auc: 0.9858 - prc: 0.7621 - val_loss: 0.0222 - val_accuracy: 0.9979 - val_precision: 0.3813 - val_recall: 0.8413 - val_auc: 0.9769 - val_prc: 0.7414\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.7311e-04 - accuracy: 0.9978 - precision: 0.4359 - recall: 0.8731 - auc: 0.9857 - prc: 0.7661 - val_loss: 0.0214 - val_accuracy: 0.9979 - val_precision: 0.3841 - val_recall: 0.8413 - val_auc: 0.9773 - val_prc: 0.7418\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.6921e-04 - accuracy: 0.9977 - precision: 0.4275 - recall: 0.8762 - auc: 0.9854 - prc: 0.7682 - val_loss: 0.0203 - val_accuracy: 0.9981 - val_precision: 0.4109 - val_recall: 0.8413 - val_auc: 0.9777 - val_prc: 0.7423\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.6319e-04 - accuracy: 0.9977 - precision: 0.4253 - recall: 0.8731 - auc: 0.9862 - prc: 0.7658 - val_loss: 0.0192 - val_accuracy: 0.9981 - val_precision: 0.4173 - val_recall: 0.8413 - val_auc: 0.9756 - val_prc: 0.7434\n",
            "1419/1419 [==============================] - 2s 2ms/step\n",
            "****************************** w =  0.10999999999999999\n",
            "****************************** w =  2\n",
            "Model: \"sequential_39\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_39 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_39 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_39 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_39 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87/89 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 0.9947 - precision: 0.2070 - recall: 0.7421 - auc: 0.9116 - prc: 0.6134"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 15ms/step - loss: 0.0032 - accuracy: 0.9948 - precision: 0.2104 - recall: 0.7461 - auc: 0.9139 - prc: 0.6207 - val_loss: 0.0491 - val_accuracy: 0.9994 - val_precision: 0.8000 - val_recall: 0.7619 - val_auc: 0.9591 - val_prc: 0.7148\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0013 - accuracy: 0.9993 - precision: 0.8069 - recall: 0.8019 - auc: 0.9536 - prc: 0.7373 - val_loss: 0.0285 - val_accuracy: 0.9993 - val_precision: 0.7391 - val_recall: 0.8095 - val_auc: 0.9633 - val_prc: 0.7106\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0011 - accuracy: 0.9992 - precision: 0.7666 - recall: 0.8235 - auc: 0.9616 - prc: 0.7554 - val_loss: 0.0326 - val_accuracy: 0.9991 - val_precision: 0.6190 - val_recall: 0.8254 - val_auc: 0.9701 - val_prc: 0.7206\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.9325e-04 - accuracy: 0.9991 - precision: 0.7117 - recall: 0.8483 - auc: 0.9709 - prc: 0.7610 - val_loss: 0.0236 - val_accuracy: 0.9990 - val_precision: 0.6145 - val_recall: 0.8095 - val_auc: 0.9733 - val_prc: 0.7211\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.5492e-04 - accuracy: 0.9990 - precision: 0.6782 - recall: 0.8483 - auc: 0.9739 - prc: 0.7622 - val_loss: 0.0234 - val_accuracy: 0.9989 - val_precision: 0.5714 - val_recall: 0.8254 - val_auc: 0.9703 - val_prc: 0.7227\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.2172e-04 - accuracy: 0.9990 - precision: 0.6675 - recall: 0.8514 - auc: 0.9775 - prc: 0.7626 - val_loss: 0.0250 - val_accuracy: 0.9987 - val_precision: 0.5098 - val_recall: 0.8254 - val_auc: 0.9712 - val_prc: 0.7116\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.9451e-04 - accuracy: 0.9989 - precision: 0.6402 - recall: 0.8483 - auc: 0.9802 - prc: 0.7680 - val_loss: 0.0223 - val_accuracy: 0.9986 - val_precision: 0.5049 - val_recall: 0.8254 - val_auc: 0.9733 - val_prc: 0.7501\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.7537e-04 - accuracy: 0.9987 - precision: 0.6053 - recall: 0.8545 - auc: 0.9815 - prc: 0.7675 - val_loss: 0.0238 - val_accuracy: 0.9985 - val_precision: 0.4685 - val_recall: 0.8254 - val_auc: 0.9734 - val_prc: 0.7385\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.6026e-04 - accuracy: 0.9986 - precision: 0.5679 - recall: 0.8545 - auc: 0.9826 - prc: 0.7630 - val_loss: 0.0214 - val_accuracy: 0.9984 - val_precision: 0.4643 - val_recall: 0.8254 - val_auc: 0.9748 - val_prc: 0.7366\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.4675e-04 - accuracy: 0.9986 - precision: 0.5780 - recall: 0.8607 - auc: 0.9835 - prc: 0.7660 - val_loss: 0.0204 - val_accuracy: 0.9984 - val_precision: 0.4643 - val_recall: 0.8254 - val_auc: 0.9750 - val_prc: 0.7644\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 8.3557e-04 - accuracy: 0.9985 - precision: 0.5442 - recall: 0.8576 - auc: 0.9855 - prc: 0.7599 - val_loss: 0.0201 - val_accuracy: 0.9984 - val_precision: 0.4643 - val_recall: 0.8254 - val_auc: 0.9757 - val_prc: 0.7541\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 8.2615e-04 - accuracy: 0.9985 - precision: 0.5460 - recall: 0.8638 - auc: 0.9856 - prc: 0.7618 - val_loss: 0.0187 - val_accuracy: 0.9985 - val_precision: 0.4727 - val_recall: 0.8254 - val_auc: 0.9737 - val_prc: 0.7646\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 8.1946e-04 - accuracy: 0.9986 - precision: 0.5639 - recall: 0.8607 - auc: 0.9859 - prc: 0.7615 - val_loss: 0.0249 - val_accuracy: 0.9977 - val_precision: 0.3581 - val_recall: 0.8413 - val_auc: 0.9745 - val_prc: 0.7396\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.0753e-04 - accuracy: 0.9982 - precision: 0.4991 - recall: 0.8638 - auc: 0.9859 - prc: 0.7630 - val_loss: 0.0194 - val_accuracy: 0.9983 - val_precision: 0.4333 - val_recall: 0.8254 - val_auc: 0.9739 - val_prc: 0.7415\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.0286e-04 - accuracy: 0.9982 - precision: 0.4973 - recall: 0.8669 - auc: 0.9865 - prc: 0.7626 - val_loss: 0.0184 - val_accuracy: 0.9984 - val_precision: 0.4561 - val_recall: 0.8254 - val_auc: 0.9743 - val_prc: 0.7425\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.9712e-04 - accuracy: 0.9984 - precision: 0.5341 - recall: 0.8731 - auc: 0.9847 - prc: 0.7713 - val_loss: 0.0213 - val_accuracy: 0.9980 - val_precision: 0.3985 - val_recall: 0.8413 - val_auc: 0.9735 - val_prc: 0.7412\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.9851e-04 - accuracy: 0.9982 - precision: 0.5018 - recall: 0.8669 - auc: 0.9857 - prc: 0.7658 - val_loss: 0.0205 - val_accuracy: 0.9981 - val_precision: 0.4141 - val_recall: 0.8413 - val_auc: 0.9741 - val_prc: 0.7421\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.0285e-04 - accuracy: 0.9981 - precision: 0.4819 - recall: 0.8638 - auc: 0.9855 - prc: 0.7634 - val_loss: 0.0185 - val_accuracy: 0.9983 - val_precision: 0.4380 - val_recall: 0.8413 - val_auc: 0.9750 - val_prc: 0.7429\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.0089e-04 - accuracy: 0.9983 - precision: 0.5193 - recall: 0.8731 - auc: 0.9852 - prc: 0.7689 - val_loss: 0.0205 - val_accuracy: 0.9979 - val_precision: 0.3869 - val_recall: 0.8413 - val_auc: 0.9745 - val_prc: 0.7420\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.8108e-04 - accuracy: 0.9982 - precision: 0.5018 - recall: 0.8731 - auc: 0.9865 - prc: 0.7741 - val_loss: 0.0188 - val_accuracy: 0.9983 - val_precision: 0.4417 - val_recall: 0.8413 - val_auc: 0.9747 - val_prc: 0.7552\n",
            "1419/1419 [==============================] - 2s 2ms/step\n",
            "****************************** w =  0.11999999999999998\n",
            "****************************** w =  2\n",
            "Model: \"sequential_40\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_40 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_40 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_40 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_40 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88/89 [============================>.] - ETA: 0s - loss: 0.0035 - accuracy: 0.9883 - precision: 0.1020 - recall: 0.7481 - auc: 0.9153 - prc: 0.6009"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 15ms/step - loss: 0.0035 - accuracy: 0.9884 - precision: 0.1020 - recall: 0.7461 - auc: 0.9139 - prc: 0.5993 - val_loss: 0.0470 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9624 - val_prc: 0.6829\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0013 - accuracy: 0.9994 - precision: 0.8371 - recall: 0.7957 - auc: 0.9434 - prc: 0.7389 - val_loss: 0.0261 - val_accuracy: 0.9993 - val_precision: 0.7391 - val_recall: 0.8095 - val_auc: 0.9660 - val_prc: 0.7147\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 0.9993 - precision: 0.7847 - recall: 0.8235 - auc: 0.9628 - prc: 0.7546 - val_loss: 0.0194 - val_accuracy: 0.9993 - val_precision: 0.6986 - val_recall: 0.8095 - val_auc: 0.9683 - val_prc: 0.7198\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0010 - accuracy: 0.9992 - precision: 0.7472 - recall: 0.8328 - auc: 0.9664 - prc: 0.7572 - val_loss: 0.0188 - val_accuracy: 0.9992 - val_precision: 0.6800 - val_recall: 0.8095 - val_auc: 0.9675 - val_prc: 0.7098\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.8738e-04 - accuracy: 0.9992 - precision: 0.7292 - recall: 0.8421 - auc: 0.9727 - prc: 0.7595 - val_loss: 0.0214 - val_accuracy: 0.9990 - val_precision: 0.6118 - val_recall: 0.8254 - val_auc: 0.9706 - val_prc: 0.7132\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.5766e-04 - accuracy: 0.9990 - precision: 0.6782 - recall: 0.8483 - auc: 0.9783 - prc: 0.7618 - val_loss: 0.0195 - val_accuracy: 0.9990 - val_precision: 0.6047 - val_recall: 0.8254 - val_auc: 0.9749 - val_prc: 0.7376\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.2220e-04 - accuracy: 0.9990 - precision: 0.6824 - recall: 0.8514 - auc: 0.9818 - prc: 0.7686 - val_loss: 0.0196 - val_accuracy: 0.9989 - val_precision: 0.5778 - val_recall: 0.8254 - val_auc: 0.9738 - val_prc: 0.7391\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.0653e-04 - accuracy: 0.9990 - precision: 0.6659 - recall: 0.8576 - auc: 0.9799 - prc: 0.7660 - val_loss: 0.0186 - val_accuracy: 0.9989 - val_precision: 0.5604 - val_recall: 0.8095 - val_auc: 0.9747 - val_prc: 0.7406\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 8.9273e-04 - accuracy: 0.9988 - precision: 0.6301 - recall: 0.8545 - auc: 0.9843 - prc: 0.7609 - val_loss: 0.0182 - val_accuracy: 0.9988 - val_precision: 0.5426 - val_recall: 0.8095 - val_auc: 0.9750 - val_prc: 0.7397\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 8.8439e-04 - accuracy: 0.9989 - precision: 0.6397 - recall: 0.8576 - auc: 0.9836 - prc: 0.7664 - val_loss: 0.0183 - val_accuracy: 0.9987 - val_precision: 0.5149 - val_recall: 0.8254 - val_auc: 0.9753 - val_prc: 0.7526\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 8.6434e-04 - accuracy: 0.9988 - precision: 0.6192 - recall: 0.8607 - auc: 0.9848 - prc: 0.7671 - val_loss: 0.0176 - val_accuracy: 0.9986 - val_precision: 0.5050 - val_recall: 0.8095 - val_auc: 0.9763 - val_prc: 0.7529\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.6253e-04 - accuracy: 0.9988 - precision: 0.6075 - recall: 0.8576 - auc: 0.9844 - prc: 0.7646 - val_loss: 0.0194 - val_accuracy: 0.9984 - val_precision: 0.4522 - val_recall: 0.8254 - val_auc: 0.9757 - val_prc: 0.7418\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.5170e-04 - accuracy: 0.9987 - precision: 0.5894 - recall: 0.8576 - auc: 0.9847 - prc: 0.7645 - val_loss: 0.0200 - val_accuracy: 0.9982 - val_precision: 0.4298 - val_recall: 0.8254 - val_auc: 0.9758 - val_prc: 0.7411\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.5514e-04 - accuracy: 0.9985 - precision: 0.5547 - recall: 0.8638 - auc: 0.9838 - prc: 0.7611 - val_loss: 0.0180 - val_accuracy: 0.9984 - val_precision: 0.4643 - val_recall: 0.8254 - val_auc: 0.9772 - val_prc: 0.7426\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.4672e-04 - accuracy: 0.9986 - precision: 0.5697 - recall: 0.8607 - auc: 0.9850 - prc: 0.7647 - val_loss: 0.0177 - val_accuracy: 0.9984 - val_precision: 0.4643 - val_recall: 0.8254 - val_auc: 0.9774 - val_prc: 0.7425\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.3856e-04 - accuracy: 0.9985 - precision: 0.5525 - recall: 0.8638 - auc: 0.9846 - prc: 0.7650 - val_loss: 0.0145 - val_accuracy: 0.9986 - val_precision: 0.5000 - val_recall: 0.8095 - val_auc: 0.9763 - val_prc: 0.7527\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.3778e-04 - accuracy: 0.9986 - precision: 0.5741 - recall: 0.8638 - auc: 0.9849 - prc: 0.7615 - val_loss: 0.0142 - val_accuracy: 0.9986 - val_precision: 0.5050 - val_recall: 0.8095 - val_auc: 0.9769 - val_prc: 0.7527\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.3605e-04 - accuracy: 0.9985 - precision: 0.5428 - recall: 0.8638 - auc: 0.9867 - prc: 0.7627 - val_loss: 0.0138 - val_accuracy: 0.9986 - val_precision: 0.5050 - val_recall: 0.8095 - val_auc: 0.9775 - val_prc: 0.7529\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.3724e-04 - accuracy: 0.9985 - precision: 0.5571 - recall: 0.8607 - auc: 0.9847 - prc: 0.7660 - val_loss: 0.0171 - val_accuracy: 0.9983 - val_precision: 0.4444 - val_recall: 0.8254 - val_auc: 0.9750 - val_prc: 0.7418\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.3282e-04 - accuracy: 0.9985 - precision: 0.5442 - recall: 0.8576 - auc: 0.9854 - prc: 0.7673 - val_loss: 0.0183 - val_accuracy: 0.9982 - val_precision: 0.4194 - val_recall: 0.8254 - val_auc: 0.9754 - val_prc: 0.7418\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.12999999999999998\n",
            "****************************** w =  2\n",
            "Model: \"sequential_41\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_41 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_41 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_41 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_41 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87/89 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9886 - precision: 0.1022 - recall: 0.7304 - auc: 0.9116 - prc: 0.5701"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 15ms/step - loss: 0.0036 - accuracy: 0.9887 - precision: 0.1032 - recall: 0.7306 - auc: 0.9109 - prc: 0.5720 - val_loss: 0.0394 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9563 - val_prc: 0.6781\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0013 - accuracy: 0.9994 - precision: 0.8399 - recall: 0.7957 - auc: 0.9425 - prc: 0.7378 - val_loss: 0.0208 - val_accuracy: 0.9993 - val_precision: 0.7286 - val_recall: 0.8095 - val_auc: 0.9643 - val_prc: 0.7057\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 0.9993 - precision: 0.7755 - recall: 0.8235 - auc: 0.9584 - prc: 0.7470 - val_loss: 0.0204 - val_accuracy: 0.9992 - val_precision: 0.6711 - val_recall: 0.8095 - val_auc: 0.9683 - val_prc: 0.7199\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 0.9992 - precision: 0.7418 - recall: 0.8359 - auc: 0.9660 - prc: 0.7571 - val_loss: 0.0199 - val_accuracy: 0.9991 - val_precision: 0.6375 - val_recall: 0.8095 - val_auc: 0.9692 - val_prc: 0.7077\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0010 - accuracy: 0.9991 - precision: 0.7193 - recall: 0.8328 - auc: 0.9717 - prc: 0.7585 - val_loss: 0.0186 - val_accuracy: 0.9990 - val_precision: 0.6000 - val_recall: 0.8095 - val_auc: 0.9720 - val_prc: 0.7068\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.7742e-04 - accuracy: 0.9990 - precision: 0.6869 - recall: 0.8421 - auc: 0.9770 - prc: 0.7614 - val_loss: 0.0150 - val_accuracy: 0.9991 - val_precision: 0.6296 - val_recall: 0.8095 - val_auc: 0.9722 - val_prc: 0.7464\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.4119e-04 - accuracy: 0.9990 - precision: 0.6894 - recall: 0.8452 - auc: 0.9793 - prc: 0.7666 - val_loss: 0.0163 - val_accuracy: 0.9989 - val_precision: 0.5730 - val_recall: 0.8095 - val_auc: 0.9771 - val_prc: 0.7455\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.2306e-04 - accuracy: 0.9990 - precision: 0.6774 - recall: 0.8452 - auc: 0.9835 - prc: 0.7606 - val_loss: 0.0175 - val_accuracy: 0.9986 - val_precision: 0.5000 - val_recall: 0.8095 - val_auc: 0.9770 - val_prc: 0.7470\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.1521e-04 - accuracy: 0.9989 - precision: 0.6447 - recall: 0.8483 - auc: 0.9821 - prc: 0.7671 - val_loss: 0.0144 - val_accuracy: 0.9988 - val_precision: 0.5484 - val_recall: 0.8095 - val_auc: 0.9752 - val_prc: 0.7615\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 9.0207e-04 - accuracy: 0.9989 - precision: 0.6516 - recall: 0.8452 - auc: 0.9830 - prc: 0.7629 - val_loss: 0.0167 - val_accuracy: 0.9985 - val_precision: 0.4811 - val_recall: 0.8095 - val_auc: 0.9745 - val_prc: 0.7474\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 8.8824e-04 - accuracy: 0.9987 - precision: 0.6040 - recall: 0.8452 - auc: 0.9851 - prc: 0.7601 - val_loss: 0.0135 - val_accuracy: 0.9987 - val_precision: 0.5312 - val_recall: 0.8095 - val_auc: 0.9774 - val_prc: 0.7623\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 8.9038e-04 - accuracy: 0.9988 - precision: 0.6102 - recall: 0.8483 - auc: 0.9834 - prc: 0.7679 - val_loss: 0.0164 - val_accuracy: 0.9985 - val_precision: 0.4679 - val_recall: 0.8095 - val_auc: 0.9755 - val_prc: 0.7391\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 8.7364e-04 - accuracy: 0.9988 - precision: 0.6100 - recall: 0.8669 - auc: 0.9840 - prc: 0.7660 - val_loss: 0.0137 - val_accuracy: 0.9987 - val_precision: 0.5204 - val_recall: 0.8095 - val_auc: 0.9778 - val_prc: 0.7513\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.8036e-04 - accuracy: 0.9988 - precision: 0.6161 - recall: 0.8545 - auc: 0.9824 - prc: 0.7667 - val_loss: 0.0154 - val_accuracy: 0.9985 - val_precision: 0.4679 - val_recall: 0.8095 - val_auc: 0.9766 - val_prc: 0.7382\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 8.7890e-04 - accuracy: 0.9986 - precision: 0.5738 - recall: 0.8545 - auc: 0.9841 - prc: 0.7625 - val_loss: 0.0142 - val_accuracy: 0.9986 - val_precision: 0.4951 - val_recall: 0.8095 - val_auc: 0.9777 - val_prc: 0.7656\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.6859e-04 - accuracy: 0.9986 - precision: 0.5765 - recall: 0.8514 - auc: 0.9844 - prc: 0.7607 - val_loss: 0.0145 - val_accuracy: 0.9985 - val_precision: 0.4722 - val_recall: 0.8095 - val_auc: 0.9766 - val_prc: 0.7510\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 8.5284e-04 - accuracy: 0.9987 - precision: 0.5853 - recall: 0.8607 - auc: 0.9861 - prc: 0.7644 - val_loss: 0.0144 - val_accuracy: 0.9985 - val_precision: 0.4722 - val_recall: 0.8095 - val_auc: 0.9770 - val_prc: 0.7397\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.5587e-04 - accuracy: 0.9987 - precision: 0.5853 - recall: 0.8607 - auc: 0.9848 - prc: 0.7682 - val_loss: 0.0133 - val_accuracy: 0.9986 - val_precision: 0.4904 - val_recall: 0.8095 - val_auc: 0.9783 - val_prc: 0.7520\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.5798e-04 - accuracy: 0.9987 - precision: 0.5957 - recall: 0.8576 - auc: 0.9857 - prc: 0.7699 - val_loss: 0.0140 - val_accuracy: 0.9985 - val_precision: 0.4722 - val_recall: 0.8095 - val_auc: 0.9777 - val_prc: 0.7533\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.5952e-04 - accuracy: 0.9986 - precision: 0.5714 - recall: 0.8545 - auc: 0.9858 - prc: 0.7617 - val_loss: 0.0166 - val_accuracy: 0.9983 - val_precision: 0.4407 - val_recall: 0.8254 - val_auc: 0.9771 - val_prc: 0.7412\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.13999999999999999\n",
            "****************************** w =  2\n",
            "Model: \"sequential_42\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_42 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_42 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_42 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_42 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89/89 [==============================] - ETA: 0s - loss: 0.0039 - accuracy: 0.9948 - precision: 0.1982 - recall: 0.6839 - auc: 0.9025 - prc: 0.5678"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 15ms/step - loss: 0.0039 - accuracy: 0.9948 - precision: 0.1982 - recall: 0.6839 - auc: 0.9025 - prc: 0.5678 - val_loss: 0.0299 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9556 - val_prc: 0.6809\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0014 - accuracy: 0.9994 - precision: 0.8515 - recall: 0.7988 - auc: 0.9454 - prc: 0.7298 - val_loss: 0.0176 - val_accuracy: 0.9994 - val_precision: 0.7812 - val_recall: 0.7937 - val_auc: 0.9647 - val_prc: 0.7068\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0012 - accuracy: 0.9993 - precision: 0.7821 - recall: 0.8111 - auc: 0.9595 - prc: 0.7535 - val_loss: 0.0141 - val_accuracy: 0.9993 - val_precision: 0.6986 - val_recall: 0.8095 - val_auc: 0.9656 - val_prc: 0.7179\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 0.9992 - precision: 0.7622 - recall: 0.8235 - auc: 0.9677 - prc: 0.7612 - val_loss: 0.0148 - val_accuracy: 0.9992 - val_precision: 0.6800 - val_recall: 0.8095 - val_auc: 0.9668 - val_prc: 0.7204\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0010 - accuracy: 0.9992 - precision: 0.7330 - recall: 0.8328 - auc: 0.9741 - prc: 0.7620 - val_loss: 0.0133 - val_accuracy: 0.9991 - val_precision: 0.6538 - val_recall: 0.8095 - val_auc: 0.9747 - val_prc: 0.7195\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0010 - accuracy: 0.9992 - precision: 0.7290 - recall: 0.8328 - auc: 0.9762 - prc: 0.7633 - val_loss: 0.0151 - val_accuracy: 0.9990 - val_precision: 0.5930 - val_recall: 0.8095 - val_auc: 0.9742 - val_prc: 0.7192\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.7817e-04 - accuracy: 0.9991 - precision: 0.7005 - recall: 0.8328 - auc: 0.9785 - prc: 0.7681 - val_loss: 0.0132 - val_accuracy: 0.9990 - val_precision: 0.6000 - val_recall: 0.8095 - val_auc: 0.9766 - val_prc: 0.7468\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.5760e-04 - accuracy: 0.9990 - precision: 0.6835 - recall: 0.8359 - auc: 0.9807 - prc: 0.7664 - val_loss: 0.0132 - val_accuracy: 0.9989 - val_precision: 0.5795 - val_recall: 0.8095 - val_auc: 0.9771 - val_prc: 0.7463\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 9.3925e-04 - accuracy: 0.9990 - precision: 0.6658 - recall: 0.8390 - auc: 0.9831 - prc: 0.7636 - val_loss: 0.0149 - val_accuracy: 0.9987 - val_precision: 0.5204 - val_recall: 0.8095 - val_auc: 0.9757 - val_prc: 0.7347\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 9.2701e-04 - accuracy: 0.9989 - precision: 0.6478 - recall: 0.8483 - auc: 0.9834 - prc: 0.7656 - val_loss: 0.0135 - val_accuracy: 0.9988 - val_precision: 0.5368 - val_recall: 0.8095 - val_auc: 0.9779 - val_prc: 0.7612\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.0858e-04 - accuracy: 0.9989 - precision: 0.6493 - recall: 0.8483 - auc: 0.9846 - prc: 0.7622 - val_loss: 0.0127 - val_accuracy: 0.9988 - val_precision: 0.5368 - val_recall: 0.8095 - val_auc: 0.9778 - val_prc: 0.7501\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.9992e-04 - accuracy: 0.9989 - precision: 0.6462 - recall: 0.8483 - auc: 0.9825 - prc: 0.7715 - val_loss: 0.0143 - val_accuracy: 0.9986 - val_precision: 0.5000 - val_recall: 0.8095 - val_auc: 0.9778 - val_prc: 0.7500\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.0963e-04 - accuracy: 0.9989 - precision: 0.6364 - recall: 0.8452 - auc: 0.9829 - prc: 0.7658 - val_loss: 0.0131 - val_accuracy: 0.9987 - val_precision: 0.5100 - val_recall: 0.8095 - val_auc: 0.9778 - val_prc: 0.7645\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.9961e-04 - accuracy: 0.9989 - precision: 0.6359 - recall: 0.8545 - auc: 0.9838 - prc: 0.7657 - val_loss: 0.0144 - val_accuracy: 0.9986 - val_precision: 0.4904 - val_recall: 0.8095 - val_auc: 0.9782 - val_prc: 0.7383\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.8363e-04 - accuracy: 0.9989 - precision: 0.6395 - recall: 0.8514 - auc: 0.9848 - prc: 0.7718 - val_loss: 0.0137 - val_accuracy: 0.9986 - val_precision: 0.4951 - val_recall: 0.8095 - val_auc: 0.9777 - val_prc: 0.7517\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.7912e-04 - accuracy: 0.9988 - precision: 0.6185 - recall: 0.8483 - auc: 0.9848 - prc: 0.7716 - val_loss: 0.0151 - val_accuracy: 0.9985 - val_precision: 0.4722 - val_recall: 0.8095 - val_auc: 0.9780 - val_prc: 0.7390\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 8.7864e-04 - accuracy: 0.9988 - precision: 0.6213 - recall: 0.8483 - auc: 0.9864 - prc: 0.7651 - val_loss: 0.0146 - val_accuracy: 0.9985 - val_precision: 0.4722 - val_recall: 0.8095 - val_auc: 0.9778 - val_prc: 0.7395\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 8.7052e-04 - accuracy: 0.9988 - precision: 0.6264 - recall: 0.8514 - auc: 0.9850 - prc: 0.7653 - val_loss: 0.0166 - val_accuracy: 0.9982 - val_precision: 0.4298 - val_recall: 0.8254 - val_auc: 0.9772 - val_prc: 0.7506\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.7948e-04 - accuracy: 0.9987 - precision: 0.6026 - recall: 0.8545 - auc: 0.9859 - prc: 0.7650 - val_loss: 0.0155 - val_accuracy: 0.9983 - val_precision: 0.4474 - val_recall: 0.8095 - val_auc: 0.9782 - val_prc: 0.7396\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.7748e-04 - accuracy: 0.9987 - precision: 0.5889 - recall: 0.8514 - auc: 0.9840 - prc: 0.7645 - val_loss: 0.0122 - val_accuracy: 0.9987 - val_precision: 0.5100 - val_recall: 0.8095 - val_auc: 0.9798 - val_prc: 0.7542\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.15\n",
            "****************************** w =  2\n",
            "Model: \"sequential_43\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_43 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_43 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_43 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_43 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85/89 [===========================>..] - ETA: 0s - loss: 0.0043 - accuracy: 0.9945 - precision: 0.1827 - recall: 0.6496 - auc: 0.8833 - prc: 0.5466"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 15ms/step - loss: 0.0041 - accuracy: 0.9947 - precision: 0.1910 - recall: 0.6606 - auc: 0.8877 - prc: 0.5614 - val_loss: 0.0290 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9561 - val_prc: 0.6866\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0014 - accuracy: 0.9994 - precision: 0.8552 - recall: 0.7864 - auc: 0.9357 - prc: 0.7326 - val_loss: 0.0173 - val_accuracy: 0.9994 - val_precision: 0.7812 - val_recall: 0.7937 - val_auc: 0.9624 - val_prc: 0.7126\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0012 - accuracy: 0.9993 - precision: 0.7868 - recall: 0.8111 - auc: 0.9560 - prc: 0.7516 - val_loss: 0.0151 - val_accuracy: 0.9993 - val_precision: 0.6986 - val_recall: 0.8095 - val_auc: 0.9698 - val_prc: 0.7201\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 0.9993 - precision: 0.7807 - recall: 0.8266 - auc: 0.9647 - prc: 0.7551 - val_loss: 0.0143 - val_accuracy: 0.9993 - val_precision: 0.6986 - val_recall: 0.8095 - val_auc: 0.9727 - val_prc: 0.7208\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 0.9992 - precision: 0.7437 - recall: 0.8266 - auc: 0.9721 - prc: 0.7637 - val_loss: 0.0140 - val_accuracy: 0.9992 - val_precision: 0.6623 - val_recall: 0.8095 - val_auc: 0.9697 - val_prc: 0.7461\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0010 - accuracy: 0.9992 - precision: 0.7514 - recall: 0.8328 - auc: 0.9754 - prc: 0.7649 - val_loss: 0.0160 - val_accuracy: 0.9990 - val_precision: 0.5977 - val_recall: 0.8254 - val_auc: 0.9762 - val_prc: 0.7361\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.9993e-04 - accuracy: 0.9991 - precision: 0.7076 - recall: 0.8390 - auc: 0.9787 - prc: 0.7636 - val_loss: 0.0140 - val_accuracy: 0.9990 - val_precision: 0.6145 - val_recall: 0.8095 - val_auc: 0.9747 - val_prc: 0.7491\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.7446e-04 - accuracy: 0.9991 - precision: 0.7039 - recall: 0.8390 - auc: 0.9839 - prc: 0.7714 - val_loss: 0.0105 - val_accuracy: 0.9991 - val_precision: 0.6538 - val_recall: 0.8095 - val_auc: 0.9768 - val_prc: 0.7617\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.6302e-04 - accuracy: 0.9991 - precision: 0.7094 - recall: 0.8390 - auc: 0.9828 - prc: 0.7667 - val_loss: 0.0117 - val_accuracy: 0.9990 - val_precision: 0.6071 - val_recall: 0.8095 - val_auc: 0.9784 - val_prc: 0.7632\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 9.4613e-04 - accuracy: 0.9990 - precision: 0.6878 - recall: 0.8390 - auc: 0.9828 - prc: 0.7656 - val_loss: 0.0127 - val_accuracy: 0.9988 - val_precision: 0.5543 - val_recall: 0.8095 - val_auc: 0.9778 - val_prc: 0.7663\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 9.3868e-04 - accuracy: 0.9989 - precision: 0.6562 - recall: 0.8390 - auc: 0.9839 - prc: 0.7682 - val_loss: 0.0118 - val_accuracy: 0.9989 - val_precision: 0.5862 - val_recall: 0.8095 - val_auc: 0.9786 - val_prc: 0.7522\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.2763e-04 - accuracy: 0.9990 - precision: 0.6667 - recall: 0.8421 - auc: 0.9837 - prc: 0.7674 - val_loss: 0.0121 - val_accuracy: 0.9988 - val_precision: 0.5484 - val_recall: 0.8095 - val_auc: 0.9784 - val_prc: 0.7527\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.1909e-04 - accuracy: 0.9989 - precision: 0.6439 - recall: 0.8452 - auc: 0.9833 - prc: 0.7697 - val_loss: 0.0112 - val_accuracy: 0.9989 - val_precision: 0.5604 - val_recall: 0.8095 - val_auc: 0.9799 - val_prc: 0.7537\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.2014e-04 - accuracy: 0.9990 - precision: 0.6732 - recall: 0.8483 - auc: 0.9822 - prc: 0.7663 - val_loss: 0.0122 - val_accuracy: 0.9987 - val_precision: 0.5258 - val_recall: 0.8095 - val_auc: 0.9785 - val_prc: 0.7659\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.0815e-04 - accuracy: 0.9990 - precision: 0.6626 - recall: 0.8452 - auc: 0.9847 - prc: 0.7721 - val_loss: 0.0137 - val_accuracy: 0.9985 - val_precision: 0.4857 - val_recall: 0.8095 - val_auc: 0.9780 - val_prc: 0.7529\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.1566e-04 - accuracy: 0.9988 - precision: 0.6244 - recall: 0.8390 - auc: 0.9846 - prc: 0.7666 - val_loss: 0.0123 - val_accuracy: 0.9986 - val_precision: 0.5050 - val_recall: 0.8095 - val_auc: 0.9796 - val_prc: 0.7406\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.0341e-04 - accuracy: 0.9989 - precision: 0.6395 - recall: 0.8514 - auc: 0.9846 - prc: 0.7654 - val_loss: 0.0119 - val_accuracy: 0.9987 - val_precision: 0.5100 - val_recall: 0.8095 - val_auc: 0.9797 - val_prc: 0.7673\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.8215e-04 - accuracy: 0.9989 - precision: 0.6571 - recall: 0.8483 - auc: 0.9847 - prc: 0.7694 - val_loss: 0.0127 - val_accuracy: 0.9985 - val_precision: 0.4857 - val_recall: 0.8095 - val_auc: 0.9792 - val_prc: 0.7665\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.9104e-04 - accuracy: 0.9989 - precision: 0.6397 - recall: 0.8576 - auc: 0.9835 - prc: 0.7715 - val_loss: 0.0115 - val_accuracy: 0.9987 - val_precision: 0.5152 - val_recall: 0.8095 - val_auc: 0.9802 - val_prc: 0.7677\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 8.9094e-04 - accuracy: 0.9989 - precision: 0.6345 - recall: 0.8545 - auc: 0.9854 - prc: 0.7690 - val_loss: 0.0117 - val_accuracy: 0.9987 - val_precision: 0.5100 - val_recall: 0.8095 - val_auc: 0.9803 - val_prc: 0.7673\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.16\n",
            "****************************** w =  2\n",
            "Model: \"sequential_44\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_44 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_44 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_44 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_44 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88/89 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9954 - precision: 0.2195 - recall: 0.6580 - auc: 0.9035 - prc: 0.5553"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 22ms/step - loss: 0.0041 - accuracy: 0.9955 - precision: 0.2207 - recall: 0.6580 - auc: 0.9036 - prc: 0.5525 - val_loss: 0.0239 - val_accuracy: 0.9994 - val_precision: 0.7903 - val_recall: 0.7778 - val_auc: 0.9575 - val_prc: 0.6969\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0014 - accuracy: 0.9994 - precision: 0.8523 - recall: 0.7864 - auc: 0.9417 - prc: 0.7371 - val_loss: 0.0148 - val_accuracy: 0.9994 - val_precision: 0.7692 - val_recall: 0.7937 - val_auc: 0.9623 - val_prc: 0.7115\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0012 - accuracy: 0.9993 - precision: 0.7874 - recall: 0.8142 - auc: 0.9540 - prc: 0.7514 - val_loss: 0.0125 - val_accuracy: 0.9993 - val_precision: 0.6986 - val_recall: 0.8095 - val_auc: 0.9681 - val_prc: 0.7260\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 0.9993 - precision: 0.7755 - recall: 0.8235 - auc: 0.9633 - prc: 0.7586 - val_loss: 0.0113 - val_accuracy: 0.9993 - val_precision: 0.6986 - val_recall: 0.8095 - val_auc: 0.9740 - val_prc: 0.7413\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 0.9992 - precision: 0.7607 - recall: 0.8266 - auc: 0.9692 - prc: 0.7601 - val_loss: 0.0121 - val_accuracy: 0.9992 - val_precision: 0.6623 - val_recall: 0.8095 - val_auc: 0.9734 - val_prc: 0.7430\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0010 - accuracy: 0.9992 - precision: 0.7424 - recall: 0.8297 - auc: 0.9723 - prc: 0.7647 - val_loss: 0.0098 - val_accuracy: 0.9993 - val_precision: 0.6986 - val_recall: 0.8095 - val_auc: 0.9777 - val_prc: 0.7564\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0010 - accuracy: 0.9992 - precision: 0.7424 - recall: 0.8297 - auc: 0.9812 - prc: 0.7666 - val_loss: 0.0104 - val_accuracy: 0.9991 - val_precision: 0.6456 - val_recall: 0.8095 - val_auc: 0.9790 - val_prc: 0.7573\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 9.9688e-04 - accuracy: 0.9992 - precision: 0.7403 - recall: 0.8297 - auc: 0.9786 - prc: 0.7663 - val_loss: 0.0105 - val_accuracy: 0.9991 - val_precision: 0.6375 - val_recall: 0.8095 - val_auc: 0.9792 - val_prc: 0.7582\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 9.7262e-04 - accuracy: 0.9992 - precision: 0.7305 - recall: 0.8390 - auc: 0.9817 - prc: 0.7643 - val_loss: 0.0106 - val_accuracy: 0.9991 - val_precision: 0.6296 - val_recall: 0.8095 - val_auc: 0.9791 - val_prc: 0.7592\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 9.5930e-04 - accuracy: 0.9991 - precision: 0.7188 - recall: 0.8390 - auc: 0.9814 - prc: 0.7657 - val_loss: 0.0118 - val_accuracy: 0.9989 - val_precision: 0.5862 - val_recall: 0.8095 - val_auc: 0.9788 - val_prc: 0.7579\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.6107e-04 - accuracy: 0.9991 - precision: 0.6969 - recall: 0.8328 - auc: 0.9831 - prc: 0.7679 - val_loss: 0.0113 - val_accuracy: 0.9990 - val_precision: 0.5930 - val_recall: 0.8095 - val_auc: 0.9792 - val_prc: 0.7582\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.4639e-04 - accuracy: 0.9991 - precision: 0.7003 - recall: 0.8390 - auc: 0.9802 - prc: 0.7645 - val_loss: 0.0112 - val_accuracy: 0.9989 - val_precision: 0.5795 - val_recall: 0.8095 - val_auc: 0.9798 - val_prc: 0.7593\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.3542e-04 - accuracy: 0.9990 - precision: 0.6791 - recall: 0.8452 - auc: 0.9806 - prc: 0.7672 - val_loss: 0.0103 - val_accuracy: 0.9990 - val_precision: 0.6071 - val_recall: 0.8095 - val_auc: 0.9807 - val_prc: 0.7583\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.2160e-04 - accuracy: 0.9990 - precision: 0.6858 - recall: 0.8514 - auc: 0.9852 - prc: 0.7696 - val_loss: 0.0097 - val_accuracy: 0.9990 - val_precision: 0.6071 - val_recall: 0.8095 - val_auc: 0.9801 - val_prc: 0.7583\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.3817e-04 - accuracy: 0.9990 - precision: 0.6750 - recall: 0.8359 - auc: 0.9813 - prc: 0.7690 - val_loss: 0.0098 - val_accuracy: 0.9990 - val_precision: 0.6000 - val_recall: 0.8095 - val_auc: 0.9799 - val_prc: 0.7598\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.2861e-04 - accuracy: 0.9990 - precision: 0.6691 - recall: 0.8390 - auc: 0.9837 - prc: 0.7654 - val_loss: 0.0090 - val_accuracy: 0.9990 - val_precision: 0.6145 - val_recall: 0.8095 - val_auc: 0.9739 - val_prc: 0.7577\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.1884e-04 - accuracy: 0.9990 - precision: 0.6642 - recall: 0.8390 - auc: 0.9860 - prc: 0.7696 - val_loss: 0.0092 - val_accuracy: 0.9990 - val_precision: 0.6071 - val_recall: 0.8095 - val_auc: 0.9808 - val_prc: 0.7588\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.1611e-04 - accuracy: 0.9990 - precision: 0.6783 - recall: 0.8421 - auc: 0.9825 - prc: 0.7739 - val_loss: 0.0099 - val_accuracy: 0.9989 - val_precision: 0.5730 - val_recall: 0.8095 - val_auc: 0.9803 - val_prc: 0.7607\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.1220e-04 - accuracy: 0.9989 - precision: 0.6562 - recall: 0.8452 - auc: 0.9826 - prc: 0.7661 - val_loss: 0.0096 - val_accuracy: 0.9989 - val_precision: 0.5730 - val_recall: 0.8095 - val_auc: 0.9806 - val_prc: 0.7575\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.1633e-04 - accuracy: 0.9990 - precision: 0.6667 - recall: 0.8483 - auc: 0.9832 - prc: 0.7732 - val_loss: 0.0105 - val_accuracy: 0.9988 - val_precision: 0.5543 - val_recall: 0.8095 - val_auc: 0.9803 - val_prc: 0.7489\n",
            "1419/1419 [==============================] - 2s 2ms/step\n",
            "****************************** w =  0.17\n",
            "****************************** w =  2\n",
            "Model: \"sequential_45\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_45 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_45 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_45 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_45 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88/89 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9943 - precision: 0.1726 - recall: 0.6208 - auc: 0.8899 - prc: 0.5191"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 16ms/step - loss: 0.0045 - accuracy: 0.9943 - precision: 0.1732 - recall: 0.6218 - auc: 0.8905 - prc: 0.5203 - val_loss: 0.0212 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9589 - val_prc: 0.6602\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0014 - accuracy: 0.9994 - precision: 0.8519 - recall: 0.7833 - auc: 0.9380 - prc: 0.7296 - val_loss: 0.0129 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9649 - val_prc: 0.7073\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0012 - accuracy: 0.9993 - precision: 0.8106 - recall: 0.8080 - auc: 0.9512 - prc: 0.7511 - val_loss: 0.0117 - val_accuracy: 0.9993 - val_precision: 0.6986 - val_recall: 0.8095 - val_auc: 0.9662 - val_prc: 0.7155\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0012 - accuracy: 0.9993 - precision: 0.7824 - recall: 0.8235 - auc: 0.9669 - prc: 0.7559 - val_loss: 0.0137 - val_accuracy: 0.9991 - val_precision: 0.6456 - val_recall: 0.8095 - val_auc: 0.9712 - val_prc: 0.7081\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 0.9992 - precision: 0.7681 - recall: 0.8204 - auc: 0.9718 - prc: 0.7651 - val_loss: 0.0124 - val_accuracy: 0.9991 - val_precision: 0.6375 - val_recall: 0.8095 - val_auc: 0.9744 - val_prc: 0.7199\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 0.9992 - precision: 0.7542 - recall: 0.8266 - auc: 0.9725 - prc: 0.7639 - val_loss: 0.0112 - val_accuracy: 0.9991 - val_precision: 0.6456 - val_recall: 0.8095 - val_auc: 0.9771 - val_prc: 0.7419\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0010 - accuracy: 0.9992 - precision: 0.7410 - recall: 0.8328 - auc: 0.9766 - prc: 0.7639 - val_loss: 0.0102 - val_accuracy: 0.9991 - val_precision: 0.6538 - val_recall: 0.8095 - val_auc: 0.9801 - val_prc: 0.7449\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0010 - accuracy: 0.9992 - precision: 0.7290 - recall: 0.8328 - auc: 0.9783 - prc: 0.7664 - val_loss: 0.0096 - val_accuracy: 0.9991 - val_precision: 0.6456 - val_recall: 0.8095 - val_auc: 0.9811 - val_prc: 0.7448\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0010 - accuracy: 0.9992 - precision: 0.7364 - recall: 0.8390 - auc: 0.9778 - prc: 0.7674 - val_loss: 0.0114 - val_accuracy: 0.9989 - val_precision: 0.5862 - val_recall: 0.8095 - val_auc: 0.9798 - val_prc: 0.7440\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.8407e-04 - accuracy: 0.9991 - precision: 0.7143 - recall: 0.8359 - auc: 0.9837 - prc: 0.7683 - val_loss: 0.0098 - val_accuracy: 0.9991 - val_precision: 0.6220 - val_recall: 0.8095 - val_auc: 0.9811 - val_prc: 0.7601\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 9.7788e-04 - accuracy: 0.9991 - precision: 0.7098 - recall: 0.8328 - auc: 0.9830 - prc: 0.7653 - val_loss: 0.0105 - val_accuracy: 0.9989 - val_precision: 0.5862 - val_recall: 0.8095 - val_auc: 0.9808 - val_prc: 0.7594\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 9.6598e-04 - accuracy: 0.9991 - precision: 0.7054 - recall: 0.8452 - auc: 0.9811 - prc: 0.7667 - val_loss: 0.0089 - val_accuracy: 0.9991 - val_precision: 0.6296 - val_recall: 0.8095 - val_auc: 0.9731 - val_prc: 0.7609\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.5871e-04 - accuracy: 0.9991 - precision: 0.7050 - recall: 0.8359 - auc: 0.9806 - prc: 0.7692 - val_loss: 0.0107 - val_accuracy: 0.9989 - val_precision: 0.5730 - val_recall: 0.8095 - val_auc: 0.9805 - val_prc: 0.7609\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.4690e-04 - accuracy: 0.9990 - precision: 0.6904 - recall: 0.8421 - auc: 0.9864 - prc: 0.7721 - val_loss: 0.0090 - val_accuracy: 0.9991 - val_precision: 0.6220 - val_recall: 0.8095 - val_auc: 0.9739 - val_prc: 0.7613\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.4874e-04 - accuracy: 0.9990 - precision: 0.6896 - recall: 0.8390 - auc: 0.9822 - prc: 0.7726 - val_loss: 0.0100 - val_accuracy: 0.9990 - val_precision: 0.6000 - val_recall: 0.8095 - val_auc: 0.9801 - val_prc: 0.7617\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.4730e-04 - accuracy: 0.9991 - precision: 0.7087 - recall: 0.8359 - auc: 0.9844 - prc: 0.7649 - val_loss: 0.0097 - val_accuracy: 0.9990 - val_precision: 0.6000 - val_recall: 0.8095 - val_auc: 0.9738 - val_prc: 0.7615\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.4121e-04 - accuracy: 0.9991 - precision: 0.6941 - recall: 0.8359 - auc: 0.9835 - prc: 0.7686 - val_loss: 0.0093 - val_accuracy: 0.9990 - val_precision: 0.6071 - val_recall: 0.8095 - val_auc: 0.9736 - val_prc: 0.7610\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.2592e-04 - accuracy: 0.9990 - precision: 0.6826 - recall: 0.8390 - auc: 0.9851 - prc: 0.7713 - val_loss: 0.0091 - val_accuracy: 0.9991 - val_precision: 0.6220 - val_recall: 0.8095 - val_auc: 0.9738 - val_prc: 0.7592\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.2755e-04 - accuracy: 0.9991 - precision: 0.6995 - recall: 0.8359 - auc: 0.9854 - prc: 0.7722 - val_loss: 0.0104 - val_accuracy: 0.9988 - val_precision: 0.5543 - val_recall: 0.8095 - val_auc: 0.9734 - val_prc: 0.7600\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.3483e-04 - accuracy: 0.9990 - precision: 0.6658 - recall: 0.8390 - auc: 0.9837 - prc: 0.7742 - val_loss: 0.0084 - val_accuracy: 0.9991 - val_precision: 0.6375 - val_recall: 0.8095 - val_auc: 0.9749 - val_prc: 0.7608\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.18000000000000002\n",
            "****************************** w =  2\n",
            "Model: \"sequential_46\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_46 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_46 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_46 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_46 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87/89 [============================>.] - ETA: 0s - loss: 0.0048 - accuracy: 0.9964 - precision: 0.2521 - recall: 0.5632 - auc: 0.8740 - prc: 0.5000"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 15ms/step - loss: 0.0048 - accuracy: 0.9965 - precision: 0.2556 - recall: 0.5648 - auc: 0.8725 - prc: 0.5029 - val_loss: 0.0198 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9584 - val_prc: 0.6604\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0014 - accuracy: 0.9994 - precision: 0.8591 - recall: 0.7926 - auc: 0.9462 - prc: 0.7311 - val_loss: 0.0115 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9627 - val_prc: 0.7088\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0012 - accuracy: 0.9993 - precision: 0.8075 - recall: 0.8050 - auc: 0.9567 - prc: 0.7479 - val_loss: 0.0101 - val_accuracy: 0.9993 - val_precision: 0.7391 - val_recall: 0.8095 - val_auc: 0.9688 - val_prc: 0.7159\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0012 - accuracy: 0.9993 - precision: 0.7817 - recall: 0.8204 - auc: 0.9665 - prc: 0.7590 - val_loss: 0.0091 - val_accuracy: 0.9993 - val_precision: 0.6986 - val_recall: 0.8095 - val_auc: 0.9733 - val_prc: 0.7176\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 0.9993 - precision: 0.7771 - recall: 0.8204 - auc: 0.9716 - prc: 0.7623 - val_loss: 0.0083 - val_accuracy: 0.9993 - val_precision: 0.6986 - val_recall: 0.8095 - val_auc: 0.9672 - val_prc: 0.7444\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 0.9993 - precision: 0.7768 - recall: 0.8297 - auc: 0.9737 - prc: 0.7652 - val_loss: 0.0106 - val_accuracy: 0.9992 - val_precision: 0.6623 - val_recall: 0.8095 - val_auc: 0.9782 - val_prc: 0.7452\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 0.9992 - precision: 0.7521 - recall: 0.8266 - auc: 0.9754 - prc: 0.7620 - val_loss: 0.0098 - val_accuracy: 0.9992 - val_precision: 0.6623 - val_recall: 0.8095 - val_auc: 0.9790 - val_prc: 0.7440\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0010 - accuracy: 0.9992 - precision: 0.7507 - recall: 0.8297 - auc: 0.9787 - prc: 0.7669 - val_loss: 0.0102 - val_accuracy: 0.9991 - val_precision: 0.6456 - val_recall: 0.8095 - val_auc: 0.9786 - val_prc: 0.7447\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0010 - accuracy: 0.9992 - precision: 0.7424 - recall: 0.8297 - auc: 0.9780 - prc: 0.7659 - val_loss: 0.0088 - val_accuracy: 0.9992 - val_precision: 0.6623 - val_recall: 0.8095 - val_auc: 0.9733 - val_prc: 0.7605\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 0.0010 - accuracy: 0.9992 - precision: 0.7350 - recall: 0.8328 - auc: 0.9775 - prc: 0.7691 - val_loss: 0.0091 - val_accuracy: 0.9991 - val_precision: 0.6538 - val_recall: 0.8095 - val_auc: 0.9746 - val_prc: 0.7591\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 9.7915e-04 - accuracy: 0.9991 - precision: 0.7231 - recall: 0.8328 - auc: 0.9806 - prc: 0.7717 - val_loss: 0.0083 - val_accuracy: 0.9992 - val_precision: 0.6623 - val_recall: 0.8095 - val_auc: 0.9744 - val_prc: 0.7613\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 9.7866e-04 - accuracy: 0.9992 - precision: 0.7344 - recall: 0.8390 - auc: 0.9791 - prc: 0.7692 - val_loss: 0.0086 - val_accuracy: 0.9991 - val_precision: 0.6538 - val_recall: 0.8095 - val_auc: 0.9738 - val_prc: 0.7594\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.7022e-04 - accuracy: 0.9992 - precision: 0.7370 - recall: 0.8328 - auc: 0.9804 - prc: 0.7712 - val_loss: 0.0103 - val_accuracy: 0.9989 - val_precision: 0.5862 - val_recall: 0.8095 - val_auc: 0.9807 - val_prc: 0.7610\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.6727e-04 - accuracy: 0.9991 - precision: 0.7057 - recall: 0.8390 - auc: 0.9834 - prc: 0.7699 - val_loss: 0.0081 - val_accuracy: 0.9991 - val_precision: 0.6538 - val_recall: 0.8095 - val_auc: 0.9748 - val_prc: 0.7607\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.5589e-04 - accuracy: 0.9992 - precision: 0.7330 - recall: 0.8328 - auc: 0.9811 - prc: 0.7714 - val_loss: 0.0093 - val_accuracy: 0.9990 - val_precision: 0.6071 - val_recall: 0.8095 - val_auc: 0.9738 - val_prc: 0.7612\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.4816e-04 - accuracy: 0.9990 - precision: 0.6878 - recall: 0.8390 - auc: 0.9837 - prc: 0.7760 - val_loss: 0.0076 - val_accuracy: 0.9992 - val_precision: 0.6623 - val_recall: 0.8095 - val_auc: 0.9746 - val_prc: 0.7608\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.5961e-04 - accuracy: 0.9991 - precision: 0.7031 - recall: 0.8359 - auc: 0.9812 - prc: 0.7728 - val_loss: 0.0083 - val_accuracy: 0.9991 - val_precision: 0.6220 - val_recall: 0.8095 - val_auc: 0.9748 - val_prc: 0.7596\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.4814e-04 - accuracy: 0.9991 - precision: 0.7031 - recall: 0.8359 - auc: 0.9845 - prc: 0.7739 - val_loss: 0.0093 - val_accuracy: 0.9990 - val_precision: 0.5930 - val_recall: 0.8095 - val_auc: 0.9741 - val_prc: 0.7608\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.4290e-04 - accuracy: 0.9991 - precision: 0.7091 - recall: 0.8452 - auc: 0.9827 - prc: 0.7739 - val_loss: 0.0077 - val_accuracy: 0.9991 - val_precision: 0.6456 - val_recall: 0.8095 - val_auc: 0.9746 - val_prc: 0.7615\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.2815e-04 - accuracy: 0.9991 - precision: 0.7044 - recall: 0.8483 - auc: 0.9852 - prc: 0.7728 - val_loss: 0.0070 - val_accuracy: 0.9992 - val_precision: 0.6800 - val_recall: 0.8095 - val_auc: 0.9755 - val_prc: 0.7608\n",
            "1419/1419 [==============================] - 2s 2ms/step\n",
            "****************************** w =  0.19000000000000003\n",
            "****************************** w =  2\n",
            "Model: \"sequential_47\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_47 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_47 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_47 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_47 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86/89 [===========================>..] - ETA: 0s - loss: 0.0051 - accuracy: 0.9931 - precision: 0.1362 - recall: 0.5764 - auc: 0.8653 - prc: 0.4902"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 15ms/step - loss: 0.0050 - accuracy: 0.9933 - precision: 0.1426 - recall: 0.5881 - auc: 0.8699 - prc: 0.5011 - val_loss: 0.0154 - val_accuracy: 0.9994 - val_precision: 0.8103 - val_recall: 0.7460 - val_auc: 0.9414 - val_prc: 0.6527\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 0.9994 - precision: 0.8576 - recall: 0.7833 - auc: 0.9328 - prc: 0.7250 - val_loss: 0.0145 - val_accuracy: 0.9994 - val_precision: 0.7812 - val_recall: 0.7937 - val_auc: 0.9682 - val_prc: 0.7160\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0013 - accuracy: 0.9993 - precision: 0.8131 - recall: 0.8080 - auc: 0.9535 - prc: 0.7515 - val_loss: 0.0098 - val_accuracy: 0.9993 - val_precision: 0.7391 - val_recall: 0.8095 - val_auc: 0.9699 - val_prc: 0.7157\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0012 - accuracy: 0.9993 - precision: 0.7834 - recall: 0.8173 - auc: 0.9627 - prc: 0.7591 - val_loss: 0.0081 - val_accuracy: 0.9993 - val_precision: 0.7083 - val_recall: 0.8095 - val_auc: 0.9674 - val_prc: 0.7169\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 0.9993 - precision: 0.7771 - recall: 0.8204 - auc: 0.9659 - prc: 0.7611 - val_loss: 0.0092 - val_accuracy: 0.9993 - val_precision: 0.6986 - val_recall: 0.8095 - val_auc: 0.9659 - val_prc: 0.7430\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 0.9993 - precision: 0.7778 - recall: 0.8235 - auc: 0.9708 - prc: 0.7643 - val_loss: 0.0074 - val_accuracy: 0.9993 - val_precision: 0.7083 - val_recall: 0.8095 - val_auc: 0.9685 - val_prc: 0.7575\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 0.9993 - precision: 0.7791 - recall: 0.8297 - auc: 0.9769 - prc: 0.7611 - val_loss: 0.0096 - val_accuracy: 0.9992 - val_precision: 0.6623 - val_recall: 0.8095 - val_auc: 0.9726 - val_prc: 0.7442\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 0.0010 - accuracy: 0.9992 - precision: 0.7614 - recall: 0.8297 - auc: 0.9779 - prc: 0.7676 - val_loss: 0.0085 - val_accuracy: 0.9992 - val_precision: 0.6892 - val_recall: 0.8095 - val_auc: 0.9739 - val_prc: 0.7604\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 0.0010 - accuracy: 0.9992 - precision: 0.7528 - recall: 0.8297 - auc: 0.9791 - prc: 0.7679 - val_loss: 0.0080 - val_accuracy: 0.9992 - val_precision: 0.6800 - val_recall: 0.8095 - val_auc: 0.9733 - val_prc: 0.7600\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 0.0010 - accuracy: 0.9992 - precision: 0.7577 - recall: 0.8328 - auc: 0.9779 - prc: 0.7688 - val_loss: 0.0088 - val_accuracy: 0.9991 - val_precision: 0.6456 - val_recall: 0.8095 - val_auc: 0.9722 - val_prc: 0.7595\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.9559e-04 - accuracy: 0.9992 - precision: 0.7370 - recall: 0.8328 - auc: 0.9784 - prc: 0.7699 - val_loss: 0.0074 - val_accuracy: 0.9992 - val_precision: 0.6892 - val_recall: 0.8095 - val_auc: 0.9746 - val_prc: 0.7621\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.8681e-04 - accuracy: 0.9992 - precision: 0.7431 - recall: 0.8328 - auc: 0.9806 - prc: 0.7635 - val_loss: 0.0082 - val_accuracy: 0.9991 - val_precision: 0.6538 - val_recall: 0.8095 - val_auc: 0.9732 - val_prc: 0.7605\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.8264e-04 - accuracy: 0.9992 - precision: 0.7390 - recall: 0.8328 - auc: 0.9826 - prc: 0.7676 - val_loss: 0.0067 - val_accuracy: 0.9993 - val_precision: 0.6986 - val_recall: 0.8095 - val_auc: 0.9750 - val_prc: 0.7606\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.8447e-04 - accuracy: 0.9992 - precision: 0.7507 - recall: 0.8297 - auc: 0.9792 - prc: 0.7706 - val_loss: 0.0090 - val_accuracy: 0.9990 - val_precision: 0.6071 - val_recall: 0.8095 - val_auc: 0.9740 - val_prc: 0.7463\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.6426e-04 - accuracy: 0.9991 - precision: 0.7207 - recall: 0.8390 - auc: 0.9818 - prc: 0.7716 - val_loss: 0.0077 - val_accuracy: 0.9991 - val_precision: 0.6456 - val_recall: 0.8095 - val_auc: 0.9746 - val_prc: 0.7591\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.5890e-04 - accuracy: 0.9992 - precision: 0.7528 - recall: 0.8297 - auc: 0.9837 - prc: 0.7727 - val_loss: 0.0099 - val_accuracy: 0.9989 - val_precision: 0.5730 - val_recall: 0.8095 - val_auc: 0.9731 - val_prc: 0.7564\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.6122e-04 - accuracy: 0.9992 - precision: 0.7302 - recall: 0.8297 - auc: 0.9827 - prc: 0.7728 - val_loss: 0.0091 - val_accuracy: 0.9990 - val_precision: 0.6000 - val_recall: 0.8095 - val_auc: 0.9744 - val_prc: 0.7585\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.4605e-04 - accuracy: 0.9992 - precision: 0.7310 - recall: 0.8328 - auc: 0.9809 - prc: 0.7670 - val_loss: 0.0091 - val_accuracy: 0.9989 - val_precision: 0.5795 - val_recall: 0.8095 - val_auc: 0.9743 - val_prc: 0.7590\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.4960e-04 - accuracy: 0.9991 - precision: 0.7057 - recall: 0.8390 - auc: 0.9847 - prc: 0.7672 - val_loss: 0.0089 - val_accuracy: 0.9990 - val_precision: 0.5930 - val_recall: 0.8095 - val_auc: 0.9747 - val_prc: 0.7437\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.4300e-04 - accuracy: 0.9991 - precision: 0.7196 - recall: 0.8421 - auc: 0.9822 - prc: 0.7714 - val_loss: 0.0081 - val_accuracy: 0.9991 - val_precision: 0.6456 - val_recall: 0.8095 - val_auc: 0.9752 - val_prc: 0.7439\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.01\n",
            "****************************** w =  3\n",
            "Model: \"sequential_48\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_48 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_48 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_48 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_48 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86/89 [===========================>..] - ETA: 0s - loss: 2.0347e-04 - accuracy: 0.2124 - precision: 0.0021 - recall: 0.9550 - auc: 0.9281 - prc: 0.4745"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 15ms/step - loss: 1.9995e-04 - accuracy: 0.2074 - precision: 0.0020 - recall: 0.9560 - auc: 0.9282 - prc: 0.4750 - val_loss: 1.1448 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9547 - val_prc: 0.4080\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.1055e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9694 - prc: 0.4441 - val_loss: 1.3384 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9560 - val_prc: 0.3461\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 5.8706e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9704 - prc: 0.3519 - val_loss: 1.4898 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9570 - val_prc: 0.2715\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 4.6762e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9702 - prc: 0.2934 - val_loss: 1.6195 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9568 - val_prc: 0.2092\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.9443e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9701 - prc: 0.2510 - val_loss: 1.7359 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9571 - val_prc: 0.1701\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.4167e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9701 - prc: 0.2200 - val_loss: 1.8425 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9574 - val_prc: 0.1483\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.9578e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9714 - prc: 0.1897 - val_loss: 1.9410 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9578 - val_prc: 0.1362\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 2.6620e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9694 - prc: 0.1625 - val_loss: 2.0299 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9564 - val_prc: 0.1153\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 2.4247e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9690 - prc: 0.1429 - val_loss: 2.1134 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9564 - val_prc: 0.1000\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.1352e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9705 - prc: 0.1294 - val_loss: 2.1890 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9558 - val_prc: 0.0945\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.9824e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9697 - prc: 0.1203 - val_loss: 2.2603 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9559 - val_prc: 0.0847\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.8635e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9689 - prc: 0.1113 - val_loss: 2.3286 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9559 - val_prc: 0.0828\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.7072e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9691 - prc: 0.1026 - val_loss: 2.3918 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9558 - val_prc: 0.0774\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.6255e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9689 - prc: 0.0960 - val_loss: 2.4524 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9552 - val_prc: 0.0722\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.5791e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9669 - prc: 0.0905 - val_loss: 2.5118 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9547 - val_prc: 0.0660\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.4283e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9692 - prc: 0.0840 - val_loss: 2.5659 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9545 - val_prc: 0.0620\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.4007e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9663 - prc: 0.0778 - val_loss: 2.6190 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9541 - val_prc: 0.0569\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.3220e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9668 - prc: 0.0742 - val_loss: 2.6688 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9534 - val_prc: 0.0537\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.2513e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9668 - prc: 0.0698 - val_loss: 2.7163 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9522 - val_prc: 0.0501\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.1931e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9670 - prc: 0.0650 - val_loss: 2.7620 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9525 - val_prc: 0.0470\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.02\n",
            "****************************** w =  3\n",
            "Model: \"sequential_49\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_49 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_49 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_49 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_49 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86/89 [===========================>..] - ETA: 0s - loss: 1.9289e-04 - accuracy: 0.0197 - precision: 0.0017 - recall: 1.0000 - auc: 0.9048 - prc: 0.0747"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 16ms/step - loss: 1.9240e-04 - accuracy: 0.0192 - precision: 0.0017 - recall: 1.0000 - auc: 0.9059 - prc: 0.0774 - val_loss: 1.0566 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9728 - val_prc: 0.5527\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.4663e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9704 - prc: 0.5703 - val_loss: 1.2333 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9715 - val_prc: 0.4887\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.2093e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9753 - prc: 0.4798 - val_loss: 1.3691 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9708 - val_prc: 0.3670\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 6.1642e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9741 - prc: 0.4152 - val_loss: 1.4842 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9701 - val_prc: 0.3163\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.4133e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9753 - prc: 0.3701 - val_loss: 1.5844 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9693 - val_prc: 0.2745\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 4.9232e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9739 - prc: 0.3273 - val_loss: 1.6734 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9695 - val_prc: 0.2267\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 4.5642e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9743 - prc: 0.2823 - val_loss: 1.7538 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9689 - val_prc: 0.2080\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 4.3335e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9740 - prc: 0.2509 - val_loss: 1.8283 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9682 - val_prc: 0.1814\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 4.1013e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9737 - prc: 0.2287 - val_loss: 1.8957 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9678 - val_prc: 0.1659\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.9653e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9726 - prc: 0.2072 - val_loss: 1.9589 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9673 - val_prc: 0.1470\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.7721e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9728 - prc: 0.1888 - val_loss: 2.0140 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9675 - val_prc: 0.1322\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.6817e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9725 - prc: 0.1739 - val_loss: 2.0652 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9672 - val_prc: 0.1252\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.5627e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9736 - prc: 0.1635 - val_loss: 2.1129 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9667 - val_prc: 0.1193\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.4814e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9739 - prc: 0.1573 - val_loss: 2.1584 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9667 - val_prc: 0.1147\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.4196e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9730 - prc: 0.1445 - val_loss: 2.1988 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9664 - val_prc: 0.1085\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.3325e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9740 - prc: 0.1400 - val_loss: 2.2352 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9662 - val_prc: 0.1043\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.2993e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9731 - prc: 0.1322 - val_loss: 2.2690 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9665 - val_prc: 0.0998\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.2620e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9733 - prc: 0.1261 - val_loss: 2.3012 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9654 - val_prc: 0.0954\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.2582e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9725 - prc: 0.1226 - val_loss: 2.3334 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9660 - val_prc: 0.0914\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 3.2283e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9724 - prc: 0.1159 - val_loss: 2.3619 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9652 - val_prc: 0.0875\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.03\n",
            "****************************** w =  3\n",
            "Model: \"sequential_50\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_50 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_50 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_50 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_50 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "84/89 [===========================>..] - ETA: 0s - loss: 2.4796e-04 - accuracy: 0.0208 - precision: 0.0017 - recall: 0.9863 - auc: 0.8899 - prc: 0.0973"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 16ms/step - loss: 2.4053e-04 - accuracy: 0.0200 - precision: 0.0017 - recall: 0.9870 - auc: 0.8951 - prc: 0.1067 - val_loss: 1.1131 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9577 - val_prc: 0.4535\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.1431e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9685 - prc: 0.4763 - val_loss: 1.2677 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9595 - val_prc: 0.4106\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.8301e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9695 - prc: 0.4321 - val_loss: 1.3794 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9607 - val_prc: 0.3481\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.9589e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9711 - prc: 0.3855 - val_loss: 1.4653 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9607 - val_prc: 0.3030\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 8.4482e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9732 - prc: 0.3498 - val_loss: 1.5347 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9609 - val_prc: 0.2787\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 8.1718e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9732 - prc: 0.3298 - val_loss: 1.5911 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9613 - val_prc: 0.2567\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 7.9822e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9726 - prc: 0.3101 - val_loss: 1.6376 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9613 - val_prc: 0.2431\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.8260e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9739 - prc: 0.3027 - val_loss: 1.6780 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9622 - val_prc: 0.2239\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.7186e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9740 - prc: 0.2868 - val_loss: 1.7118 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9622 - val_prc: 0.2136\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 7.6472e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9740 - prc: 0.2789 - val_loss: 1.7396 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9620 - val_prc: 0.2041\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.5825e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9744 - prc: 0.2672 - val_loss: 1.7632 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9623 - val_prc: 0.2022\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 7.5348e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9746 - prc: 0.2569 - val_loss: 1.7843 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9623 - val_prc: 0.1927\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.4632e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9750 - prc: 0.2488 - val_loss: 1.8009 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9628 - val_prc: 0.1843\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.4497e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9750 - prc: 0.2435 - val_loss: 1.8148 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9619 - val_prc: 0.1785\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.3787e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9756 - prc: 0.2371 - val_loss: 1.8258 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9631 - val_prc: 0.1750\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 7.3631e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9760 - prc: 0.2317 - val_loss: 1.8360 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9627 - val_prc: 0.1723\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 7.2939e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9766 - prc: 0.2273 - val_loss: 1.8409 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9634 - val_prc: 0.1704\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 7.2865e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9766 - prc: 0.2286 - val_loss: 1.8458 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9632 - val_prc: 0.1698\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 7.2519e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9773 - prc: 0.2247 - val_loss: 1.8501 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9630 - val_prc: 0.1671\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 7.2359e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9772 - prc: 0.2177 - val_loss: 1.8520 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9639 - val_prc: 0.1664\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.04\n",
            "****************************** w =  3\n",
            "Model: \"sequential_51\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_51 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_51 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_51 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_51 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86/89 [===========================>..] - ETA: 0s - loss: 2.5628e-04 - accuracy: 0.0174 - precision: 0.0017 - recall: 0.9947 - auc: 0.9158 - prc: 0.2066"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 16ms/step - loss: 2.5166e-04 - accuracy: 0.0171 - precision: 0.0017 - recall: 0.9948 - auc: 0.9181 - prc: 0.2162 - val_loss: 1.0210 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9558 - val_prc: 0.5650\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.6585e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9677 - prc: 0.5914 - val_loss: 1.1385 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9603 - val_prc: 0.5332\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.5103e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9717 - prc: 0.5486 - val_loss: 1.2119 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9625 - val_prc: 0.5082\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.4435e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9747 - prc: 0.5160 - val_loss: 1.2626 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9634 - val_prc: 0.4596\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.4074e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9750 - prc: 0.5035 - val_loss: 1.2983 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9641 - val_prc: 0.4365\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.3799e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9761 - prc: 0.4624 - val_loss: 1.3208 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9652 - val_prc: 0.3832\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.3700e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9757 - prc: 0.4460 - val_loss: 1.3367 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9659 - val_prc: 0.3717\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.3580e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9755 - prc: 0.4258 - val_loss: 1.3470 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9662 - val_prc: 0.3650\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.3427e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9779 - prc: 0.4320 - val_loss: 1.3551 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9671 - val_prc: 0.3538\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.3278e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9785 - prc: 0.4266 - val_loss: 1.3586 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9666 - val_prc: 0.3479\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.3270e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9777 - prc: 0.4211 - val_loss: 1.3597 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9673 - val_prc: 0.3479\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.3168e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9787 - prc: 0.4121 - val_loss: 1.3573 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9674 - val_prc: 0.3447\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.3091e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9790 - prc: 0.4176 - val_loss: 1.3549 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9681 - val_prc: 0.3415\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.3000e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9791 - prc: 0.4107 - val_loss: 1.3491 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9679 - val_prc: 0.3419\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.2841e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9809 - prc: 0.4113 - val_loss: 1.3418 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9682 - val_prc: 0.3402\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.2806e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9804 - prc: 0.4108 - val_loss: 1.3362 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9683 - val_prc: 0.3446\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.2658e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9816 - prc: 0.4197 - val_loss: 1.3290 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9681 - val_prc: 0.3447\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.2622e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9816 - prc: 0.4115 - val_loss: 1.3205 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9686 - val_prc: 0.3506\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.2638e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9809 - prc: 0.4092 - val_loss: 1.3135 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9684 - val_prc: 0.3535\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.2560e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9812 - prc: 0.4073 - val_loss: 1.3057 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9689 - val_prc: 0.3575\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.05\n",
            "****************************** w =  3\n",
            "Model: \"sequential_52\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_52 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_52 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_52 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_52 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87/89 [============================>.] - ETA: 0s - loss: 3.3408e-04 - accuracy: 0.0170 - precision: 0.0017 - recall: 0.9842 - auc: 0.9209 - prc: 0.3402"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 16ms/step - loss: 3.3290e-04 - accuracy: 0.0168 - precision: 0.0017 - recall: 0.9845 - auc: 0.9213 - prc: 0.3439 - val_loss: 0.9914 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9611 - val_prc: 0.5926\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 2.3255e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9664 - prc: 0.6291 - val_loss: 1.0596 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9648 - val_prc: 0.5779\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.1987e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9720 - prc: 0.6313 - val_loss: 1.0841 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9661 - val_prc: 0.5636\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 2.1487e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9752 - prc: 0.6123 - val_loss: 1.0928 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9676 - val_prc: 0.5564\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 2.0952e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9777 - prc: 0.6255 - val_loss: 1.0898 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9680 - val_prc: 0.5585\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.0775e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9772 - prc: 0.6117 - val_loss: 1.0806 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9696 - val_prc: 0.5407\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 2.0541e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9777 - prc: 0.6047 - val_loss: 1.0712 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9697 - val_prc: 0.5292\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 2.0130e-04 - accuracy: 0.0020 - precision: 0.0018 - recall: 1.0000 - auc: 0.9802 - prc: 0.5802 - val_loss: 1.0564 - val_accuracy: 0.0015 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9701 - val_prc: 0.5218\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 2.0001e-04 - accuracy: 0.0029 - precision: 0.0018 - recall: 1.0000 - auc: 0.9797 - prc: 0.5953 - val_loss: 1.0457 - val_accuracy: 0.0023 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9699 - val_prc: 0.5039\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 1.9698e-04 - accuracy: 0.0042 - precision: 0.0018 - recall: 1.0000 - auc: 0.9799 - prc: 0.5695 - val_loss: 1.0284 - val_accuracy: 0.0042 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9704 - val_prc: 0.5121\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.9507e-04 - accuracy: 0.0070 - precision: 0.0018 - recall: 1.0000 - auc: 0.9821 - prc: 0.5814 - val_loss: 1.0133 - val_accuracy: 0.0071 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9707 - val_prc: 0.5069\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.9250e-04 - accuracy: 0.0127 - precision: 0.0018 - recall: 1.0000 - auc: 0.9827 - prc: 0.5744 - val_loss: 0.9986 - val_accuracy: 0.0127 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9713 - val_prc: 0.5075\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.9009e-04 - accuracy: 0.0218 - precision: 0.0018 - recall: 1.0000 - auc: 0.9829 - prc: 0.5782 - val_loss: 0.9821 - val_accuracy: 0.0221 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9716 - val_prc: 0.5099\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.8903e-04 - accuracy: 0.0360 - precision: 0.0018 - recall: 1.0000 - auc: 0.9824 - prc: 0.5855 - val_loss: 0.9686 - val_accuracy: 0.0384 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9714 - val_prc: 0.5108\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.8673e-04 - accuracy: 0.0542 - precision: 0.0019 - recall: 1.0000 - auc: 0.9829 - prc: 0.5796 - val_loss: 0.9558 - val_accuracy: 0.0591 - val_precision: 0.0015 - val_recall: 1.0000 - val_auc: 0.9713 - val_prc: 0.5053\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.8463e-04 - accuracy: 0.0870 - precision: 0.0019 - recall: 1.0000 - auc: 0.9835 - prc: 0.5880 - val_loss: 0.9465 - val_accuracy: 0.0851 - val_precision: 0.0015 - val_recall: 1.0000 - val_auc: 0.9714 - val_prc: 0.4974\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.8267e-04 - accuracy: 0.1047 - precision: 0.0020 - recall: 1.0000 - auc: 0.9837 - prc: 0.5875 - val_loss: 0.9319 - val_accuracy: 0.1173 - val_precision: 0.0016 - val_recall: 1.0000 - val_auc: 0.9714 - val_prc: 0.4996\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.8127e-04 - accuracy: 0.1428 - precision: 0.0021 - recall: 1.0000 - auc: 0.9840 - prc: 0.5972 - val_loss: 0.9194 - val_accuracy: 0.1535 - val_precision: 0.0016 - val_recall: 0.9841 - val_auc: 0.9717 - val_prc: 0.5099\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.8094e-04 - accuracy: 0.1885 - precision: 0.0022 - recall: 1.0000 - auc: 0.9832 - prc: 0.5940 - val_loss: 0.9105 - val_accuracy: 0.1891 - val_precision: 0.0017 - val_recall: 0.9841 - val_auc: 0.9716 - val_prc: 0.5105\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.7839e-04 - accuracy: 0.2047 - precision: 0.0022 - recall: 0.9969 - auc: 0.9838 - prc: 0.5849 - val_loss: 0.8979 - val_accuracy: 0.2318 - val_precision: 0.0018 - val_recall: 0.9841 - val_auc: 0.9718 - val_prc: 0.5163\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.060000000000000005\n",
            "****************************** w =  3\n",
            "Model: \"sequential_53\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_53 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_53 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_53 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_53 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87/89 [============================>.] - ETA: 0s - loss: 3.9079e-04 - accuracy: 0.0567 - precision: 0.0018 - recall: 0.9947 - auc: 0.9498 - prc: 0.4732"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 15ms/step - loss: 3.8982e-04 - accuracy: 0.0559 - precision: 0.0018 - recall: 0.9948 - auc: 0.9502 - prc: 0.4721 - val_loss: 0.9321 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9697 - val_prc: 0.6025\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.1888e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9718 - prc: 0.6458 - val_loss: 0.9442 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9723 - val_prc: 0.6070\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.0603e-04 - accuracy: 0.0019 - precision: 0.0018 - recall: 1.0000 - auc: 0.9738 - prc: 0.6573 - val_loss: 0.9270 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9736 - val_prc: 0.6315\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 2.9858e-04 - accuracy: 0.0029 - precision: 0.0018 - recall: 1.0000 - auc: 0.9744 - prc: 0.6726 - val_loss: 0.9060 - val_accuracy: 0.0018 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9747 - val_prc: 0.6403\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 2.8878e-04 - accuracy: 0.0114 - precision: 0.0018 - recall: 1.0000 - auc: 0.9794 - prc: 0.6638 - val_loss: 0.8792 - val_accuracy: 0.0115 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9743 - val_prc: 0.6357\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.8207e-04 - accuracy: 0.0447 - precision: 0.0019 - recall: 1.0000 - auc: 0.9805 - prc: 0.6786 - val_loss: 0.8498 - val_accuracy: 0.0671 - val_precision: 0.0015 - val_recall: 1.0000 - val_auc: 0.9753 - val_prc: 0.6400\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 2.7654e-04 - accuracy: 0.1281 - precision: 0.0020 - recall: 1.0000 - auc: 0.9802 - prc: 0.6794 - val_loss: 0.8237 - val_accuracy: 0.1766 - val_precision: 0.0017 - val_recall: 1.0000 - val_auc: 0.9743 - val_prc: 0.6404\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 2.7025e-04 - accuracy: 0.2646 - precision: 0.0024 - recall: 0.9969 - auc: 0.9817 - prc: 0.6893 - val_loss: 0.8010 - val_accuracy: 0.2982 - val_precision: 0.0019 - val_recall: 0.9841 - val_auc: 0.9745 - val_prc: 0.6580\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 2.6605e-04 - accuracy: 0.3637 - precision: 0.0028 - recall: 0.9938 - auc: 0.9807 - prc: 0.6908 - val_loss: 0.7784 - val_accuracy: 0.4094 - val_precision: 0.0023 - val_recall: 0.9841 - val_auc: 0.9740 - val_prc: 0.6479\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 2.6009e-04 - accuracy: 0.4632 - precision: 0.0033 - recall: 0.9969 - auc: 0.9825 - prc: 0.6742 - val_loss: 0.7580 - val_accuracy: 0.5028 - val_precision: 0.0027 - val_recall: 0.9841 - val_auc: 0.9742 - val_prc: 0.6521\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.5645e-04 - accuracy: 0.5173 - precision: 0.0037 - recall: 0.9969 - auc: 0.9837 - prc: 0.6789 - val_loss: 0.7347 - val_accuracy: 0.5800 - val_precision: 0.0032 - val_recall: 0.9841 - val_auc: 0.9742 - val_prc: 0.6169\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 2.5181e-04 - accuracy: 0.6108 - precision: 0.0045 - recall: 0.9938 - auc: 0.9837 - prc: 0.6625 - val_loss: 0.7199 - val_accuracy: 0.6236 - val_precision: 0.0036 - val_recall: 0.9683 - val_auc: 0.9741 - val_prc: 0.5810\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 2.4864e-04 - accuracy: 0.6317 - precision: 0.0048 - recall: 0.9907 - auc: 0.9834 - prc: 0.6543 - val_loss: 0.7006 - val_accuracy: 0.6660 - val_precision: 0.0040 - val_recall: 0.9683 - val_auc: 0.9742 - val_prc: 0.5860\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 2.4523e-04 - accuracy: 0.6807 - precision: 0.0055 - recall: 0.9876 - auc: 0.9831 - prc: 0.6467 - val_loss: 0.6862 - val_accuracy: 0.6924 - val_precision: 0.0043 - val_recall: 0.9683 - val_auc: 0.9738 - val_prc: 0.5857\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 2.4148e-04 - accuracy: 0.7077 - precision: 0.0060 - recall: 0.9876 - auc: 0.9834 - prc: 0.6358 - val_loss: 0.6741 - val_accuracy: 0.7109 - val_precision: 0.0046 - val_recall: 0.9683 - val_auc: 0.9741 - val_prc: 0.5770\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.3652e-04 - accuracy: 0.7199 - precision: 0.0063 - recall: 0.9938 - auc: 0.9848 - prc: 0.6348 - val_loss: 0.6584 - val_accuracy: 0.7310 - val_precision: 0.0050 - val_recall: 0.9683 - val_auc: 0.9740 - val_prc: 0.5770\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.3423e-04 - accuracy: 0.7411 - precision: 0.0067 - recall: 0.9876 - auc: 0.9847 - prc: 0.6369 - val_loss: 0.6464 - val_accuracy: 0.7438 - val_precision: 0.0052 - val_recall: 0.9683 - val_auc: 0.9738 - val_prc: 0.5766\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 2.3106e-04 - accuracy: 0.7506 - precision: 0.0070 - recall: 0.9938 - auc: 0.9852 - prc: 0.6345 - val_loss: 0.6339 - val_accuracy: 0.7562 - val_precision: 0.0055 - val_recall: 0.9683 - val_auc: 0.9737 - val_prc: 0.5766\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.2836e-04 - accuracy: 0.7672 - precision: 0.0075 - recall: 0.9876 - auc: 0.9851 - prc: 0.6240 - val_loss: 0.6243 - val_accuracy: 0.7644 - val_precision: 0.0057 - val_recall: 0.9683 - val_auc: 0.9739 - val_prc: 0.5676\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 2.2624e-04 - accuracy: 0.7742 - precision: 0.0077 - recall: 0.9876 - auc: 0.9845 - prc: 0.6363 - val_loss: 0.6146 - val_accuracy: 0.7725 - val_precision: 0.0059 - val_recall: 0.9683 - val_auc: 0.9737 - val_prc: 0.5603\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.07\n",
            "****************************** w =  3\n",
            "Model: \"sequential_54\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_54 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_54 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_54 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_54 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85/89 [===========================>..] - ETA: 0s - loss: 4.8174e-04 - accuracy: 0.2364 - precision: 0.0022 - recall: 0.9894 - auc: 0.9632 - prc: 0.5116"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 15ms/step - loss: 4.7708e-04 - accuracy: 0.2300 - precision: 0.0022 - recall: 0.9896 - auc: 0.9636 - prc: 0.5177 - val_loss: 0.8136 - val_accuracy: 0.0233 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9675 - val_prc: 0.6331\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.0549e-04 - accuracy: 0.1163 - precision: 0.0020 - recall: 1.0000 - auc: 0.9708 - prc: 0.6989 - val_loss: 0.7736 - val_accuracy: 0.2240 - val_precision: 0.0018 - val_recall: 0.9841 - val_auc: 0.9710 - val_prc: 0.6572\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.8550e-04 - accuracy: 0.3524 - precision: 0.0027 - recall: 0.9969 - auc: 0.9752 - prc: 0.7119 - val_loss: 0.7248 - val_accuracy: 0.5217 - val_precision: 0.0028 - val_recall: 0.9841 - val_auc: 0.9725 - val_prc: 0.6739\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.6822e-04 - accuracy: 0.6047 - precision: 0.0044 - recall: 0.9876 - auc: 0.9783 - prc: 0.7307 - val_loss: 0.6823 - val_accuracy: 0.6876 - val_precision: 0.0044 - val_recall: 0.9841 - val_auc: 0.9734 - val_prc: 0.6873\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 3.5479e-04 - accuracy: 0.7443 - precision: 0.0067 - recall: 0.9721 - auc: 0.9781 - prc: 0.7328 - val_loss: 0.6485 - val_accuracy: 0.7666 - val_precision: 0.0057 - val_recall: 0.9683 - val_auc: 0.9739 - val_prc: 0.6931\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 3.4100e-04 - accuracy: 0.7896 - precision: 0.0081 - recall: 0.9659 - auc: 0.9801 - prc: 0.7318 - val_loss: 0.6089 - val_accuracy: 0.8268 - val_precision: 0.0077 - val_recall: 0.9683 - val_auc: 0.9748 - val_prc: 0.6941\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 3.2930e-04 - accuracy: 0.8359 - precision: 0.0104 - recall: 0.9659 - auc: 0.9813 - prc: 0.7367 - val_loss: 0.5758 - val_accuracy: 0.8573 - val_precision: 0.0092 - val_recall: 0.9524 - val_auc: 0.9750 - val_prc: 0.6980\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.2063e-04 - accuracy: 0.8636 - precision: 0.0124 - recall: 0.9628 - auc: 0.9819 - prc: 0.7402 - val_loss: 0.5500 - val_accuracy: 0.8731 - val_precision: 0.0103 - val_recall: 0.9524 - val_auc: 0.9752 - val_prc: 0.6979\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.1324e-04 - accuracy: 0.8806 - precision: 0.0141 - recall: 0.9598 - auc: 0.9820 - prc: 0.7407 - val_loss: 0.5318 - val_accuracy: 0.8794 - val_precision: 0.0108 - val_recall: 0.9524 - val_auc: 0.9752 - val_prc: 0.6934\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.0395e-04 - accuracy: 0.8855 - precision: 0.0147 - recall: 0.9598 - auc: 0.9836 - prc: 0.7384 - val_loss: 0.5118 - val_accuracy: 0.8864 - val_precision: 0.0113 - val_recall: 0.9365 - val_auc: 0.9750 - val_prc: 0.6923\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.9902e-04 - accuracy: 0.8900 - precision: 0.0152 - recall: 0.9567 - auc: 0.9836 - prc: 0.7306 - val_loss: 0.4925 - val_accuracy: 0.8931 - val_precision: 0.0120 - val_recall: 0.9365 - val_auc: 0.9751 - val_prc: 0.6724\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 2.9136e-04 - accuracy: 0.8966 - precision: 0.0163 - recall: 0.9628 - auc: 0.9839 - prc: 0.7008 - val_loss: 0.4749 - val_accuracy: 0.8983 - val_precision: 0.0126 - val_recall: 0.9365 - val_auc: 0.9750 - val_prc: 0.6607\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 2.8525e-04 - accuracy: 0.8985 - precision: 0.0165 - recall: 0.9567 - auc: 0.9844 - prc: 0.6905 - val_loss: 0.4570 - val_accuracy: 0.9034 - val_precision: 0.0133 - val_recall: 0.9365 - val_auc: 0.9750 - val_prc: 0.6200\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.8051e-04 - accuracy: 0.9042 - precision: 0.0175 - recall: 0.9598 - auc: 0.9847 - prc: 0.6792 - val_loss: 0.4456 - val_accuracy: 0.9040 - val_precision: 0.0136 - val_recall: 0.9524 - val_auc: 0.9750 - val_prc: 0.6108\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 2.7450e-04 - accuracy: 0.9007 - precision: 0.0169 - recall: 0.9598 - auc: 0.9849 - prc: 0.6721 - val_loss: 0.4288 - val_accuracy: 0.9083 - val_precision: 0.0140 - val_recall: 0.9365 - val_auc: 0.9748 - val_prc: 0.6012\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.7238e-04 - accuracy: 0.9069 - precision: 0.0180 - recall: 0.9567 - auc: 0.9845 - prc: 0.6739 - val_loss: 0.4188 - val_accuracy: 0.9091 - val_precision: 0.0141 - val_recall: 0.9365 - val_auc: 0.9746 - val_prc: 0.6018\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.6491e-04 - accuracy: 0.9110 - precision: 0.0188 - recall: 0.9567 - auc: 0.9862 - prc: 0.6713 - val_loss: 0.4130 - val_accuracy: 0.9076 - val_precision: 0.0139 - val_recall: 0.9365 - val_auc: 0.9745 - val_prc: 0.6040\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 2.6213e-04 - accuracy: 0.9109 - precision: 0.0187 - recall: 0.9536 - auc: 0.9856 - prc: 0.6586 - val_loss: 0.4051 - val_accuracy: 0.9077 - val_precision: 0.0139 - val_recall: 0.9365 - val_auc: 0.9743 - val_prc: 0.6069\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 2.5712e-04 - accuracy: 0.9111 - precision: 0.0189 - recall: 0.9598 - auc: 0.9862 - prc: 0.6552 - val_loss: 0.3996 - val_accuracy: 0.9070 - val_precision: 0.0138 - val_recall: 0.9365 - val_auc: 0.9745 - val_prc: 0.6002\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 2.5372e-04 - accuracy: 0.9079 - precision: 0.0181 - recall: 0.9536 - auc: 0.9858 - prc: 0.6498 - val_loss: 0.3888 - val_accuracy: 0.9089 - val_precision: 0.0141 - val_recall: 0.9365 - val_auc: 0.9745 - val_prc: 0.5999\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.08\n",
            "****************************** w =  3\n",
            "Model: \"sequential_55\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_55 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_55 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_55 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_55 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "84/89 [===========================>..] - ETA: 0s - loss: 6.2421e-04 - accuracy: 0.2080 - precision: 0.0021 - recall: 0.9810 - auc: 0.9542 - prc: 0.5408"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 15ms/step - loss: 6.2525e-04 - accuracy: 0.2021 - precision: 0.0021 - recall: 0.9819 - auc: 0.9529 - prc: 0.5438 - val_loss: 0.7943 - val_accuracy: 0.0478 - val_precision: 0.0015 - val_recall: 1.0000 - val_auc: 0.9676 - val_prc: 0.6500\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.3104e-04 - accuracy: 0.3155 - precision: 0.0026 - recall: 0.9907 - auc: 0.9696 - prc: 0.7148 - val_loss: 0.6997 - val_accuracy: 0.5917 - val_precision: 0.0033 - val_recall: 0.9841 - val_auc: 0.9726 - val_prc: 0.6776\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 4.8842e-04 - accuracy: 0.7589 - precision: 0.0071 - recall: 0.9659 - auc: 0.9693 - prc: 0.7357 - val_loss: 0.6249 - val_accuracy: 0.8465 - val_precision: 0.0085 - val_recall: 0.9524 - val_auc: 0.9736 - val_prc: 0.6832\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.5804e-04 - accuracy: 0.8794 - precision: 0.0138 - recall: 0.9505 - auc: 0.9746 - prc: 0.7457 - val_loss: 0.5579 - val_accuracy: 0.9173 - val_precision: 0.0152 - val_recall: 0.9206 - val_auc: 0.9737 - val_prc: 0.6940\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 4.3117e-04 - accuracy: 0.9316 - precision: 0.0235 - recall: 0.9226 - auc: 0.9725 - prc: 0.7514 - val_loss: 0.5113 - val_accuracy: 0.9379 - val_precision: 0.0202 - val_recall: 0.9206 - val_auc: 0.9744 - val_prc: 0.6958\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.0926e-04 - accuracy: 0.9391 - precision: 0.0267 - recall: 0.9350 - auc: 0.9779 - prc: 0.7516 - val_loss: 0.4633 - val_accuracy: 0.9545 - val_precision: 0.0274 - val_recall: 0.9206 - val_auc: 0.9742 - val_prc: 0.7020\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.9192e-04 - accuracy: 0.9546 - precision: 0.0351 - recall: 0.9257 - auc: 0.9803 - prc: 0.7506 - val_loss: 0.4297 - val_accuracy: 0.9597 - val_precision: 0.0308 - val_recall: 0.9206 - val_auc: 0.9744 - val_prc: 0.7030\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 3.7667e-04 - accuracy: 0.9563 - precision: 0.0364 - recall: 0.9257 - auc: 0.9803 - prc: 0.7525 - val_loss: 0.3983 - val_accuracy: 0.9640 - val_precision: 0.0344 - val_recall: 0.9206 - val_auc: 0.9746 - val_prc: 0.7173\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 3.6432e-04 - accuracy: 0.9643 - precision: 0.0435 - recall: 0.9102 - auc: 0.9815 - prc: 0.7554 - val_loss: 0.3821 - val_accuracy: 0.9627 - val_precision: 0.0332 - val_recall: 0.9206 - val_auc: 0.9745 - val_prc: 0.7178\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.5343e-04 - accuracy: 0.9617 - precision: 0.0409 - recall: 0.9164 - auc: 0.9827 - prc: 0.7509 - val_loss: 0.3650 - val_accuracy: 0.9623 - val_precision: 0.0329 - val_recall: 0.9206 - val_auc: 0.9744 - val_prc: 0.7183\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.4447e-04 - accuracy: 0.9634 - precision: 0.0424 - recall: 0.9071 - auc: 0.9824 - prc: 0.7537 - val_loss: 0.3521 - val_accuracy: 0.9608 - val_precision: 0.0317 - val_recall: 0.9206 - val_auc: 0.9745 - val_prc: 0.7305\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.3276e-04 - accuracy: 0.9603 - precision: 0.0395 - recall: 0.9133 - auc: 0.9849 - prc: 0.7493 - val_loss: 0.3345 - val_accuracy: 0.9619 - val_precision: 0.0325 - val_recall: 0.9206 - val_auc: 0.9745 - val_prc: 0.7215\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.2905e-04 - accuracy: 0.9610 - precision: 0.0403 - recall: 0.9164 - auc: 0.9829 - prc: 0.7342 - val_loss: 0.3252 - val_accuracy: 0.9601 - val_precision: 0.0311 - val_recall: 0.9206 - val_auc: 0.9743 - val_prc: 0.7122\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.2115e-04 - accuracy: 0.9595 - precision: 0.0387 - recall: 0.9133 - auc: 0.9843 - prc: 0.7327 - val_loss: 0.3133 - val_accuracy: 0.9599 - val_precision: 0.0310 - val_recall: 0.9206 - val_auc: 0.9741 - val_prc: 0.6755\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.1512e-04 - accuracy: 0.9575 - precision: 0.0374 - recall: 0.9257 - auc: 0.9840 - prc: 0.7102 - val_loss: 0.3001 - val_accuracy: 0.9605 - val_precision: 0.0315 - val_recall: 0.9206 - val_auc: 0.9739 - val_prc: 0.6445\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.0677e-04 - accuracy: 0.9585 - precision: 0.0381 - recall: 0.9195 - auc: 0.9864 - prc: 0.6990 - val_loss: 0.2918 - val_accuracy: 0.9593 - val_precision: 0.0305 - val_recall: 0.9206 - val_auc: 0.9738 - val_prc: 0.6346\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.0409e-04 - accuracy: 0.9601 - precision: 0.0396 - recall: 0.9226 - auc: 0.9849 - prc: 0.7077 - val_loss: 0.2908 - val_accuracy: 0.9553 - val_precision: 0.0278 - val_recall: 0.9206 - val_auc: 0.9737 - val_prc: 0.6206\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.9640e-04 - accuracy: 0.9571 - precision: 0.0371 - recall: 0.9257 - auc: 0.9862 - prc: 0.7017 - val_loss: 0.2849 - val_accuracy: 0.9541 - val_precision: 0.0271 - val_recall: 0.9206 - val_auc: 0.9737 - val_prc: 0.6146\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 2.9231e-04 - accuracy: 0.9522 - precision: 0.0332 - recall: 0.9195 - auc: 0.9868 - prc: 0.6781 - val_loss: 0.2768 - val_accuracy: 0.9540 - val_precision: 0.0271 - val_recall: 0.9206 - val_auc: 0.9736 - val_prc: 0.6138\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 2.9130e-04 - accuracy: 0.9559 - precision: 0.0362 - recall: 0.9288 - auc: 0.9859 - prc: 0.6990 - val_loss: 0.2773 - val_accuracy: 0.9505 - val_precision: 0.0252 - val_recall: 0.9206 - val_auc: 0.9734 - val_prc: 0.6047\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.09\n",
            "****************************** w =  3\n",
            "Model: \"sequential_56\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_56 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_56 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_56 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_56 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88/89 [============================>.] - ETA: 0s - loss: 7.5337e-04 - accuracy: 0.3847 - precision: 0.0027 - recall: 0.9714 - auc: 0.9517 - prc: 0.5761"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 15ms/step - loss: 7.5181e-04 - accuracy: 0.3862 - precision: 0.0027 - recall: 0.9715 - auc: 0.9520 - prc: 0.5776 - val_loss: 0.6837 - val_accuracy: 0.6737 - val_precision: 0.0041 - val_recall: 0.9683 - val_auc: 0.9769 - val_prc: 0.6988\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 6.2615e-04 - accuracy: 0.8702 - precision: 0.0125 - recall: 0.9195 - auc: 0.9600 - prc: 0.7276 - val_loss: 0.5502 - val_accuracy: 0.9565 - val_precision: 0.0286 - val_recall: 0.9206 - val_auc: 0.9733 - val_prc: 0.7066\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.6027e-04 - accuracy: 0.9647 - precision: 0.0438 - recall: 0.9040 - auc: 0.9600 - prc: 0.7356 - val_loss: 0.4680 - val_accuracy: 0.9719 - val_precision: 0.0436 - val_recall: 0.9206 - val_auc: 0.9715 - val_prc: 0.7173\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.1387e-04 - accuracy: 0.9763 - precision: 0.0631 - recall: 0.8885 - auc: 0.9653 - prc: 0.7437 - val_loss: 0.4116 - val_accuracy: 0.9758 - val_precision: 0.0496 - val_recall: 0.9048 - val_auc: 0.9722 - val_prc: 0.7300\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.8009e-04 - accuracy: 0.9778 - precision: 0.0666 - recall: 0.8824 - auc: 0.9701 - prc: 0.7504 - val_loss: 0.3610 - val_accuracy: 0.9785 - val_precision: 0.0554 - val_recall: 0.9048 - val_auc: 0.9731 - val_prc: 0.7307\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 4.5122e-04 - accuracy: 0.9796 - precision: 0.0724 - recall: 0.8854 - auc: 0.9738 - prc: 0.7523 - val_loss: 0.3219 - val_accuracy: 0.9796 - val_precision: 0.0585 - val_recall: 0.9048 - val_auc: 0.9747 - val_prc: 0.7319\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 4.2963e-04 - accuracy: 0.9805 - precision: 0.0758 - recall: 0.8885 - auc: 0.9770 - prc: 0.7563 - val_loss: 0.2915 - val_accuracy: 0.9805 - val_precision: 0.0609 - val_recall: 0.9048 - val_auc: 0.9751 - val_prc: 0.7366\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 4.1420e-04 - accuracy: 0.9812 - precision: 0.0782 - recall: 0.8885 - auc: 0.9778 - prc: 0.7543 - val_loss: 0.2772 - val_accuracy: 0.9794 - val_precision: 0.0578 - val_recall: 0.9048 - val_auc: 0.9755 - val_prc: 0.7362\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 3.9460e-04 - accuracy: 0.9808 - precision: 0.0771 - recall: 0.8947 - auc: 0.9819 - prc: 0.7522 - val_loss: 0.2621 - val_accuracy: 0.9787 - val_precision: 0.0561 - val_recall: 0.9048 - val_auc: 0.9759 - val_prc: 0.7394\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.8665e-04 - accuracy: 0.9802 - precision: 0.0745 - recall: 0.8854 - auc: 0.9816 - prc: 0.7506 - val_loss: 0.2563 - val_accuracy: 0.9772 - val_precision: 0.0525 - val_recall: 0.9048 - val_auc: 0.9756 - val_prc: 0.7271\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.7392e-04 - accuracy: 0.9780 - precision: 0.0679 - recall: 0.8947 - auc: 0.9832 - prc: 0.7420 - val_loss: 0.2380 - val_accuracy: 0.9777 - val_precision: 0.0536 - val_recall: 0.9048 - val_auc: 0.9756 - val_prc: 0.7095\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.6553e-04 - accuracy: 0.9791 - precision: 0.0714 - recall: 0.8947 - auc: 0.9837 - prc: 0.7469 - val_loss: 0.2346 - val_accuracy: 0.9760 - val_precision: 0.0500 - val_recall: 0.9048 - val_auc: 0.9752 - val_prc: 0.7183\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.5608e-04 - accuracy: 0.9775 - precision: 0.0667 - recall: 0.8978 - auc: 0.9838 - prc: 0.7270 - val_loss: 0.2252 - val_accuracy: 0.9755 - val_precision: 0.0491 - val_recall: 0.9048 - val_auc: 0.9751 - val_prc: 0.6832\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.5240e-04 - accuracy: 0.9767 - precision: 0.0645 - recall: 0.8978 - auc: 0.9839 - prc: 0.7139 - val_loss: 0.2216 - val_accuracy: 0.9742 - val_precision: 0.0466 - val_recall: 0.9048 - val_auc: 0.9748 - val_prc: 0.6438\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.4624e-04 - accuracy: 0.9752 - precision: 0.0609 - recall: 0.8978 - auc: 0.9839 - prc: 0.7113 - val_loss: 0.2143 - val_accuracy: 0.9739 - val_precision: 0.0462 - val_recall: 0.9048 - val_auc: 0.9744 - val_prc: 0.6343\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.3849e-04 - accuracy: 0.9754 - precision: 0.0616 - recall: 0.9009 - auc: 0.9854 - prc: 0.7222 - val_loss: 0.2116 - val_accuracy: 0.9730 - val_precision: 0.0446 - val_recall: 0.9048 - val_auc: 0.9742 - val_prc: 0.6274\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.3343e-04 - accuracy: 0.9742 - precision: 0.0590 - recall: 0.9040 - auc: 0.9850 - prc: 0.7046 - val_loss: 0.2059 - val_accuracy: 0.9726 - val_precision: 0.0439 - val_recall: 0.9048 - val_auc: 0.9741 - val_prc: 0.6239\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.2854e-04 - accuracy: 0.9741 - precision: 0.0589 - recall: 0.9040 - auc: 0.9857 - prc: 0.7042 - val_loss: 0.2037 - val_accuracy: 0.9717 - val_precision: 0.0426 - val_recall: 0.9048 - val_auc: 0.9739 - val_prc: 0.6228\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.2599e-04 - accuracy: 0.9714 - precision: 0.0536 - recall: 0.9040 - auc: 0.9856 - prc: 0.6916 - val_loss: 0.1944 - val_accuracy: 0.9722 - val_precision: 0.0433 - val_recall: 0.9048 - val_auc: 0.9739 - val_prc: 0.6224\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 3.2286e-04 - accuracy: 0.9726 - precision: 0.0557 - recall: 0.9009 - auc: 0.9860 - prc: 0.7027 - val_loss: 0.1977 - val_accuracy: 0.9703 - val_precision: 0.0407 - val_recall: 0.9048 - val_auc: 0.9735 - val_prc: 0.6242\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.09999999999999999\n",
            "****************************** w =  3\n",
            "Model: \"sequential_57\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_57 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_57 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_57 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_57 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88/89 [============================>.] - ETA: 0s - loss: 9.0079e-04 - accuracy: 0.8046 - precision: 0.0079 - recall: 0.9112 - auc: 0.9500 - prc: 0.5703"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 16ms/step - loss: 9.0216e-04 - accuracy: 0.8054 - precision: 0.0079 - recall: 0.9093 - auc: 0.9486 - prc: 0.5704 - val_loss: 0.5763 - val_accuracy: 0.9505 - val_precision: 0.0252 - val_recall: 0.9206 - val_auc: 0.9678 - val_prc: 0.6906\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 7.1006e-04 - accuracy: 0.9716 - precision: 0.0528 - recall: 0.8854 - auc: 0.9637 - prc: 0.7284 - val_loss: 0.4233 - val_accuracy: 0.9848 - val_precision: 0.0748 - val_recall: 0.8730 - val_auc: 0.9660 - val_prc: 0.7178\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 6.1934e-04 - accuracy: 0.9864 - precision: 0.1034 - recall: 0.8669 - auc: 0.9615 - prc: 0.7317 - val_loss: 0.3467 - val_accuracy: 0.9852 - val_precision: 0.0764 - val_recall: 0.8730 - val_auc: 0.9677 - val_prc: 0.7174\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.5474e-04 - accuracy: 0.9874 - precision: 0.1105 - recall: 0.8638 - auc: 0.9635 - prc: 0.7426 - val_loss: 0.2977 - val_accuracy: 0.9848 - val_precision: 0.0748 - val_recall: 0.8730 - val_auc: 0.9705 - val_prc: 0.7276\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.0819e-04 - accuracy: 0.9861 - precision: 0.1024 - recall: 0.8762 - auc: 0.9745 - prc: 0.7519 - val_loss: 0.2502 - val_accuracy: 0.9861 - val_precision: 0.0811 - val_recall: 0.8730 - val_auc: 0.9725 - val_prc: 0.7287\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.7800e-04 - accuracy: 0.9872 - precision: 0.1100 - recall: 0.8731 - auc: 0.9763 - prc: 0.7555 - val_loss: 0.2257 - val_accuracy: 0.9855 - val_precision: 0.0780 - val_recall: 0.8730 - val_auc: 0.9731 - val_prc: 0.7325\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 4.5638e-04 - accuracy: 0.9858 - precision: 0.1007 - recall: 0.8793 - auc: 0.9765 - prc: 0.7534 - val_loss: 0.2045 - val_accuracy: 0.9854 - val_precision: 0.0787 - val_recall: 0.8889 - val_auc: 0.9735 - val_prc: 0.7347\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 4.3475e-04 - accuracy: 0.9859 - precision: 0.1013 - recall: 0.8793 - auc: 0.9808 - prc: 0.7549 - val_loss: 0.1965 - val_accuracy: 0.9840 - val_precision: 0.0733 - val_recall: 0.9048 - val_auc: 0.9735 - val_prc: 0.7347\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 4.2090e-04 - accuracy: 0.9845 - precision: 0.0935 - recall: 0.8854 - auc: 0.9820 - prc: 0.7564 - val_loss: 0.1871 - val_accuracy: 0.9834 - val_precision: 0.0707 - val_recall: 0.9048 - val_auc: 0.9738 - val_prc: 0.7335\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 4.0562e-04 - accuracy: 0.9837 - precision: 0.0901 - recall: 0.8947 - auc: 0.9825 - prc: 0.7527 - val_loss: 0.1764 - val_accuracy: 0.9831 - val_precision: 0.0699 - val_recall: 0.9048 - val_auc: 0.9736 - val_prc: 0.7273\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.9851e-04 - accuracy: 0.9836 - precision: 0.0893 - recall: 0.8916 - auc: 0.9833 - prc: 0.7390 - val_loss: 0.1719 - val_accuracy: 0.9822 - val_precision: 0.0662 - val_recall: 0.9048 - val_auc: 0.9738 - val_prc: 0.7121\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.9052e-04 - accuracy: 0.9823 - precision: 0.0833 - recall: 0.8947 - auc: 0.9842 - prc: 0.7360 - val_loss: 0.1649 - val_accuracy: 0.9819 - val_precision: 0.0653 - val_recall: 0.9048 - val_auc: 0.9732 - val_prc: 0.6952\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.7818e-04 - accuracy: 0.9817 - precision: 0.0810 - recall: 0.8978 - auc: 0.9847 - prc: 0.7308 - val_loss: 0.1564 - val_accuracy: 0.9822 - val_precision: 0.0664 - val_recall: 0.9048 - val_auc: 0.9732 - val_prc: 0.6953\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.6826e-04 - accuracy: 0.9824 - precision: 0.0838 - recall: 0.8978 - auc: 0.9858 - prc: 0.7306 - val_loss: 0.1565 - val_accuracy: 0.9808 - val_precision: 0.0618 - val_recall: 0.9048 - val_auc: 0.9732 - val_prc: 0.6773\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.6948e-04 - accuracy: 0.9805 - precision: 0.0765 - recall: 0.9009 - auc: 0.9848 - prc: 0.7166 - val_loss: 0.1531 - val_accuracy: 0.9805 - val_precision: 0.0609 - val_recall: 0.9048 - val_auc: 0.9731 - val_prc: 0.6617\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.6118e-04 - accuracy: 0.9822 - precision: 0.0831 - recall: 0.8978 - auc: 0.9857 - prc: 0.7181 - val_loss: 0.1603 - val_accuracy: 0.9777 - val_precision: 0.0535 - val_recall: 0.9048 - val_auc: 0.9727 - val_prc: 0.6249\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.5840e-04 - accuracy: 0.9789 - precision: 0.0711 - recall: 0.9009 - auc: 0.9853 - prc: 0.7020 - val_loss: 0.1489 - val_accuracy: 0.9792 - val_precision: 0.0572 - val_recall: 0.9048 - val_auc: 0.9732 - val_prc: 0.6229\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.5249e-04 - accuracy: 0.9796 - precision: 0.0735 - recall: 0.9009 - auc: 0.9858 - prc: 0.6997 - val_loss: 0.1464 - val_accuracy: 0.9787 - val_precision: 0.0560 - val_recall: 0.9048 - val_auc: 0.9733 - val_prc: 0.6223\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.5169e-04 - accuracy: 0.9793 - precision: 0.0725 - recall: 0.9009 - auc: 0.9862 - prc: 0.6974 - val_loss: 0.1475 - val_accuracy: 0.9777 - val_precision: 0.0535 - val_recall: 0.9048 - val_auc: 0.9725 - val_prc: 0.6250\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.4579e-04 - accuracy: 0.9787 - precision: 0.0707 - recall: 0.9040 - auc: 0.9869 - prc: 0.6966 - val_loss: 0.1508 - val_accuracy: 0.9761 - val_precision: 0.0502 - val_recall: 0.9048 - val_auc: 0.9725 - val_prc: 0.6188\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.10999999999999999\n",
            "****************************** w =  3\n",
            "Model: \"sequential_58\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_58 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_58 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_58 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_58 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86/89 [===========================>..] - ETA: 0s - loss: 0.0011 - accuracy: 0.7391 - precision: 0.0060 - recall: 0.9237 - auc: 0.9454 - prc: 0.5639"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 15ms/step - loss: 0.0011 - accuracy: 0.7450 - precision: 0.0061 - recall: 0.9249 - auc: 0.9469 - prc: 0.5690 - val_loss: 0.5099 - val_accuracy: 0.9867 - val_precision: 0.0846 - val_recall: 0.8730 - val_auc: 0.9626 - val_prc: 0.7061\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 8.1594e-04 - accuracy: 0.9918 - precision: 0.1605 - recall: 0.8576 - auc: 0.9515 - prc: 0.7237 - val_loss: 0.3497 - val_accuracy: 0.9941 - val_precision: 0.1714 - val_recall: 0.8571 - val_auc: 0.9634 - val_prc: 0.7307\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 6.7421e-04 - accuracy: 0.9931 - precision: 0.1863 - recall: 0.8607 - auc: 0.9591 - prc: 0.7363 - val_loss: 0.2540 - val_accuracy: 0.9954 - val_precision: 0.2109 - val_recall: 0.8571 - val_auc: 0.9669 - val_prc: 0.7368\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 5.9155e-04 - accuracy: 0.9941 - precision: 0.2137 - recall: 0.8607 - auc: 0.9678 - prc: 0.7407 - val_loss: 0.2079 - val_accuracy: 0.9941 - val_precision: 0.1720 - val_recall: 0.8571 - val_auc: 0.9696 - val_prc: 0.7334\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.4617e-04 - accuracy: 0.9934 - precision: 0.1926 - recall: 0.8576 - auc: 0.9694 - prc: 0.7504 - val_loss: 0.1885 - val_accuracy: 0.9915 - val_precision: 0.1250 - val_recall: 0.8571 - val_auc: 0.9728 - val_prc: 0.7348\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 5.0776e-04 - accuracy: 0.9918 - precision: 0.1631 - recall: 0.8669 - auc: 0.9746 - prc: 0.7538 - val_loss: 0.1693 - val_accuracy: 0.9903 - val_precision: 0.1109 - val_recall: 0.8571 - val_auc: 0.9743 - val_prc: 0.7361\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.8138e-04 - accuracy: 0.9913 - precision: 0.1538 - recall: 0.8700 - auc: 0.9788 - prc: 0.7574 - val_loss: 0.1577 - val_accuracy: 0.9887 - val_precision: 0.0971 - val_recall: 0.8571 - val_auc: 0.9742 - val_prc: 0.7364\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 4.6242e-04 - accuracy: 0.9895 - precision: 0.1305 - recall: 0.8700 - auc: 0.9814 - prc: 0.7576 - val_loss: 0.1448 - val_accuracy: 0.9887 - val_precision: 0.0986 - val_recall: 0.8730 - val_auc: 0.9743 - val_prc: 0.7366\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 4.4776e-04 - accuracy: 0.9893 - precision: 0.1298 - recall: 0.8762 - auc: 0.9818 - prc: 0.7579 - val_loss: 0.1388 - val_accuracy: 0.9877 - val_precision: 0.0909 - val_recall: 0.8730 - val_auc: 0.9744 - val_prc: 0.7425\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 4.3430e-04 - accuracy: 0.9883 - precision: 0.1193 - recall: 0.8731 - auc: 0.9832 - prc: 0.7636 - val_loss: 0.1343 - val_accuracy: 0.9868 - val_precision: 0.0863 - val_recall: 0.8889 - val_auc: 0.9739 - val_prc: 0.7289\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.2576e-04 - accuracy: 0.9876 - precision: 0.1145 - recall: 0.8854 - auc: 0.9836 - prc: 0.7502 - val_loss: 0.1289 - val_accuracy: 0.9864 - val_precision: 0.0842 - val_recall: 0.8889 - val_auc: 0.9740 - val_prc: 0.7151\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 4.1528e-04 - accuracy: 0.9866 - precision: 0.1070 - recall: 0.8885 - auc: 0.9845 - prc: 0.7401 - val_loss: 0.1238 - val_accuracy: 0.9862 - val_precision: 0.0827 - val_recall: 0.8889 - val_auc: 0.9741 - val_prc: 0.7130\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.0571e-04 - accuracy: 0.9866 - precision: 0.1071 - recall: 0.8885 - auc: 0.9857 - prc: 0.7454 - val_loss: 0.1210 - val_accuracy: 0.9855 - val_precision: 0.0803 - val_recall: 0.9048 - val_auc: 0.9737 - val_prc: 0.7162\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.9956e-04 - accuracy: 0.9866 - precision: 0.1067 - recall: 0.8885 - auc: 0.9860 - prc: 0.7255 - val_loss: 0.1215 - val_accuracy: 0.9845 - val_precision: 0.0755 - val_recall: 0.9048 - val_auc: 0.9735 - val_prc: 0.6771\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.9313e-04 - accuracy: 0.9853 - precision: 0.0985 - recall: 0.8885 - auc: 0.9857 - prc: 0.7176 - val_loss: 0.1161 - val_accuracy: 0.9850 - val_precision: 0.0780 - val_recall: 0.9048 - val_auc: 0.9734 - val_prc: 0.6537\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.9422e-04 - accuracy: 0.9849 - precision: 0.0963 - recall: 0.8947 - auc: 0.9850 - prc: 0.7120 - val_loss: 0.1139 - val_accuracy: 0.9848 - val_precision: 0.0769 - val_recall: 0.9048 - val_auc: 0.9734 - val_prc: 0.6534\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.8257e-04 - accuracy: 0.9859 - precision: 0.1030 - recall: 0.8978 - auc: 0.9868 - prc: 0.7202 - val_loss: 0.1186 - val_accuracy: 0.9832 - val_precision: 0.0699 - val_recall: 0.9048 - val_auc: 0.9736 - val_prc: 0.6329\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.8090e-04 - accuracy: 0.9836 - precision: 0.0898 - recall: 0.8978 - auc: 0.9864 - prc: 0.6997 - val_loss: 0.1113 - val_accuracy: 0.9843 - val_precision: 0.0747 - val_recall: 0.9048 - val_auc: 0.9730 - val_prc: 0.6334\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.8450e-04 - accuracy: 0.9858 - precision: 0.1021 - recall: 0.8978 - auc: 0.9856 - prc: 0.7050 - val_loss: 0.1196 - val_accuracy: 0.9817 - val_precision: 0.0648 - val_recall: 0.9048 - val_auc: 0.9730 - val_prc: 0.6170\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.8118e-04 - accuracy: 0.9824 - precision: 0.0841 - recall: 0.8978 - auc: 0.9863 - prc: 0.6976 - val_loss: 0.1124 - val_accuracy: 0.9830 - val_precision: 0.0694 - val_recall: 0.9048 - val_auc: 0.9734 - val_prc: 0.6207\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.11999999999999998\n",
            "****************************** w =  3\n",
            "Model: \"sequential_59\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_59 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_59 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_59 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_59 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89/89 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.9204 - precision: 0.0184 - recall: 0.8756 - auc: 0.9363 - prc: 0.5844"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 16ms/step - loss: 0.0013 - accuracy: 0.9204 - precision: 0.0184 - recall: 0.8756 - auc: 0.9363 - prc: 0.5844 - val_loss: 0.4215 - val_accuracy: 0.9940 - val_precision: 0.1667 - val_recall: 0.8254 - val_auc: 0.9633 - val_prc: 0.6910\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 8.6544e-04 - accuracy: 0.9950 - precision: 0.2427 - recall: 0.8483 - auc: 0.9521 - prc: 0.7142 - val_loss: 0.2614 - val_accuracy: 0.9974 - val_precision: 0.3270 - val_recall: 0.8254 - val_auc: 0.9645 - val_prc: 0.7031\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 6.9933e-04 - accuracy: 0.9969 - precision: 0.3460 - recall: 0.8483 - auc: 0.9552 - prc: 0.7302 - val_loss: 0.1959 - val_accuracy: 0.9957 - val_precision: 0.2203 - val_recall: 0.8254 - val_auc: 0.9666 - val_prc: 0.7228\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 6.0734e-04 - accuracy: 0.9955 - precision: 0.2623 - recall: 0.8576 - auc: 0.9671 - prc: 0.7466 - val_loss: 0.1501 - val_accuracy: 0.9955 - val_precision: 0.2122 - val_recall: 0.8254 - val_auc: 0.9687 - val_prc: 0.7310\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 5.5968e-04 - accuracy: 0.9944 - precision: 0.2231 - recall: 0.8669 - auc: 0.9694 - prc: 0.7547 - val_loss: 0.1293 - val_accuracy: 0.9946 - val_precision: 0.1834 - val_recall: 0.8413 - val_auc: 0.9722 - val_prc: 0.7328\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 5.2875e-04 - accuracy: 0.9941 - precision: 0.2121 - recall: 0.8607 - auc: 0.9736 - prc: 0.7564 - val_loss: 0.1266 - val_accuracy: 0.9923 - val_precision: 0.1374 - val_recall: 0.8571 - val_auc: 0.9735 - val_prc: 0.7213\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 5.0180e-04 - accuracy: 0.9924 - precision: 0.1729 - recall: 0.8669 - auc: 0.9795 - prc: 0.7563 - val_loss: 0.1162 - val_accuracy: 0.9918 - val_precision: 0.1292 - val_recall: 0.8571 - val_auc: 0.9742 - val_prc: 0.7378\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 4.8641e-04 - accuracy: 0.9914 - precision: 0.1563 - recall: 0.8669 - auc: 0.9788 - prc: 0.7577 - val_loss: 0.1059 - val_accuracy: 0.9919 - val_precision: 0.1308 - val_recall: 0.8571 - val_auc: 0.9742 - val_prc: 0.7402\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 4.7264e-04 - accuracy: 0.9912 - precision: 0.1522 - recall: 0.8700 - auc: 0.9829 - prc: 0.7497 - val_loss: 0.1005 - val_accuracy: 0.9915 - val_precision: 0.1247 - val_recall: 0.8571 - val_auc: 0.9739 - val_prc: 0.7303\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.6143e-04 - accuracy: 0.9913 - precision: 0.1538 - recall: 0.8669 - auc: 0.9821 - prc: 0.7555 - val_loss: 0.1017 - val_accuracy: 0.9899 - val_precision: 0.1087 - val_recall: 0.8730 - val_auc: 0.9744 - val_prc: 0.7295\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 4.4810e-04 - accuracy: 0.9909 - precision: 0.1490 - recall: 0.8793 - auc: 0.9830 - prc: 0.7423 - val_loss: 0.1073 - val_accuracy: 0.9880 - val_precision: 0.0941 - val_recall: 0.8889 - val_auc: 0.9739 - val_prc: 0.7145\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.4226e-04 - accuracy: 0.9884 - precision: 0.1223 - recall: 0.8916 - auc: 0.9844 - prc: 0.7364 - val_loss: 0.0934 - val_accuracy: 0.9897 - val_precision: 0.1070 - val_recall: 0.8730 - val_auc: 0.9738 - val_prc: 0.7273\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.2985e-04 - accuracy: 0.9896 - precision: 0.1342 - recall: 0.8885 - auc: 0.9851 - prc: 0.7475 - val_loss: 0.0903 - val_accuracy: 0.9896 - val_precision: 0.1075 - val_recall: 0.8889 - val_auc: 0.9738 - val_prc: 0.7157\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 4.2920e-04 - accuracy: 0.9897 - precision: 0.1343 - recall: 0.8793 - auc: 0.9850 - prc: 0.7339 - val_loss: 0.0934 - val_accuracy: 0.9885 - val_precision: 0.0977 - val_recall: 0.8889 - val_auc: 0.9732 - val_prc: 0.7108\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 4.1859e-04 - accuracy: 0.9886 - precision: 0.1231 - recall: 0.8854 - auc: 0.9858 - prc: 0.7207 - val_loss: 0.0892 - val_accuracy: 0.9887 - val_precision: 0.0998 - val_recall: 0.8889 - val_auc: 0.9734 - val_prc: 0.6963\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 4.1388e-04 - accuracy: 0.9889 - precision: 0.1264 - recall: 0.8885 - auc: 0.9863 - prc: 0.7315 - val_loss: 0.0900 - val_accuracy: 0.9880 - val_precision: 0.0946 - val_recall: 0.8889 - val_auc: 0.9734 - val_prc: 0.6838\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.1368e-04 - accuracy: 0.9889 - precision: 0.1270 - recall: 0.8916 - auc: 0.9854 - prc: 0.7199 - val_loss: 0.0924 - val_accuracy: 0.9874 - val_precision: 0.0905 - val_recall: 0.8889 - val_auc: 0.9734 - val_prc: 0.6523\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.0914e-04 - accuracy: 0.9881 - precision: 0.1191 - recall: 0.8916 - auc: 0.9859 - prc: 0.7174 - val_loss: 0.0901 - val_accuracy: 0.9875 - val_precision: 0.0911 - val_recall: 0.8889 - val_auc: 0.9732 - val_prc: 0.6425\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.0773e-04 - accuracy: 0.9872 - precision: 0.1123 - recall: 0.8978 - auc: 0.9856 - prc: 0.7077 - val_loss: 0.0845 - val_accuracy: 0.9883 - val_precision: 0.0962 - val_recall: 0.8889 - val_auc: 0.9734 - val_prc: 0.6622\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 4.0608e-04 - accuracy: 0.9880 - precision: 0.1180 - recall: 0.8916 - auc: 0.9859 - prc: 0.7120 - val_loss: 0.0861 - val_accuracy: 0.9877 - val_precision: 0.0923 - val_recall: 0.8889 - val_auc: 0.9733 - val_prc: 0.6441\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.12999999999999998\n",
            "****************************** w =  3\n",
            "Model: \"sequential_60\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_60 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_60 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_60 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_60 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87/89 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 0.9532 - precision: 0.0306 - recall: 0.8654 - auc: 0.9325 - prc: 0.6077"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 16ms/step - loss: 0.0014 - accuracy: 0.9539 - precision: 0.0311 - recall: 0.8653 - auc: 0.9325 - prc: 0.6098 - val_loss: 0.3527 - val_accuracy: 0.9991 - val_precision: 0.6265 - val_recall: 0.8254 - val_auc: 0.9608 - val_prc: 0.7000\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.1285e-04 - accuracy: 0.9989 - precision: 0.6626 - recall: 0.8328 - auc: 0.9402 - prc: 0.7182 - val_loss: 0.2003 - val_accuracy: 0.9991 - val_precision: 0.6265 - val_recall: 0.8254 - val_auc: 0.9647 - val_prc: 0.7240\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 7.2116e-04 - accuracy: 0.9987 - precision: 0.5922 - recall: 0.8452 - auc: 0.9527 - prc: 0.7318 - val_loss: 0.1464 - val_accuracy: 0.9980 - val_precision: 0.3881 - val_recall: 0.8254 - val_auc: 0.9668 - val_prc: 0.7276\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 6.2961e-04 - accuracy: 0.9971 - precision: 0.3614 - recall: 0.8514 - auc: 0.9631 - prc: 0.7496 - val_loss: 0.1114 - val_accuracy: 0.9977 - val_precision: 0.3611 - val_recall: 0.8254 - val_auc: 0.9705 - val_prc: 0.7358\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 5.8302e-04 - accuracy: 0.9972 - precision: 0.3723 - recall: 0.8483 - auc: 0.9681 - prc: 0.7518 - val_loss: 0.1046 - val_accuracy: 0.9958 - val_precision: 0.2261 - val_recall: 0.8254 - val_auc: 0.9732 - val_prc: 0.7397\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 5.5383e-04 - accuracy: 0.9951 - precision: 0.2465 - recall: 0.8607 - auc: 0.9743 - prc: 0.7550 - val_loss: 0.0936 - val_accuracy: 0.9953 - val_precision: 0.2054 - val_recall: 0.8413 - val_auc: 0.9747 - val_prc: 0.7398\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.3082e-04 - accuracy: 0.9949 - precision: 0.2388 - recall: 0.8607 - auc: 0.9756 - prc: 0.7583 - val_loss: 0.0904 - val_accuracy: 0.9941 - val_precision: 0.1710 - val_recall: 0.8413 - val_auc: 0.9753 - val_prc: 0.7403\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.0758e-04 - accuracy: 0.9935 - precision: 0.1991 - recall: 0.8700 - auc: 0.9806 - prc: 0.7589 - val_loss: 0.0832 - val_accuracy: 0.9940 - val_precision: 0.1683 - val_recall: 0.8413 - val_auc: 0.9753 - val_prc: 0.7413\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 4.9241e-04 - accuracy: 0.9938 - precision: 0.2059 - recall: 0.8669 - auc: 0.9824 - prc: 0.7642 - val_loss: 0.0837 - val_accuracy: 0.9926 - val_precision: 0.1395 - val_recall: 0.8413 - val_auc: 0.9750 - val_prc: 0.7418\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 4.8055e-04 - accuracy: 0.9930 - precision: 0.1852 - recall: 0.8669 - auc: 0.9814 - prc: 0.7583 - val_loss: 0.0825 - val_accuracy: 0.9919 - val_precision: 0.1311 - val_recall: 0.8571 - val_auc: 0.9751 - val_prc: 0.7335\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 4.7485e-04 - accuracy: 0.9921 - precision: 0.1676 - recall: 0.8700 - auc: 0.9815 - prc: 0.7469 - val_loss: 0.0773 - val_accuracy: 0.9925 - val_precision: 0.1377 - val_recall: 0.8413 - val_auc: 0.9747 - val_prc: 0.7330\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.6410e-04 - accuracy: 0.9925 - precision: 0.1761 - recall: 0.8731 - auc: 0.9839 - prc: 0.7543 - val_loss: 0.0788 - val_accuracy: 0.9913 - val_precision: 0.1227 - val_recall: 0.8571 - val_auc: 0.9748 - val_prc: 0.7199\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 4.5233e-04 - accuracy: 0.9912 - precision: 0.1548 - recall: 0.8793 - auc: 0.9854 - prc: 0.7464 - val_loss: 0.0689 - val_accuracy: 0.9930 - val_precision: 0.1468 - val_recall: 0.8413 - val_auc: 0.9744 - val_prc: 0.7194\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.4555e-04 - accuracy: 0.9925 - precision: 0.1753 - recall: 0.8731 - auc: 0.9851 - prc: 0.7407 - val_loss: 0.0745 - val_accuracy: 0.9913 - val_precision: 0.1250 - val_recall: 0.8730 - val_auc: 0.9745 - val_prc: 0.7195\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.3715e-04 - accuracy: 0.9914 - precision: 0.1587 - recall: 0.8854 - auc: 0.9861 - prc: 0.7320 - val_loss: 0.0716 - val_accuracy: 0.9916 - val_precision: 0.1302 - val_recall: 0.8889 - val_auc: 0.9743 - val_prc: 0.7192\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.4558e-04 - accuracy: 0.9910 - precision: 0.1507 - recall: 0.8793 - auc: 0.9845 - prc: 0.7325 - val_loss: 0.0697 - val_accuracy: 0.9917 - val_precision: 0.1318 - val_recall: 0.8889 - val_auc: 0.9742 - val_prc: 0.7218\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 4.3242e-04 - accuracy: 0.9919 - precision: 0.1651 - recall: 0.8824 - auc: 0.9860 - prc: 0.7395 - val_loss: 0.0737 - val_accuracy: 0.9898 - val_precision: 0.1094 - val_recall: 0.8889 - val_auc: 0.9745 - val_prc: 0.6963\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 4.3719e-04 - accuracy: 0.9908 - precision: 0.1492 - recall: 0.8824 - auc: 0.9849 - prc: 0.7227 - val_loss: 0.0698 - val_accuracy: 0.9911 - val_precision: 0.1236 - val_recall: 0.8889 - val_auc: 0.9736 - val_prc: 0.6955\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.2722e-04 - accuracy: 0.9910 - precision: 0.1515 - recall: 0.8885 - auc: 0.9863 - prc: 0.7255 - val_loss: 0.0702 - val_accuracy: 0.9905 - val_precision: 0.1172 - val_recall: 0.8889 - val_auc: 0.9737 - val_prc: 0.6965\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 4.3043e-04 - accuracy: 0.9902 - precision: 0.1419 - recall: 0.8885 - auc: 0.9857 - prc: 0.7269 - val_loss: 0.0662 - val_accuracy: 0.9917 - val_precision: 0.1315 - val_recall: 0.8889 - val_auc: 0.9736 - val_prc: 0.6970\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.13999999999999999\n",
            "****************************** w =  3\n",
            "Model: \"sequential_61\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_61 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_61 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_61 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_61 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "84/89 [===========================>..] - ETA: 0s - loss: 0.0016 - accuracy: 0.9699 - precision: 0.0458 - recall: 0.8249 - auc: 0.9238 - prc: 0.6053"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 16ms/step - loss: 0.0016 - accuracy: 0.9711 - precision: 0.0465 - recall: 0.8187 - auc: 0.9223 - prc: 0.6037 - val_loss: 0.2775 - val_accuracy: 0.9994 - val_precision: 0.7812 - val_recall: 0.7937 - val_auc: 0.9629 - val_prc: 0.7083\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.2835e-04 - accuracy: 0.9993 - precision: 0.8062 - recall: 0.8111 - auc: 0.9381 - prc: 0.7229 - val_loss: 0.1542 - val_accuracy: 0.9992 - val_precision: 0.6711 - val_recall: 0.8095 - val_auc: 0.9669 - val_prc: 0.7266\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.2164e-04 - accuracy: 0.9991 - precision: 0.7076 - recall: 0.8390 - auc: 0.9582 - prc: 0.7398 - val_loss: 0.1081 - val_accuracy: 0.9989 - val_precision: 0.5652 - val_recall: 0.8254 - val_auc: 0.9676 - val_prc: 0.7341\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 6.4192e-04 - accuracy: 0.9983 - precision: 0.5180 - recall: 0.8483 - auc: 0.9622 - prc: 0.7509 - val_loss: 0.0905 - val_accuracy: 0.9978 - val_precision: 0.3741 - val_recall: 0.8254 - val_auc: 0.9705 - val_prc: 0.7415\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.9828e-04 - accuracy: 0.9974 - precision: 0.3965 - recall: 0.8483 - auc: 0.9651 - prc: 0.7532 - val_loss: 0.0793 - val_accuracy: 0.9970 - val_precision: 0.2905 - val_recall: 0.8254 - val_auc: 0.9729 - val_prc: 0.7417\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.6274e-04 - accuracy: 0.9963 - precision: 0.3051 - recall: 0.8576 - auc: 0.9718 - prc: 0.7639 - val_loss: 0.0703 - val_accuracy: 0.9967 - val_precision: 0.2751 - val_recall: 0.8254 - val_auc: 0.9750 - val_prc: 0.7426\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 5.4270e-04 - accuracy: 0.9959 - precision: 0.2854 - recall: 0.8607 - auc: 0.9744 - prc: 0.7633 - val_loss: 0.0671 - val_accuracy: 0.9960 - val_precision: 0.2366 - val_recall: 0.8413 - val_auc: 0.9751 - val_prc: 0.7443\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 5.2793e-04 - accuracy: 0.9956 - precision: 0.2679 - recall: 0.8669 - auc: 0.9783 - prc: 0.7609 - val_loss: 0.0645 - val_accuracy: 0.9955 - val_precision: 0.2137 - val_recall: 0.8413 - val_auc: 0.9759 - val_prc: 0.7472\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 5.1133e-04 - accuracy: 0.9956 - precision: 0.2714 - recall: 0.8638 - auc: 0.9808 - prc: 0.7638 - val_loss: 0.0690 - val_accuracy: 0.9941 - val_precision: 0.1699 - val_recall: 0.8413 - val_auc: 0.9746 - val_prc: 0.7339\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 4.9913e-04 - accuracy: 0.9945 - precision: 0.2253 - recall: 0.8669 - auc: 0.9821 - prc: 0.7600 - val_loss: 0.0661 - val_accuracy: 0.9941 - val_precision: 0.1693 - val_recall: 0.8413 - val_auc: 0.9748 - val_prc: 0.7356\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 4.8710e-04 - accuracy: 0.9943 - precision: 0.2222 - recall: 0.8731 - auc: 0.9842 - prc: 0.7597 - val_loss: 0.0639 - val_accuracy: 0.9940 - val_precision: 0.1672 - val_recall: 0.8413 - val_auc: 0.9746 - val_prc: 0.7355\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.8364e-04 - accuracy: 0.9940 - precision: 0.2134 - recall: 0.8793 - auc: 0.9834 - prc: 0.7604 - val_loss: 0.0593 - val_accuracy: 0.9943 - val_precision: 0.1749 - val_recall: 0.8413 - val_auc: 0.9749 - val_prc: 0.7372\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 4.7757e-04 - accuracy: 0.9941 - precision: 0.2148 - recall: 0.8731 - auc: 0.9842 - prc: 0.7581 - val_loss: 0.0607 - val_accuracy: 0.9937 - val_precision: 0.1606 - val_recall: 0.8413 - val_auc: 0.9750 - val_prc: 0.7382\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.7183e-04 - accuracy: 0.9936 - precision: 0.2026 - recall: 0.8793 - auc: 0.9847 - prc: 0.7561 - val_loss: 0.0581 - val_accuracy: 0.9938 - val_precision: 0.1631 - val_recall: 0.8413 - val_auc: 0.9749 - val_prc: 0.7371\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.6926e-04 - accuracy: 0.9935 - precision: 0.1999 - recall: 0.8762 - auc: 0.9851 - prc: 0.7575 - val_loss: 0.0585 - val_accuracy: 0.9934 - val_precision: 0.1541 - val_recall: 0.8413 - val_auc: 0.9737 - val_prc: 0.7357\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.6070e-04 - accuracy: 0.9932 - precision: 0.1920 - recall: 0.8793 - auc: 0.9852 - prc: 0.7507 - val_loss: 0.0560 - val_accuracy: 0.9936 - val_precision: 0.1582 - val_recall: 0.8413 - val_auc: 0.9745 - val_prc: 0.7224\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.6288e-04 - accuracy: 0.9934 - precision: 0.1975 - recall: 0.8854 - auc: 0.9852 - prc: 0.7538 - val_loss: 0.0565 - val_accuracy: 0.9934 - val_precision: 0.1541 - val_recall: 0.8413 - val_auc: 0.9738 - val_prc: 0.7223\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.4842e-04 - accuracy: 0.9932 - precision: 0.1921 - recall: 0.8854 - auc: 0.9865 - prc: 0.7471 - val_loss: 0.0532 - val_accuracy: 0.9938 - val_precision: 0.1636 - val_recall: 0.8413 - val_auc: 0.9744 - val_prc: 0.7214\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.5424e-04 - accuracy: 0.9936 - precision: 0.2031 - recall: 0.8854 - auc: 0.9857 - prc: 0.7479 - val_loss: 0.0594 - val_accuracy: 0.9925 - val_precision: 0.1425 - val_recall: 0.8730 - val_auc: 0.9736 - val_prc: 0.7255\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.4585e-04 - accuracy: 0.9925 - precision: 0.1780 - recall: 0.8885 - auc: 0.9866 - prc: 0.7454 - val_loss: 0.0518 - val_accuracy: 0.9939 - val_precision: 0.1646 - val_recall: 0.8413 - val_auc: 0.9740 - val_prc: 0.7249\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.15\n",
            "****************************** w =  3\n",
            "Model: \"sequential_62\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_62 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_62 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_62 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_62 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "84/89 [===========================>..] - ETA: 0s - loss: 0.0018 - accuracy: 0.9792 - precision: 0.0603 - recall: 0.7879 - auc: 0.9156 - prc: 0.6044"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 16ms/step - loss: 0.0018 - accuracy: 0.9800 - precision: 0.0638 - recall: 0.7876 - auc: 0.9151 - prc: 0.6118 - val_loss: 0.2300 - val_accuracy: 0.9994 - val_precision: 0.7812 - val_recall: 0.7937 - val_auc: 0.9601 - val_prc: 0.6892\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.5529e-04 - accuracy: 0.9993 - precision: 0.8000 - recall: 0.8050 - auc: 0.9438 - prc: 0.7140 - val_loss: 0.1111 - val_accuracy: 0.9993 - val_precision: 0.7083 - val_recall: 0.8095 - val_auc: 0.9660 - val_prc: 0.7045\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.4289e-04 - accuracy: 0.9992 - precision: 0.7458 - recall: 0.8266 - auc: 0.9513 - prc: 0.7359 - val_loss: 0.0828 - val_accuracy: 0.9991 - val_precision: 0.6265 - val_recall: 0.8254 - val_auc: 0.9664 - val_prc: 0.7234\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 6.5371e-04 - accuracy: 0.9984 - precision: 0.5385 - recall: 0.8452 - auc: 0.9621 - prc: 0.7484 - val_loss: 0.0691 - val_accuracy: 0.9986 - val_precision: 0.4952 - val_recall: 0.8254 - val_auc: 0.9693 - val_prc: 0.7352\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 6.1668e-04 - accuracy: 0.9981 - precision: 0.4815 - recall: 0.8452 - auc: 0.9674 - prc: 0.7552 - val_loss: 0.0665 - val_accuracy: 0.9974 - val_precision: 0.3270 - val_recall: 0.8254 - val_auc: 0.9722 - val_prc: 0.7115\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 5.8364e-04 - accuracy: 0.9971 - precision: 0.3692 - recall: 0.8607 - auc: 0.9726 - prc: 0.7589 - val_loss: 0.0622 - val_accuracy: 0.9967 - val_precision: 0.2708 - val_recall: 0.8254 - val_auc: 0.9734 - val_prc: 0.7125\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 5.6512e-04 - accuracy: 0.9967 - precision: 0.3310 - recall: 0.8638 - auc: 0.9754 - prc: 0.7603 - val_loss: 0.0568 - val_accuracy: 0.9966 - val_precision: 0.2690 - val_recall: 0.8413 - val_auc: 0.9745 - val_prc: 0.7245\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 5.3797e-04 - accuracy: 0.9963 - precision: 0.3059 - recall: 0.8638 - auc: 0.9799 - prc: 0.7605 - val_loss: 0.0502 - val_accuracy: 0.9971 - val_precision: 0.3041 - val_recall: 0.8254 - val_auc: 0.9748 - val_prc: 0.7392\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 5.2564e-04 - accuracy: 0.9966 - precision: 0.3306 - recall: 0.8638 - auc: 0.9805 - prc: 0.7633 - val_loss: 0.0567 - val_accuracy: 0.9952 - val_precision: 0.2046 - val_recall: 0.8413 - val_auc: 0.9752 - val_prc: 0.7161\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 5.1553e-04 - accuracy: 0.9959 - precision: 0.2828 - recall: 0.8669 - auc: 0.9821 - prc: 0.7613 - val_loss: 0.0547 - val_accuracy: 0.9952 - val_precision: 0.2031 - val_recall: 0.8413 - val_auc: 0.9739 - val_prc: 0.7296\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 5.0719e-04 - accuracy: 0.9956 - precision: 0.2719 - recall: 0.8638 - auc: 0.9832 - prc: 0.7562 - val_loss: 0.0547 - val_accuracy: 0.9948 - val_precision: 0.1906 - val_recall: 0.8413 - val_auc: 0.9746 - val_prc: 0.7165\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 4.9739e-04 - accuracy: 0.9946 - precision: 0.2313 - recall: 0.8793 - auc: 0.9838 - prc: 0.7534 - val_loss: 0.0451 - val_accuracy: 0.9963 - val_precision: 0.2512 - val_recall: 0.8413 - val_auc: 0.9731 - val_prc: 0.7326\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.9653e-04 - accuracy: 0.9957 - precision: 0.2733 - recall: 0.8638 - auc: 0.9835 - prc: 0.7521 - val_loss: 0.0498 - val_accuracy: 0.9951 - val_precision: 0.2000 - val_recall: 0.8413 - val_auc: 0.9737 - val_prc: 0.7189\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 4.8178e-04 - accuracy: 0.9954 - precision: 0.2611 - recall: 0.8731 - auc: 0.9858 - prc: 0.7497 - val_loss: 0.0505 - val_accuracy: 0.9947 - val_precision: 0.1901 - val_recall: 0.8571 - val_auc: 0.9739 - val_prc: 0.7238\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 4.7726e-04 - accuracy: 0.9950 - precision: 0.2448 - recall: 0.8793 - auc: 0.9848 - prc: 0.7545 - val_loss: 0.0477 - val_accuracy: 0.9950 - val_precision: 0.2000 - val_recall: 0.8571 - val_auc: 0.9736 - val_prc: 0.7220\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.7479e-04 - accuracy: 0.9950 - precision: 0.2453 - recall: 0.8793 - auc: 0.9856 - prc: 0.7553 - val_loss: 0.0458 - val_accuracy: 0.9952 - val_precision: 0.2061 - val_recall: 0.8571 - val_auc: 0.9742 - val_prc: 0.7223\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 4.7150e-04 - accuracy: 0.9950 - precision: 0.2465 - recall: 0.8762 - auc: 0.9860 - prc: 0.7465 - val_loss: 0.0488 - val_accuracy: 0.9946 - val_precision: 0.1849 - val_recall: 0.8571 - val_auc: 0.9737 - val_prc: 0.7211\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 4.6037e-04 - accuracy: 0.9951 - precision: 0.2522 - recall: 0.8885 - auc: 0.9857 - prc: 0.7486 - val_loss: 0.0492 - val_accuracy: 0.9942 - val_precision: 0.1748 - val_recall: 0.8571 - val_auc: 0.9740 - val_prc: 0.7205\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.6543e-04 - accuracy: 0.9943 - precision: 0.2209 - recall: 0.8824 - auc: 0.9863 - prc: 0.7432 - val_loss: 0.0428 - val_accuracy: 0.9953 - val_precision: 0.2101 - val_recall: 0.8571 - val_auc: 0.9745 - val_prc: 0.7335\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.6314e-04 - accuracy: 0.9946 - precision: 0.2334 - recall: 0.8824 - auc: 0.9869 - prc: 0.7490 - val_loss: 0.0457 - val_accuracy: 0.9945 - val_precision: 0.1831 - val_recall: 0.8571 - val_auc: 0.9740 - val_prc: 0.7220\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.16\n",
            "****************************** w =  3\n",
            "Model: \"sequential_63\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_63 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_63 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_63 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_63 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "84/89 [===========================>..] - ETA: 0s - loss: 0.0020 - accuracy: 0.9752 - precision: 0.0520 - recall: 0.7892 - auc: 0.9125 - prc: 0.6119"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 16ms/step - loss: 0.0020 - accuracy: 0.9762 - precision: 0.0540 - recall: 0.7876 - auc: 0.9120 - prc: 0.6130 - val_loss: 0.1911 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9531 - val_prc: 0.6769\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.7276e-04 - accuracy: 0.9994 - precision: 0.8449 - recall: 0.7926 - auc: 0.9355 - prc: 0.7119 - val_loss: 0.0899 - val_accuracy: 0.9993 - val_precision: 0.7463 - val_recall: 0.7937 - val_auc: 0.9669 - val_prc: 0.7139\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 7.5539e-04 - accuracy: 0.9993 - precision: 0.7739 - recall: 0.8266 - auc: 0.9506 - prc: 0.7400 - val_loss: 0.0626 - val_accuracy: 0.9993 - val_precision: 0.6986 - val_recall: 0.8095 - val_auc: 0.9707 - val_prc: 0.7149\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 6.7037e-04 - accuracy: 0.9991 - precision: 0.7071 - recall: 0.8297 - auc: 0.9632 - prc: 0.7516 - val_loss: 0.0546 - val_accuracy: 0.9990 - val_precision: 0.6118 - val_recall: 0.8254 - val_auc: 0.9746 - val_prc: 0.7123\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 6.2442e-04 - accuracy: 0.9988 - precision: 0.6190 - recall: 0.8452 - auc: 0.9703 - prc: 0.7578 - val_loss: 0.0551 - val_accuracy: 0.9981 - val_precision: 0.4160 - val_recall: 0.8254 - val_auc: 0.9752 - val_prc: 0.7156\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.9982e-04 - accuracy: 0.9983 - precision: 0.5140 - recall: 0.8545 - auc: 0.9721 - prc: 0.7582 - val_loss: 0.0566 - val_accuracy: 0.9970 - val_precision: 0.2978 - val_recall: 0.8413 - val_auc: 0.9760 - val_prc: 0.7066\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 5.8098e-04 - accuracy: 0.9971 - precision: 0.3676 - recall: 0.8638 - auc: 0.9758 - prc: 0.7611 - val_loss: 0.0439 - val_accuracy: 0.9979 - val_precision: 0.3824 - val_recall: 0.8254 - val_auc: 0.9759 - val_prc: 0.7048\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.6015e-04 - accuracy: 0.9975 - precision: 0.4043 - recall: 0.8638 - auc: 0.9787 - prc: 0.7621 - val_loss: 0.0449 - val_accuracy: 0.9974 - val_precision: 0.3292 - val_recall: 0.8413 - val_auc: 0.9749 - val_prc: 0.7058\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 5.5297e-04 - accuracy: 0.9972 - precision: 0.3725 - recall: 0.8638 - auc: 0.9783 - prc: 0.7568 - val_loss: 0.0440 - val_accuracy: 0.9969 - val_precision: 0.2912 - val_recall: 0.8413 - val_auc: 0.9754 - val_prc: 0.7053\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 5.3945e-04 - accuracy: 0.9972 - precision: 0.3740 - recall: 0.8638 - auc: 0.9808 - prc: 0.7578 - val_loss: 0.0487 - val_accuracy: 0.9957 - val_precision: 0.2236 - val_recall: 0.8413 - val_auc: 0.9746 - val_prc: 0.7073\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 5.2286e-04 - accuracy: 0.9960 - precision: 0.2900 - recall: 0.8638 - auc: 0.9837 - prc: 0.7523 - val_loss: 0.0396 - val_accuracy: 0.9969 - val_precision: 0.2896 - val_recall: 0.8413 - val_auc: 0.9758 - val_prc: 0.7314\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.1865e-04 - accuracy: 0.9971 - precision: 0.3666 - recall: 0.8638 - auc: 0.9833 - prc: 0.7637 - val_loss: 0.0440 - val_accuracy: 0.9958 - val_precision: 0.2255 - val_recall: 0.8413 - val_auc: 0.9750 - val_prc: 0.7204\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.0872e-04 - accuracy: 0.9959 - precision: 0.2838 - recall: 0.8638 - auc: 0.9828 - prc: 0.7480 - val_loss: 0.0394 - val_accuracy: 0.9963 - val_precision: 0.2524 - val_recall: 0.8413 - val_auc: 0.9754 - val_prc: 0.7216\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.0240e-04 - accuracy: 0.9963 - precision: 0.3098 - recall: 0.8700 - auc: 0.9847 - prc: 0.7589 - val_loss: 0.0397 - val_accuracy: 0.9960 - val_precision: 0.2377 - val_recall: 0.8413 - val_auc: 0.9753 - val_prc: 0.7235\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.0923e-04 - accuracy: 0.9964 - precision: 0.3163 - recall: 0.8638 - auc: 0.9834 - prc: 0.7583 - val_loss: 0.0417 - val_accuracy: 0.9956 - val_precision: 0.2190 - val_recall: 0.8413 - val_auc: 0.9737 - val_prc: 0.7248\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 5.0162e-04 - accuracy: 0.9958 - precision: 0.2799 - recall: 0.8700 - auc: 0.9851 - prc: 0.7491 - val_loss: 0.0400 - val_accuracy: 0.9957 - val_precision: 0.2246 - val_recall: 0.8413 - val_auc: 0.9740 - val_prc: 0.7239\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 4.9378e-04 - accuracy: 0.9959 - precision: 0.2836 - recall: 0.8700 - auc: 0.9857 - prc: 0.7552 - val_loss: 0.0396 - val_accuracy: 0.9956 - val_precision: 0.2181 - val_recall: 0.8413 - val_auc: 0.9743 - val_prc: 0.7235\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 4.9100e-04 - accuracy: 0.9956 - precision: 0.2712 - recall: 0.8700 - auc: 0.9852 - prc: 0.7566 - val_loss: 0.0367 - val_accuracy: 0.9960 - val_precision: 0.2345 - val_recall: 0.8413 - val_auc: 0.9737 - val_prc: 0.7235\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 4.8902e-04 - accuracy: 0.9957 - precision: 0.2784 - recall: 0.8731 - auc: 0.9851 - prc: 0.7535 - val_loss: 0.0368 - val_accuracy: 0.9958 - val_precision: 0.2265 - val_recall: 0.8413 - val_auc: 0.9739 - val_prc: 0.7232\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 2s 22ms/step - loss: 4.8370e-04 - accuracy: 0.9955 - precision: 0.2676 - recall: 0.8731 - auc: 0.9843 - prc: 0.7570 - val_loss: 0.0331 - val_accuracy: 0.9965 - val_precision: 0.2624 - val_recall: 0.8413 - val_auc: 0.9747 - val_prc: 0.7246\n",
            "1419/1419 [==============================] - 9s 6ms/step\n",
            "****************************** w =  0.17\n",
            "****************************** w =  3\n",
            "Model: \"sequential_64\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_64 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_64 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_64 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_64 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88/89 [============================>.] - ETA: 0s - loss: 0.0021 - accuracy: 0.9823 - precision: 0.0716 - recall: 0.7839 - auc: 0.9160 - prc: 0.6259"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 6s 35ms/step - loss: 0.0021 - accuracy: 0.9824 - precision: 0.0721 - recall: 0.7850 - auc: 0.9167 - prc: 0.6272 - val_loss: 0.1415 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9534 - val_prc: 0.6833\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 2s 19ms/step - loss: 9.6617e-04 - accuracy: 0.9994 - precision: 0.8482 - recall: 0.7957 - auc: 0.9339 - prc: 0.7180 - val_loss: 0.0735 - val_accuracy: 0.9993 - val_precision: 0.7463 - val_recall: 0.7937 - val_auc: 0.9631 - val_prc: 0.6940\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 7.6775e-04 - accuracy: 0.9993 - precision: 0.7857 - recall: 0.8173 - auc: 0.9481 - prc: 0.7388 - val_loss: 0.0553 - val_accuracy: 0.9991 - val_precision: 0.6538 - val_recall: 0.8095 - val_auc: 0.9641 - val_prc: 0.7068\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 2s 21ms/step - loss: 6.8976e-04 - accuracy: 0.9991 - precision: 0.7076 - recall: 0.8390 - auc: 0.9604 - prc: 0.7482 - val_loss: 0.0420 - val_accuracy: 0.9991 - val_precision: 0.6420 - val_recall: 0.8254 - val_auc: 0.9654 - val_prc: 0.7115\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 2s 21ms/step - loss: 6.4851e-04 - accuracy: 0.9991 - precision: 0.6949 - recall: 0.8390 - auc: 0.9653 - prc: 0.7528 - val_loss: 0.0440 - val_accuracy: 0.9988 - val_precision: 0.5417 - val_recall: 0.8254 - val_auc: 0.9709 - val_prc: 0.7168\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 6.1794e-04 - accuracy: 0.9984 - precision: 0.5342 - recall: 0.8452 - auc: 0.9724 - prc: 0.7527 - val_loss: 0.0397 - val_accuracy: 0.9987 - val_precision: 0.5200 - val_recall: 0.8254 - val_auc: 0.9719 - val_prc: 0.7159\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 2s 19ms/step - loss: 5.9950e-04 - accuracy: 0.9984 - precision: 0.5380 - recall: 0.8545 - auc: 0.9747 - prc: 0.7583 - val_loss: 0.0383 - val_accuracy: 0.9983 - val_precision: 0.4483 - val_recall: 0.8254 - val_auc: 0.9750 - val_prc: 0.7057\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 5.8004e-04 - accuracy: 0.9983 - precision: 0.5055 - recall: 0.8545 - auc: 0.9778 - prc: 0.7619 - val_loss: 0.0404 - val_accuracy: 0.9976 - val_precision: 0.3421 - val_recall: 0.8254 - val_auc: 0.9740 - val_prc: 0.7061\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 5.7291e-04 - accuracy: 0.9976 - precision: 0.4155 - recall: 0.8607 - auc: 0.9802 - prc: 0.7612 - val_loss: 0.0368 - val_accuracy: 0.9979 - val_precision: 0.3768 - val_recall: 0.8254 - val_auc: 0.9741 - val_prc: 0.7068\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 5.5664e-04 - accuracy: 0.9976 - precision: 0.4133 - recall: 0.8638 - auc: 0.9821 - prc: 0.7617 - val_loss: 0.0332 - val_accuracy: 0.9982 - val_precision: 0.4228 - val_recall: 0.8254 - val_auc: 0.9741 - val_prc: 0.7190\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 5.4417e-04 - accuracy: 0.9976 - precision: 0.4202 - recall: 0.8638 - auc: 0.9817 - prc: 0.7668 - val_loss: 0.0331 - val_accuracy: 0.9980 - val_precision: 0.3969 - val_recall: 0.8254 - val_auc: 0.9743 - val_prc: 0.7323\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 5.3663e-04 - accuracy: 0.9978 - precision: 0.4422 - recall: 0.8638 - auc: 0.9830 - prc: 0.7628 - val_loss: 0.0344 - val_accuracy: 0.9975 - val_precision: 0.3397 - val_recall: 0.8413 - val_auc: 0.9743 - val_prc: 0.7341\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 5.3148e-04 - accuracy: 0.9975 - precision: 0.4103 - recall: 0.8638 - auc: 0.9835 - prc: 0.7685 - val_loss: 0.0350 - val_accuracy: 0.9971 - val_precision: 0.3046 - val_recall: 0.8413 - val_auc: 0.9745 - val_prc: 0.7341\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 17ms/step - loss: 5.2651e-04 - accuracy: 0.9974 - precision: 0.3980 - recall: 0.8638 - auc: 0.9841 - prc: 0.7657 - val_loss: 0.0354 - val_accuracy: 0.9968 - val_precision: 0.2789 - val_recall: 0.8413 - val_auc: 0.9728 - val_prc: 0.7359\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 2s 18ms/step - loss: 5.1548e-04 - accuracy: 0.9969 - precision: 0.3470 - recall: 0.8638 - auc: 0.9851 - prc: 0.7601 - val_loss: 0.0312 - val_accuracy: 0.9975 - val_precision: 0.3376 - val_recall: 0.8413 - val_auc: 0.9740 - val_prc: 0.7361\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 17ms/step - loss: 5.1614e-04 - accuracy: 0.9972 - precision: 0.3765 - recall: 0.8638 - auc: 0.9846 - prc: 0.7577 - val_loss: 0.0319 - val_accuracy: 0.9971 - val_precision: 0.3029 - val_recall: 0.8413 - val_auc: 0.9739 - val_prc: 0.7353\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 2s 18ms/step - loss: 5.0450e-04 - accuracy: 0.9968 - precision: 0.3448 - recall: 0.8700 - auc: 0.9854 - prc: 0.7521 - val_loss: 0.0295 - val_accuracy: 0.9975 - val_precision: 0.3397 - val_recall: 0.8413 - val_auc: 0.9731 - val_prc: 0.7358\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 5.0229e-04 - accuracy: 0.9973 - precision: 0.3804 - recall: 0.8669 - auc: 0.9863 - prc: 0.7593 - val_loss: 0.0306 - val_accuracy: 0.9972 - val_precision: 0.3118 - val_recall: 0.8413 - val_auc: 0.9730 - val_prc: 0.7379\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.0996e-04 - accuracy: 0.9972 - precision: 0.3732 - recall: 0.8700 - auc: 0.9862 - prc: 0.7582 - val_loss: 0.0345 - val_accuracy: 0.9963 - val_precision: 0.2488 - val_recall: 0.8413 - val_auc: 0.9738 - val_prc: 0.7242\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.9724e-04 - accuracy: 0.9968 - precision: 0.3390 - recall: 0.8669 - auc: 0.9863 - prc: 0.7547 - val_loss: 0.0340 - val_accuracy: 0.9963 - val_precision: 0.2488 - val_recall: 0.8413 - val_auc: 0.9741 - val_prc: 0.7241\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.18000000000000002\n",
            "****************************** w =  3\n",
            "Model: \"sequential_65\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_65 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_65 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_65 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_65 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89/89 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 0.9955 - precision: 0.2428 - recall: 0.7668 - auc: 0.9127 - prc: 0.6074"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 16ms/step - loss: 0.0022 - accuracy: 0.9955 - precision: 0.2428 - recall: 0.7668 - auc: 0.9127 - prc: 0.6074 - val_loss: 0.1161 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9575 - val_prc: 0.6768\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.4511e-04 - accuracy: 0.9994 - precision: 0.8477 - recall: 0.7926 - auc: 0.9373 - prc: 0.7201 - val_loss: 0.0617 - val_accuracy: 0.9994 - val_precision: 0.7576 - val_recall: 0.7937 - val_auc: 0.9652 - val_prc: 0.6918\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 7.6061e-04 - accuracy: 0.9993 - precision: 0.7781 - recall: 0.8142 - auc: 0.9519 - prc: 0.7426 - val_loss: 0.0414 - val_accuracy: 0.9993 - val_precision: 0.6986 - val_recall: 0.8095 - val_auc: 0.9695 - val_prc: 0.7062\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 6.8589e-04 - accuracy: 0.9992 - precision: 0.7437 - recall: 0.8266 - auc: 0.9646 - prc: 0.7522 - val_loss: 0.0374 - val_accuracy: 0.9991 - val_precision: 0.6375 - val_recall: 0.8095 - val_auc: 0.9724 - val_prc: 0.7112\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 6.4566e-04 - accuracy: 0.9990 - precision: 0.6843 - recall: 0.8390 - auc: 0.9693 - prc: 0.7599 - val_loss: 0.0355 - val_accuracy: 0.9990 - val_precision: 0.5977 - val_recall: 0.8254 - val_auc: 0.9737 - val_prc: 0.7135\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 6.2460e-04 - accuracy: 0.9988 - precision: 0.6239 - recall: 0.8421 - auc: 0.9720 - prc: 0.7630 - val_loss: 0.0356 - val_accuracy: 0.9987 - val_precision: 0.5149 - val_recall: 0.8254 - val_auc: 0.9757 - val_prc: 0.7025\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.9829e-04 - accuracy: 0.9985 - precision: 0.5442 - recall: 0.8576 - auc: 0.9772 - prc: 0.7610 - val_loss: 0.0288 - val_accuracy: 0.9990 - val_precision: 0.5977 - val_recall: 0.8254 - val_auc: 0.9759 - val_prc: 0.7129\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.9214e-04 - accuracy: 0.9987 - precision: 0.5996 - recall: 0.8576 - auc: 0.9764 - prc: 0.7611 - val_loss: 0.0309 - val_accuracy: 0.9986 - val_precision: 0.5000 - val_recall: 0.8254 - val_auc: 0.9759 - val_prc: 0.7045\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.6975e-04 - accuracy: 0.9984 - precision: 0.5336 - recall: 0.8607 - auc: 0.9814 - prc: 0.7681 - val_loss: 0.0302 - val_accuracy: 0.9985 - val_precision: 0.4727 - val_recall: 0.8254 - val_auc: 0.9745 - val_prc: 0.7063\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.6489e-04 - accuracy: 0.9984 - precision: 0.5237 - recall: 0.8545 - auc: 0.9799 - prc: 0.7605 - val_loss: 0.0340 - val_accuracy: 0.9976 - val_precision: 0.3464 - val_recall: 0.8413 - val_auc: 0.9755 - val_prc: 0.7075\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.5055e-04 - accuracy: 0.9979 - precision: 0.4551 - recall: 0.8638 - auc: 0.9845 - prc: 0.7688 - val_loss: 0.0302 - val_accuracy: 0.9980 - val_precision: 0.3985 - val_recall: 0.8413 - val_auc: 0.9731 - val_prc: 0.7326\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.4160e-04 - accuracy: 0.9981 - precision: 0.4844 - recall: 0.8638 - auc: 0.9835 - prc: 0.7592 - val_loss: 0.0306 - val_accuracy: 0.9977 - val_precision: 0.3557 - val_recall: 0.8413 - val_auc: 0.9733 - val_prc: 0.7335\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.3541e-04 - accuracy: 0.9979 - precision: 0.4464 - recall: 0.8638 - auc: 0.9835 - prc: 0.7642 - val_loss: 0.0289 - val_accuracy: 0.9979 - val_precision: 0.3786 - val_recall: 0.8413 - val_auc: 0.9738 - val_prc: 0.7339\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.2883e-04 - accuracy: 0.9977 - precision: 0.4253 - recall: 0.8638 - auc: 0.9844 - prc: 0.7602 - val_loss: 0.0262 - val_accuracy: 0.9980 - val_precision: 0.3955 - val_recall: 0.8413 - val_auc: 0.9748 - val_prc: 0.7334\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 5.2645e-04 - accuracy: 0.9980 - precision: 0.4714 - recall: 0.8669 - auc: 0.9847 - prc: 0.7641 - val_loss: 0.0295 - val_accuracy: 0.9974 - val_precision: 0.3292 - val_recall: 0.8413 - val_auc: 0.9742 - val_prc: 0.7357\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 5.2156e-04 - accuracy: 0.9974 - precision: 0.3911 - recall: 0.8669 - auc: 0.9849 - prc: 0.7638 - val_loss: 0.0262 - val_accuracy: 0.9978 - val_precision: 0.3759 - val_recall: 0.8413 - val_auc: 0.9752 - val_prc: 0.7497\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 5.2210e-04 - accuracy: 0.9980 - precision: 0.4636 - recall: 0.8669 - auc: 0.9845 - prc: 0.7663 - val_loss: 0.0296 - val_accuracy: 0.9971 - val_precision: 0.3064 - val_recall: 0.8413 - val_auc: 0.9745 - val_prc: 0.7365\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 5.1946e-04 - accuracy: 0.9976 - precision: 0.4121 - recall: 0.8638 - auc: 0.9858 - prc: 0.7597 - val_loss: 0.0307 - val_accuracy: 0.9968 - val_precision: 0.2834 - val_recall: 0.8413 - val_auc: 0.9745 - val_prc: 0.7362\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.2387e-04 - accuracy: 0.9972 - precision: 0.3774 - recall: 0.8669 - auc: 0.9844 - prc: 0.7623 - val_loss: 0.0280 - val_accuracy: 0.9973 - val_precision: 0.3212 - val_recall: 0.8413 - val_auc: 0.9752 - val_prc: 0.7500\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 5.0996e-04 - accuracy: 0.9970 - precision: 0.3626 - recall: 0.8700 - auc: 0.9851 - prc: 0.7613 - val_loss: 0.0240 - val_accuracy: 0.9978 - val_precision: 0.3732 - val_recall: 0.8413 - val_auc: 0.9747 - val_prc: 0.7497\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.19000000000000003\n",
            "****************************** w =  3\n",
            "Model: \"sequential_66\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_66 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_66 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_66 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_66 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88/89 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 0.9907 - precision: 0.1233 - recall: 0.7402 - auc: 0.9051 - prc: 0.5967"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 4s 24ms/step - loss: 0.0024 - accuracy: 0.9907 - precision: 0.1243 - recall: 0.7383 - auc: 0.9038 - prc: 0.5951 - val_loss: 0.0991 - val_accuracy: 0.9994 - val_precision: 0.7937 - val_recall: 0.7937 - val_auc: 0.9607 - val_prc: 0.6851\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 2s 18ms/step - loss: 9.6566e-04 - accuracy: 0.9994 - precision: 0.8405 - recall: 0.7833 - auc: 0.9344 - prc: 0.7165 - val_loss: 0.0456 - val_accuracy: 0.9994 - val_precision: 0.7812 - val_recall: 0.7937 - val_auc: 0.9673 - val_prc: 0.7170\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 2s 21ms/step - loss: 7.6274e-04 - accuracy: 0.9994 - precision: 0.8219 - recall: 0.8142 - auc: 0.9471 - prc: 0.7467 - val_loss: 0.0398 - val_accuracy: 0.9992 - val_precision: 0.6892 - val_recall: 0.8095 - val_auc: 0.9686 - val_prc: 0.7110\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 2s 18ms/step - loss: 7.0091e-04 - accuracy: 0.9992 - precision: 0.7606 - recall: 0.8359 - auc: 0.9591 - prc: 0.7526 - val_loss: 0.0303 - val_accuracy: 0.9992 - val_precision: 0.6711 - val_recall: 0.8095 - val_auc: 0.9710 - val_prc: 0.7182\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 6.6212e-04 - accuracy: 0.9992 - precision: 0.7297 - recall: 0.8359 - auc: 0.9663 - prc: 0.7540 - val_loss: 0.0289 - val_accuracy: 0.9991 - val_precision: 0.6220 - val_recall: 0.8095 - val_auc: 0.9735 - val_prc: 0.7178\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 6.3499e-04 - accuracy: 0.9990 - precision: 0.6834 - recall: 0.8421 - auc: 0.9721 - prc: 0.7607 - val_loss: 0.0294 - val_accuracy: 0.9990 - val_precision: 0.6047 - val_recall: 0.8254 - val_auc: 0.9739 - val_prc: 0.7042\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 6.2174e-04 - accuracy: 0.9989 - precision: 0.6524 - recall: 0.8483 - auc: 0.9734 - prc: 0.7573 - val_loss: 0.0286 - val_accuracy: 0.9988 - val_precision: 0.5474 - val_recall: 0.8254 - val_auc: 0.9764 - val_prc: 0.7055\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 6.0240e-04 - accuracy: 0.9989 - precision: 0.6304 - recall: 0.8607 - auc: 0.9783 - prc: 0.7644 - val_loss: 0.0267 - val_accuracy: 0.9988 - val_precision: 0.5361 - val_recall: 0.8254 - val_auc: 0.9765 - val_prc: 0.7305\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.8627e-04 - accuracy: 0.9987 - precision: 0.6009 - recall: 0.8576 - auc: 0.9787 - prc: 0.7587 - val_loss: 0.0246 - val_accuracy: 0.9987 - val_precision: 0.5306 - val_recall: 0.8254 - val_auc: 0.9760 - val_prc: 0.7311\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.6991e-04 - accuracy: 0.9988 - precision: 0.6061 - recall: 0.8576 - auc: 0.9818 - prc: 0.7590 - val_loss: 0.0251 - val_accuracy: 0.9986 - val_precision: 0.4906 - val_recall: 0.8254 - val_auc: 0.9762 - val_prc: 0.7346\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.5912e-04 - accuracy: 0.9985 - precision: 0.5478 - recall: 0.8514 - auc: 0.9838 - prc: 0.7605 - val_loss: 0.0218 - val_accuracy: 0.9987 - val_precision: 0.5098 - val_recall: 0.8254 - val_auc: 0.9759 - val_prc: 0.7310\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.6054e-04 - accuracy: 0.9985 - precision: 0.5594 - recall: 0.8607 - auc: 0.9824 - prc: 0.7625 - val_loss: 0.0217 - val_accuracy: 0.9986 - val_precision: 0.4906 - val_recall: 0.8254 - val_auc: 0.9760 - val_prc: 0.7331\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 5.5045e-04 - accuracy: 0.9986 - precision: 0.5659 - recall: 0.8638 - auc: 0.9839 - prc: 0.7606 - val_loss: 0.0229 - val_accuracy: 0.9985 - val_precision: 0.4727 - val_recall: 0.8254 - val_auc: 0.9762 - val_prc: 0.7336\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 2s 25ms/step - loss: 5.4878e-04 - accuracy: 0.9984 - precision: 0.5327 - recall: 0.8576 - auc: 0.9837 - prc: 0.7604 - val_loss: 0.0241 - val_accuracy: 0.9984 - val_precision: 0.4522 - val_recall: 0.8254 - val_auc: 0.9758 - val_prc: 0.7348\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 2s 20ms/step - loss: 5.3942e-04 - accuracy: 0.9983 - precision: 0.5129 - recall: 0.8638 - auc: 0.9844 - prc: 0.7577 - val_loss: 0.0229 - val_accuracy: 0.9983 - val_precision: 0.4483 - val_recall: 0.8254 - val_auc: 0.9763 - val_prc: 0.7356\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 5.4153e-04 - accuracy: 0.9984 - precision: 0.5225 - recall: 0.8638 - auc: 0.9835 - prc: 0.7609 - val_loss: 0.0252 - val_accuracy: 0.9981 - val_precision: 0.4141 - val_recall: 0.8413 - val_auc: 0.9755 - val_prc: 0.7380\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 5.3216e-04 - accuracy: 0.9982 - precision: 0.5045 - recall: 0.8638 - auc: 0.9855 - prc: 0.7610 - val_loss: 0.0243 - val_accuracy: 0.9981 - val_precision: 0.4127 - val_recall: 0.8254 - val_auc: 0.9759 - val_prc: 0.7361\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.2739e-04 - accuracy: 0.9982 - precision: 0.4930 - recall: 0.8669 - auc: 0.9857 - prc: 0.7588 - val_loss: 0.0231 - val_accuracy: 0.9982 - val_precision: 0.4262 - val_recall: 0.8254 - val_auc: 0.9767 - val_prc: 0.7391\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 5.3200e-04 - accuracy: 0.9980 - precision: 0.4705 - recall: 0.8638 - auc: 0.9843 - prc: 0.7647 - val_loss: 0.0212 - val_accuracy: 0.9983 - val_precision: 0.4444 - val_recall: 0.8254 - val_auc: 0.9755 - val_prc: 0.7500\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 5.2488e-04 - accuracy: 0.9983 - precision: 0.5167 - recall: 0.8638 - auc: 0.9841 - prc: 0.7621 - val_loss: 0.0254 - val_accuracy: 0.9976 - val_precision: 0.3510 - val_recall: 0.8413 - val_auc: 0.9759 - val_prc: 0.7380\n",
            "1419/1419 [==============================] - 4s 3ms/step\n",
            "****************************** w =  0.01\n",
            "****************************** w =  4\n",
            "Model: \"sequential_67\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_67 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_67 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_67 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_67 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89/89 [==============================] - ETA: 0s - loss: 2.0252e-04 - accuracy: 0.2161 - precision: 0.0021 - recall: 0.9741 - auc: 0.9312 - prc: 0.5210"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 4s 29ms/step - loss: 2.0252e-04 - accuracy: 0.2161 - precision: 0.0021 - recall: 0.9741 - auc: 0.9312 - prc: 0.5210 - val_loss: 1.1186 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9595 - val_prc: 0.4606\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 8.4623e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9686 - prc: 0.4595 - val_loss: 1.3139 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9604 - val_prc: 0.4148\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 5.9676e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9733 - prc: 0.4065 - val_loss: 1.4685 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9610 - val_prc: 0.3094\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 4.6984e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9732 - prc: 0.3375 - val_loss: 1.6043 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9609 - val_prc: 0.2580\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 3.8191e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9734 - prc: 0.2843 - val_loss: 1.7238 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9613 - val_prc: 0.2044\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 2s 17ms/step - loss: 3.2255e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9733 - prc: 0.2433 - val_loss: 1.8311 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9603 - val_prc: 0.1615\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 2s 18ms/step - loss: 2.7772e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9738 - prc: 0.2137 - val_loss: 1.9303 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9602 - val_prc: 0.1440\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 2s 18ms/step - loss: 2.4489e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9737 - prc: 0.1901 - val_loss: 2.0242 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9595 - val_prc: 0.1310\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 2.1844e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9726 - prc: 0.1672 - val_loss: 2.1106 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9591 - val_prc: 0.1184\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 1.9453e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9724 - prc: 0.1481 - val_loss: 2.1909 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9595 - val_prc: 0.1077\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 2s 20ms/step - loss: 1.7247e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9729 - prc: 0.1336 - val_loss: 2.2657 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9585 - val_prc: 0.0986\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 2s 20ms/step - loss: 1.6258e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9710 - prc: 0.1223 - val_loss: 2.3382 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9587 - val_prc: 0.0879\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 2s 21ms/step - loss: 1.4553e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9717 - prc: 0.1137 - val_loss: 2.4060 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9581 - val_prc: 0.0849\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 2s 18ms/step - loss: 1.3431e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9711 - prc: 0.1054 - val_loss: 2.4714 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9579 - val_prc: 0.0793\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 2s 22ms/step - loss: 1.2463e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9703 - prc: 0.0976 - val_loss: 2.5334 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9574 - val_prc: 0.0736\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 2s 18ms/step - loss: 1.1447e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9711 - prc: 0.0923 - val_loss: 2.5915 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9576 - val_prc: 0.0693\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 1.0612e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9701 - prc: 0.0862 - val_loss: 2.6470 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9569 - val_prc: 0.0645\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 2s 21ms/step - loss: 1.0108e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9696 - prc: 0.0797 - val_loss: 2.7008 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9560 - val_prc: 0.0608\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 9.2927e-06 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9695 - prc: 0.0742 - val_loss: 2.7516 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9564 - val_prc: 0.0557\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 8.8249e-06 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9690 - prc: 0.0703 - val_loss: 2.8009 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9543 - val_prc: 0.0515\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.02\n",
            "****************************** w =  4\n",
            "Model: \"sequential_68\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_68 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_68 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_68 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_68 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88/89 [============================>.] - ETA: 0s - loss: 1.8251e-04 - accuracy: 0.0073 - precision: 0.0017 - recall: 0.9974 - auc: 0.9007 - prc: 0.0750"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 5s 26ms/step - loss: 1.8115e-04 - accuracy: 0.0072 - precision: 0.0017 - recall: 0.9974 - auc: 0.9016 - prc: 0.0767 - val_loss: 1.1135 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9722 - val_prc: 0.4425\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 7.9912e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9663 - prc: 0.4456 - val_loss: 1.2886 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9704 - val_prc: 0.3533\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 5.9875e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9701 - prc: 0.3900 - val_loss: 1.4341 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9690 - val_prc: 0.2786\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 4.7010e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9712 - prc: 0.3491 - val_loss: 1.5598 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9682 - val_prc: 0.2513\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 3.9224e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9712 - prc: 0.3033 - val_loss: 1.6719 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9679 - val_prc: 0.2022\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 3.3593e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9715 - prc: 0.2600 - val_loss: 1.7752 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9671 - val_prc: 0.1652\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 2.8885e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9717 - prc: 0.2203 - val_loss: 1.8692 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9670 - val_prc: 0.1572\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 2.5508e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9713 - prc: 0.1965 - val_loss: 1.9569 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9666 - val_prc: 0.1415\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 2.2894e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9707 - prc: 0.1753 - val_loss: 2.0392 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9656 - val_prc: 0.1252\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 2.0761e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9704 - prc: 0.1583 - val_loss: 2.1187 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9654 - val_prc: 0.1135\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 1.8476e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9717 - prc: 0.1435 - val_loss: 2.1918 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9650 - val_prc: 0.1030\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 1.7131e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9700 - prc: 0.1285 - val_loss: 2.2615 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9646 - val_prc: 0.0959\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 2s 17ms/step - loss: 1.5844e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9695 - prc: 0.1199 - val_loss: 2.3284 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9634 - val_prc: 0.0900\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.5037e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9684 - prc: 0.1125 - val_loss: 2.3930 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9641 - val_prc: 0.0831\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.3788e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9685 - prc: 0.1058 - val_loss: 2.4542 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9638 - val_prc: 0.0787\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 1.2633e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9687 - prc: 0.0977 - val_loss: 2.5118 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9628 - val_prc: 0.0729\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 1.1931e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9685 - prc: 0.0933 - val_loss: 2.5681 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9621 - val_prc: 0.0688\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 2s 18ms/step - loss: 1.1049e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9686 - prc: 0.0872 - val_loss: 2.6209 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9611 - val_prc: 0.0650\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 2s 17ms/step - loss: 1.0495e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9679 - prc: 0.0807 - val_loss: 2.6723 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9615 - val_prc: 0.0612\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 2s 21ms/step - loss: 1.0300e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9651 - prc: 0.0756 - val_loss: 2.7240 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9602 - val_prc: 0.0557\n",
            "1419/1419 [==============================] - 4s 3ms/step\n",
            "****************************** w =  0.03\n",
            "****************************** w =  4\n",
            "Model: \"sequential_69\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_69 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_69 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_69 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_69 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89/89 [==============================] - ETA: 0s - loss: 2.1804e-04 - accuracy: 0.0091 - precision: 0.0017 - recall: 0.9767 - auc: 0.8743 - prc: 0.0690"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 5s 35ms/step - loss: 2.1804e-04 - accuracy: 0.0091 - precision: 0.0017 - recall: 0.9767 - auc: 0.8743 - prc: 0.0690 - val_loss: 1.1250 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9686 - val_prc: 0.4178\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 2s 26ms/step - loss: 7.9598e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9683 - prc: 0.4494 - val_loss: 1.3017 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9675 - val_prc: 0.3930\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 17ms/step - loss: 5.8807e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9697 - prc: 0.4045 - val_loss: 1.4430 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9663 - val_prc: 0.3139\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 2s 19ms/step - loss: 4.6589e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9702 - prc: 0.3596 - val_loss: 1.5645 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9659 - val_prc: 0.2625\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 2s 26ms/step - loss: 3.9871e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9705 - prc: 0.3077 - val_loss: 1.6752 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9655 - val_prc: 0.2198\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 2s 20ms/step - loss: 3.4227e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9710 - prc: 0.2807 - val_loss: 1.7759 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9651 - val_prc: 0.1859\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 2s 18ms/step - loss: 2.9805e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9707 - prc: 0.2448 - val_loss: 1.8679 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9649 - val_prc: 0.1617\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 2s 20ms/step - loss: 2.6418e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9714 - prc: 0.2203 - val_loss: 1.9528 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9641 - val_prc: 0.1504\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 2s 21ms/step - loss: 2.4454e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9694 - prc: 0.2005 - val_loss: 2.0337 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9645 - val_prc: 0.1382\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 2.1611e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9717 - prc: 0.1819 - val_loss: 2.1081 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9638 - val_prc: 0.1265\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 2s 20ms/step - loss: 2.0089e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9697 - prc: 0.1618 - val_loss: 2.1788 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9635 - val_prc: 0.1132\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 2s 18ms/step - loss: 1.8684e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9698 - prc: 0.1522 - val_loss: 2.2461 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9632 - val_prc: 0.1047\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 2s 21ms/step - loss: 1.7449e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9693 - prc: 0.1363 - val_loss: 2.3110 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9622 - val_prc: 0.0954\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.6151e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9695 - prc: 0.1276 - val_loss: 2.3714 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9625 - val_prc: 0.0890\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.5379e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9688 - prc: 0.1187 - val_loss: 2.4296 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9620 - val_prc: 0.0835\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 1.4549e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9682 - prc: 0.1113 - val_loss: 2.4850 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9617 - val_prc: 0.0797\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 2s 26ms/step - loss: 1.3638e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9687 - prc: 0.1039 - val_loss: 2.5386 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9613 - val_prc: 0.0765\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 1.2786e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9691 - prc: 0.0999 - val_loss: 2.5883 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9613 - val_prc: 0.0733\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.2419e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9686 - prc: 0.0948 - val_loss: 2.6381 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9606 - val_prc: 0.0703\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.1985e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9669 - prc: 0.0893 - val_loss: 2.6851 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9610 - val_prc: 0.0662\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.04\n",
            "****************************** w =  4\n",
            "Model: \"sequential_70\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_70 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_70 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_70 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_70 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89/89 [==============================] - ETA: 0s - loss: 2.1331e-04 - accuracy: 0.0322 - precision: 0.0017 - recall: 0.9819 - auc: 0.8757 - prc: 0.0830"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 4s 25ms/step - loss: 2.1331e-04 - accuracy: 0.0322 - precision: 0.0017 - recall: 0.9819 - auc: 0.8757 - prc: 0.0830 - val_loss: 1.0608 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9683 - val_prc: 0.4678\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 3s 29ms/step - loss: 8.5519e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9693 - prc: 0.4960 - val_loss: 1.2417 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9670 - val_prc: 0.4073\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 6.2599e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9703 - prc: 0.4089 - val_loss: 1.3838 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9665 - val_prc: 0.3413\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 5.1072e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9710 - prc: 0.3681 - val_loss: 1.5080 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9658 - val_prc: 0.2752\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 4.2718e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9734 - prc: 0.3184 - val_loss: 1.6154 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9650 - val_prc: 0.2412\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 3.7631e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9718 - prc: 0.2718 - val_loss: 1.7127 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9644 - val_prc: 0.1999\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 3.3303e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9723 - prc: 0.2462 - val_loss: 1.8009 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9637 - val_prc: 0.1625\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 3.0219e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9717 - prc: 0.2158 - val_loss: 1.8821 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9638 - val_prc: 0.1446\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 2s 19ms/step - loss: 2.7765e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9725 - prc: 0.1976 - val_loss: 1.9576 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9637 - val_prc: 0.1385\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 2.5915e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9718 - prc: 0.1793 - val_loss: 2.0284 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9629 - val_prc: 0.1240\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 2.4182e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9721 - prc: 0.1601 - val_loss: 2.0948 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9633 - val_prc: 0.1140\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 2.2928e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9714 - prc: 0.1430 - val_loss: 2.1574 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9624 - val_prc: 0.1059\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 2.1362e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9719 - prc: 0.1361 - val_loss: 2.2145 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9626 - val_prc: 0.0987\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 2.0481e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9714 - prc: 0.1273 - val_loss: 2.2688 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9619 - val_prc: 0.0903\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.9983e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9708 - prc: 0.1190 - val_loss: 2.3225 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9622 - val_prc: 0.0876\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 1.9017e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9710 - prc: 0.1128 - val_loss: 2.3724 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9620 - val_prc: 0.0853\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 1.8560e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9704 - prc: 0.1058 - val_loss: 2.4209 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9620 - val_prc: 0.0794\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 1.7991e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9697 - prc: 0.1010 - val_loss: 2.4661 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9613 - val_prc: 0.0756\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 1.7165e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9706 - prc: 0.0949 - val_loss: 2.5080 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9605 - val_prc: 0.0722\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.6813e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9703 - prc: 0.0917 - val_loss: 2.5483 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9605 - val_prc: 0.0683\n",
            "1419/1419 [==============================] - 4s 3ms/step\n",
            "****************************** w =  0.05\n",
            "****************************** w =  4\n",
            "Model: \"sequential_71\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_71 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_71 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_71 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_71 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88/89 [============================>.] - ETA: 0s - loss: 1.8094e-04 - accuracy: 0.0076 - precision: 0.0017 - recall: 0.9974 - auc: 0.8980 - prc: 0.0832"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 4s 19ms/step - loss: 1.8146e-04 - accuracy: 0.0076 - precision: 0.0017 - recall: 0.9974 - auc: 0.8987 - prc: 0.0851 - val_loss: 1.1030 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9610 - val_prc: 0.4461\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 8.4024e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9702 - prc: 0.4645 - val_loss: 1.2698 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9634 - val_prc: 0.3916\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 6.4410e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9723 - prc: 0.4249 - val_loss: 1.3997 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9636 - val_prc: 0.3360\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 2s 19ms/step - loss: 5.3746e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9733 - prc: 0.3788 - val_loss: 1.5083 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9630 - val_prc: 0.2958\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 2s 18ms/step - loss: 4.7495e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9735 - prc: 0.3333 - val_loss: 1.6047 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9635 - val_prc: 0.2619\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 4.2724e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9743 - prc: 0.2951 - val_loss: 1.6896 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9634 - val_prc: 0.2296\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 3.9462e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9746 - prc: 0.2762 - val_loss: 1.7679 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9625 - val_prc: 0.1985\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 17ms/step - loss: 3.6736e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9750 - prc: 0.2496 - val_loss: 1.8381 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9630 - val_prc: 0.1722\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 3.4671e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9737 - prc: 0.2210 - val_loss: 1.9013 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9630 - val_prc: 0.1534\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.3483e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9740 - prc: 0.2049 - val_loss: 1.9621 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9621 - val_prc: 0.1458\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.1987e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9740 - prc: 0.1881 - val_loss: 2.0166 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9622 - val_prc: 0.1345\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 2s 19ms/step - loss: 3.1114e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9732 - prc: 0.1766 - val_loss: 2.0674 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9621 - val_prc: 0.1220\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 2s 17ms/step - loss: 2.9713e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9746 - prc: 0.1661 - val_loss: 2.1138 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9611 - val_prc: 0.1145\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 2s 24ms/step - loss: 2.9143e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9742 - prc: 0.1523 - val_loss: 2.1571 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9616 - val_prc: 0.1105\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 2s 18ms/step - loss: 2.8552e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9735 - prc: 0.1472 - val_loss: 2.1978 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9616 - val_prc: 0.1068\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 2s 17ms/step - loss: 2.8206e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9729 - prc: 0.1379 - val_loss: 2.2370 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9613 - val_prc: 0.0998\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 2.7552e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9736 - prc: 0.1307 - val_loss: 2.2725 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9607 - val_prc: 0.0940\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 2s 21ms/step - loss: 2.7282e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9731 - prc: 0.1262 - val_loss: 2.3063 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9605 - val_prc: 0.0937\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 2s 22ms/step - loss: 2.6977e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9727 - prc: 0.1221 - val_loss: 2.3382 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9616 - val_prc: 0.0915\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 2s 19ms/step - loss: 2.6307e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9741 - prc: 0.1196 - val_loss: 2.3673 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9608 - val_prc: 0.0874\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.060000000000000005\n",
            "****************************** w =  4\n",
            "Model: \"sequential_72\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_72 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_72 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_72 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_72 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87/89 [============================>.] - ETA: 0s - loss: 1.7677e-04 - accuracy: 0.0093 - precision: 0.0017 - recall: 0.9974 - auc: 0.9069 - prc: 0.1065"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 16ms/step - loss: 1.7530e-04 - accuracy: 0.0092 - precision: 0.0017 - recall: 0.9974 - auc: 0.9084 - prc: 0.1098 - val_loss: 1.0766 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9687 - val_prc: 0.5171\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.0405e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9697 - prc: 0.5455 - val_loss: 1.2251 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9692 - val_prc: 0.4959\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.2659e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9726 - prc: 0.4811 - val_loss: 1.3390 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9684 - val_prc: 0.4223\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 6.4717e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9727 - prc: 0.4227 - val_loss: 1.4342 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9672 - val_prc: 0.3221\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.9871e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9719 - prc: 0.3753 - val_loss: 1.5190 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9674 - val_prc: 0.2795\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.4547e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9755 - prc: 0.3374 - val_loss: 1.5897 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9670 - val_prc: 0.2697\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.2362e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9740 - prc: 0.3083 - val_loss: 1.6534 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9669 - val_prc: 0.2395\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.0027e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9751 - prc: 0.2926 - val_loss: 1.7101 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9663 - val_prc: 0.2080\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.8869e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9735 - prc: 0.2688 - val_loss: 1.7613 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9668 - val_prc: 0.1958\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 4.7340e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9742 - prc: 0.2522 - val_loss: 1.8065 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9665 - val_prc: 0.1804\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 4.6387e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9748 - prc: 0.2397 - val_loss: 1.8482 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9661 - val_prc: 0.1678\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 4.5948e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9740 - prc: 0.2248 - val_loss: 1.8873 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9661 - val_prc: 0.1625\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.5306e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9743 - prc: 0.2115 - val_loss: 1.9237 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9659 - val_prc: 0.1517\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.4182e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9755 - prc: 0.1986 - val_loss: 1.9544 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9661 - val_prc: 0.1425\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.4453e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9735 - prc: 0.1944 - val_loss: 1.9851 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9657 - val_prc: 0.1372\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.3588e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9747 - prc: 0.1859 - val_loss: 2.0113 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9657 - val_prc: 0.1328\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.3191e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9744 - prc: 0.1777 - val_loss: 2.0344 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9659 - val_prc: 0.1304\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.3391e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9733 - prc: 0.1701 - val_loss: 2.0569 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9656 - val_prc: 0.1251\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.2623e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9749 - prc: 0.1660 - val_loss: 2.0767 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9656 - val_prc: 0.1209\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.2185e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9758 - prc: 0.1607 - val_loss: 2.0944 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9655 - val_prc: 0.1198\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.07\n",
            "****************************** w =  4\n",
            "Model: \"sequential_73\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_73 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_73 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_73 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_73 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89/89 [==============================] - ETA: 0s - loss: 1.7935e-04 - accuracy: 0.0212 - precision: 0.0017 - recall: 1.0000 - auc: 0.9193 - prc: 0.1656"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 16ms/step - loss: 1.7935e-04 - accuracy: 0.0212 - precision: 0.0017 - recall: 1.0000 - auc: 0.9193 - prc: 0.1656 - val_loss: 1.0025 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9673 - val_prc: 0.5517\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.0549e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9677 - prc: 0.5731 - val_loss: 1.1364 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9678 - val_prc: 0.5129\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.0188e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9709 - prc: 0.5296 - val_loss: 1.2362 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9677 - val_prc: 0.4436\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 8.1438e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9732 - prc: 0.4838 - val_loss: 1.3149 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9674 - val_prc: 0.3711\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 7.7155e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9723 - prc: 0.4308 - val_loss: 1.3808 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9675 - val_prc: 0.3488\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 7.3528e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9730 - prc: 0.4030 - val_loss: 1.4355 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9677 - val_prc: 0.3232\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 7.1242e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9734 - prc: 0.3792 - val_loss: 1.4838 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9674 - val_prc: 0.2938\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 7.0041e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9737 - prc: 0.3595 - val_loss: 1.5263 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9674 - val_prc: 0.2793\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 6.8226e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9745 - prc: 0.3470 - val_loss: 1.5615 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9674 - val_prc: 0.2640\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 6.7277e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9742 - prc: 0.3252 - val_loss: 1.5910 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9674 - val_prc: 0.2487\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 6.6598e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9743 - prc: 0.3143 - val_loss: 1.6171 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9675 - val_prc: 0.2500\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 6.6152e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9746 - prc: 0.2929 - val_loss: 1.6405 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9674 - val_prc: 0.2428\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 6.5878e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9745 - prc: 0.2883 - val_loss: 1.6619 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9678 - val_prc: 0.2326\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 6.5345e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9752 - prc: 0.2839 - val_loss: 1.6813 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9675 - val_prc: 0.2190\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 6.5154e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9748 - prc: 0.2758 - val_loss: 1.6977 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9679 - val_prc: 0.2170\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 6.4106e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9759 - prc: 0.2712 - val_loss: 1.7102 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9680 - val_prc: 0.2115\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 6.4779e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9745 - prc: 0.2565 - val_loss: 1.7231 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9676 - val_prc: 0.1988\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 6.3419e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9766 - prc: 0.2525 - val_loss: 1.7315 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9678 - val_prc: 0.1949\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 6.3870e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9755 - prc: 0.2528 - val_loss: 1.7410 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9678 - val_prc: 0.1905\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 6.3252e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9766 - prc: 0.2491 - val_loss: 1.7489 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9680 - val_prc: 0.1847\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.08\n",
            "****************************** w =  4\n",
            "Model: \"sequential_74\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_74 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_74 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_74 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_74 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "84/89 [===========================>..] - ETA: 0s - loss: 2.3074e-04 - accuracy: 0.0229 - precision: 0.0017 - recall: 0.9866 - auc: 0.8975 - prc: 0.1821"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 16ms/step - loss: 2.2493e-04 - accuracy: 0.0220 - precision: 0.0017 - recall: 0.9870 - auc: 0.8996 - prc: 0.1880 - val_loss: 1.0439 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9503 - val_prc: 0.4832\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.2476e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9664 - prc: 0.4864 - val_loss: 1.1659 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9565 - val_prc: 0.4407\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.1039e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9712 - prc: 0.4633 - val_loss: 1.2483 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9579 - val_prc: 0.4176\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.0457e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9725 - prc: 0.4249 - val_loss: 1.3098 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9595 - val_prc: 0.3620\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.0075e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9738 - prc: 0.4015 - val_loss: 1.3567 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9601 - val_prc: 0.3174\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.8737e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9731 - prc: 0.3684 - val_loss: 1.3940 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9611 - val_prc: 0.2939\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.7278e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9737 - prc: 0.3526 - val_loss: 1.4220 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9619 - val_prc: 0.2722\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 2s 22ms/step - loss: 9.5863e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9749 - prc: 0.3420 - val_loss: 1.4442 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9622 - val_prc: 0.2726\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 3s 29ms/step - loss: 9.4980e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9746 - prc: 0.3277 - val_loss: 1.4599 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9632 - val_prc: 0.2720\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 2s 21ms/step - loss: 9.4433e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9748 - prc: 0.3248 - val_loss: 1.4717 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9633 - val_prc: 0.2684\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.3680e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9753 - prc: 0.3231 - val_loss: 1.4797 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9637 - val_prc: 0.2608\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.2730e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9763 - prc: 0.3192 - val_loss: 1.4851 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9644 - val_prc: 0.2553\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.2503e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9765 - prc: 0.3237 - val_loss: 1.4898 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9642 - val_prc: 0.2550\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.2079e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9771 - prc: 0.3174 - val_loss: 1.4927 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9651 - val_prc: 0.2505\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.1517e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9772 - prc: 0.3147 - val_loss: 1.4945 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9648 - val_prc: 0.2475\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.0746e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9785 - prc: 0.3082 - val_loss: 1.4943 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9649 - val_prc: 0.2441\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.0617e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9780 - prc: 0.3159 - val_loss: 1.4926 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9653 - val_prc: 0.2470\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 8.9888e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9792 - prc: 0.3132 - val_loss: 1.4908 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9655 - val_prc: 0.2468\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 9.0344e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9772 - prc: 0.3073 - val_loss: 1.4894 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9660 - val_prc: 0.2471\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 8.9782e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9784 - prc: 0.3039 - val_loss: 1.4877 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9664 - val_prc: 0.2455\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.09\n",
            "****************************** w =  4\n",
            "Model: \"sequential_75\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_75 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_75 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_75 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_75 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87/89 [============================>.] - ETA: 0s - loss: 2.2250e-04 - accuracy: 0.0077 - precision: 0.0017 - recall: 0.9974 - auc: 0.9216 - prc: 0.2454"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 16ms/step - loss: 2.1962e-04 - accuracy: 0.0076 - precision: 0.0017 - recall: 0.9974 - auc: 0.9224 - prc: 0.2508 - val_loss: 1.0234 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9655 - val_prc: 0.4930\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.4971e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9652 - prc: 0.5346 - val_loss: 1.1133 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9672 - val_prc: 0.4583\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 1.3954e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9688 - prc: 0.5365 - val_loss: 1.1679 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9682 - val_prc: 0.4762\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 1.3376e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9730 - prc: 0.5072 - val_loss: 1.2018 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9686 - val_prc: 0.4679\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 1.3096e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9742 - prc: 0.4965 - val_loss: 1.2221 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9700 - val_prc: 0.4580\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.2955e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9743 - prc: 0.4865 - val_loss: 1.2364 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9698 - val_prc: 0.4512\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.2869e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9741 - prc: 0.4832 - val_loss: 1.2459 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9701 - val_prc: 0.4312\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.2744e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9755 - prc: 0.4618 - val_loss: 1.2490 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9704 - val_prc: 0.4063\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.2570e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9771 - prc: 0.4724 - val_loss: 1.2496 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9707 - val_prc: 0.4043\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.2463e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9788 - prc: 0.4554 - val_loss: 1.2486 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9706 - val_prc: 0.4011\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.2370e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9786 - prc: 0.4562 - val_loss: 1.2457 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9716 - val_prc: 0.3951\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.2292e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9786 - prc: 0.4488 - val_loss: 1.2404 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9717 - val_prc: 0.4002\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.2278e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9779 - prc: 0.4540 - val_loss: 1.2356 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9718 - val_prc: 0.3964\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.2214e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9789 - prc: 0.4594 - val_loss: 1.2305 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9716 - val_prc: 0.3965\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 1.2020e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9808 - prc: 0.4510 - val_loss: 1.2218 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9719 - val_prc: 0.4024\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 1.2035e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9792 - prc: 0.4575 - val_loss: 1.2174 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9725 - val_prc: 0.4014\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 1.1864e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9814 - prc: 0.4499 - val_loss: 1.2088 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9726 - val_prc: 0.4020\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.1823e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9812 - prc: 0.4676 - val_loss: 1.2000 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9729 - val_prc: 0.4021\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.1714e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9825 - prc: 0.4630 - val_loss: 1.1922 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9729 - val_prc: 0.4067\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.1702e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9818 - prc: 0.4719 - val_loss: 1.1845 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9731 - val_prc: 0.4069\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.09999999999999999\n",
            "****************************** w =  4\n",
            "Model: \"sequential_76\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_76 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_76 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_76 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_76 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87/89 [============================>.] - ETA: 0s - loss: 2.5488e-04 - accuracy: 0.0064 - precision: 0.0017 - recall: 1.0000 - auc: 0.9393 - prc: 0.3436"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 20ms/step - loss: 2.5203e-04 - accuracy: 0.0064 - precision: 0.0017 - recall: 1.0000 - auc: 0.9398 - prc: 0.3473 - val_loss: 0.9804 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9654 - val_prc: 0.4978\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 1.8769e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9685 - prc: 0.5528 - val_loss: 1.0482 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9676 - val_prc: 0.4966\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.7754e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9705 - prc: 0.5414 - val_loss: 1.0756 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9682 - val_prc: 0.5147\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.7239e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9758 - prc: 0.5455 - val_loss: 1.0883 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9693 - val_prc: 0.5118\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.6906e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9759 - prc: 0.5578 - val_loss: 1.0917 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9695 - val_prc: 0.5188\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.6748e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9761 - prc: 0.5412 - val_loss: 1.0886 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9700 - val_prc: 0.5227\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.6479e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9783 - prc: 0.5519 - val_loss: 1.0801 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9703 - val_prc: 0.5164\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.6279e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9792 - prc: 0.5581 - val_loss: 1.0699 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9708 - val_prc: 0.5220\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.6035e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9805 - prc: 0.5545 - val_loss: 1.0563 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9708 - val_prc: 0.5173\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.5984e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9799 - prc: 0.5624 - val_loss: 1.0487 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9715 - val_prc: 0.5177\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.5789e-04 - accuracy: 0.0020 - precision: 0.0018 - recall: 1.0000 - auc: 0.9803 - prc: 0.5552 - val_loss: 1.0365 - val_accuracy: 0.0015 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9716 - val_prc: 0.5140\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.5684e-04 - accuracy: 0.0025 - precision: 0.0018 - recall: 1.0000 - auc: 0.9806 - prc: 0.5508 - val_loss: 1.0248 - val_accuracy: 0.0019 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9723 - val_prc: 0.4919\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 1.5513e-04 - accuracy: 0.0042 - precision: 0.0018 - recall: 1.0000 - auc: 0.9811 - prc: 0.5427 - val_loss: 1.0114 - val_accuracy: 0.0033 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9723 - val_prc: 0.4751\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 1.5223e-04 - accuracy: 0.0069 - precision: 0.0018 - recall: 1.0000 - auc: 0.9834 - prc: 0.5430 - val_loss: 0.9964 - val_accuracy: 0.0063 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9723 - val_prc: 0.4812\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.5224e-04 - accuracy: 0.0143 - precision: 0.0018 - recall: 1.0000 - auc: 0.9822 - prc: 0.5416 - val_loss: 0.9862 - val_accuracy: 0.0115 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9725 - val_prc: 0.4692\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.5061e-04 - accuracy: 0.0234 - precision: 0.0018 - recall: 1.0000 - auc: 0.9827 - prc: 0.5417 - val_loss: 0.9748 - val_accuracy: 0.0209 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9726 - val_prc: 0.4654\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.4956e-04 - accuracy: 0.0353 - precision: 0.0018 - recall: 1.0000 - auc: 0.9827 - prc: 0.5419 - val_loss: 0.9634 - val_accuracy: 0.0355 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9729 - val_prc: 0.4661\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.4860e-04 - accuracy: 0.0541 - precision: 0.0019 - recall: 1.0000 - auc: 0.9825 - prc: 0.5442 - val_loss: 0.9538 - val_accuracy: 0.0549 - val_precision: 0.0014 - val_recall: 0.9841 - val_auc: 0.9729 - val_prc: 0.4609\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.4659e-04 - accuracy: 0.0743 - precision: 0.0019 - recall: 1.0000 - auc: 0.9840 - prc: 0.5449 - val_loss: 0.9415 - val_accuracy: 0.0825 - val_precision: 0.0015 - val_recall: 0.9841 - val_auc: 0.9732 - val_prc: 0.4564\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.4563e-04 - accuracy: 0.1070 - precision: 0.0020 - recall: 1.0000 - auc: 0.9836 - prc: 0.5466 - val_loss: 0.9307 - val_accuracy: 0.1095 - val_precision: 0.0015 - val_recall: 0.9841 - val_auc: 0.9732 - val_prc: 0.4576\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.10999999999999999\n",
            "****************************** w =  4\n",
            "Model: \"sequential_77\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_77 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_77 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_77 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_77 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88/89 [============================>.] - ETA: 0s - loss: 3.1185e-04 - accuracy: 0.0600 - precision: 0.0018 - recall: 0.9766 - auc: 0.9249 - prc: 0.4197"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 16ms/step - loss: 3.1059e-04 - accuracy: 0.0597 - precision: 0.0018 - recall: 0.9767 - auc: 0.9251 - prc: 0.4224 - val_loss: 0.9204 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9596 - val_prc: 0.5624\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 2.2990e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9693 - prc: 0.6038 - val_loss: 0.9594 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9630 - val_prc: 0.5740\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 2.2045e-04 - accuracy: 0.0019 - precision: 0.0018 - recall: 1.0000 - auc: 0.9704 - prc: 0.6265 - val_loss: 0.9649 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9648 - val_prc: 0.5857\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.1390e-04 - accuracy: 0.0021 - precision: 0.0018 - recall: 1.0000 - auc: 0.9771 - prc: 0.6388 - val_loss: 0.9579 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9667 - val_prc: 0.6021\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.1044e-04 - accuracy: 0.0026 - precision: 0.0018 - recall: 1.0000 - auc: 0.9752 - prc: 0.6360 - val_loss: 0.9403 - val_accuracy: 0.0017 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9680 - val_prc: 0.6092\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.0716e-04 - accuracy: 0.0058 - precision: 0.0018 - recall: 1.0000 - auc: 0.9758 - prc: 0.6535 - val_loss: 0.9226 - val_accuracy: 0.0041 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9677 - val_prc: 0.6138\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.0165e-04 - accuracy: 0.0147 - precision: 0.0018 - recall: 1.0000 - auc: 0.9804 - prc: 0.6566 - val_loss: 0.8994 - val_accuracy: 0.0159 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9691 - val_prc: 0.6306\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.9983e-04 - accuracy: 0.0378 - precision: 0.0018 - recall: 1.0000 - auc: 0.9801 - prc: 0.6546 - val_loss: 0.8796 - val_accuracy: 0.0474 - val_precision: 0.0015 - val_recall: 1.0000 - val_auc: 0.9690 - val_prc: 0.6299\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.9558e-04 - accuracy: 0.0929 - precision: 0.0020 - recall: 1.0000 - auc: 0.9815 - prc: 0.6700 - val_loss: 0.8613 - val_accuracy: 0.1022 - val_precision: 0.0015 - val_recall: 0.9841 - val_auc: 0.9700 - val_prc: 0.6314\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.9301e-04 - accuracy: 0.1395 - precision: 0.0021 - recall: 1.0000 - auc: 0.9810 - prc: 0.6721 - val_loss: 0.8404 - val_accuracy: 0.1741 - val_precision: 0.0017 - val_recall: 0.9841 - val_auc: 0.9699 - val_prc: 0.6377\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.8964e-04 - accuracy: 0.2307 - precision: 0.0023 - recall: 1.0000 - auc: 0.9823 - prc: 0.6709 - val_loss: 0.8239 - val_accuracy: 0.2439 - val_precision: 0.0018 - val_recall: 0.9841 - val_auc: 0.9700 - val_prc: 0.6386\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.8706e-04 - accuracy: 0.2921 - precision: 0.0025 - recall: 0.9969 - auc: 0.9834 - prc: 0.6718 - val_loss: 0.8084 - val_accuracy: 0.3103 - val_precision: 0.0020 - val_recall: 0.9841 - val_auc: 0.9704 - val_prc: 0.6305\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.8520e-04 - accuracy: 0.3575 - precision: 0.0028 - recall: 0.9969 - auc: 0.9815 - prc: 0.6690 - val_loss: 0.7924 - val_accuracy: 0.3786 - val_precision: 0.0022 - val_recall: 0.9841 - val_auc: 0.9709 - val_prc: 0.6315\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.8211e-04 - accuracy: 0.4013 - precision: 0.0029 - recall: 0.9938 - auc: 0.9835 - prc: 0.6486 - val_loss: 0.7754 - val_accuracy: 0.4476 - val_precision: 0.0025 - val_recall: 0.9841 - val_auc: 0.9708 - val_prc: 0.6124\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.8022e-04 - accuracy: 0.4743 - precision: 0.0034 - recall: 0.9969 - auc: 0.9843 - prc: 0.6546 - val_loss: 0.7624 - val_accuracy: 0.5012 - val_precision: 0.0027 - val_recall: 0.9841 - val_auc: 0.9711 - val_prc: 0.5695\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.7697e-04 - accuracy: 0.5339 - precision: 0.0038 - recall: 0.9938 - auc: 0.9841 - prc: 0.6391 - val_loss: 0.7490 - val_accuracy: 0.5451 - val_precision: 0.0030 - val_recall: 0.9841 - val_auc: 0.9715 - val_prc: 0.5711\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.7448e-04 - accuracy: 0.5479 - precision: 0.0039 - recall: 0.9938 - auc: 0.9848 - prc: 0.6388 - val_loss: 0.7336 - val_accuracy: 0.5882 - val_precision: 0.0033 - val_recall: 0.9683 - val_auc: 0.9712 - val_prc: 0.5667\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.7348e-04 - accuracy: 0.6089 - precision: 0.0045 - recall: 0.9876 - auc: 0.9845 - prc: 0.6358 - val_loss: 0.7269 - val_accuracy: 0.6066 - val_precision: 0.0034 - val_recall: 0.9683 - val_auc: 0.9713 - val_prc: 0.5592\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.7198e-04 - accuracy: 0.6108 - precision: 0.0045 - recall: 0.9907 - auc: 0.9843 - prc: 0.6433 - val_loss: 0.7124 - val_accuracy: 0.6362 - val_precision: 0.0037 - val_recall: 0.9683 - val_auc: 0.9717 - val_prc: 0.5600\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.6975e-04 - accuracy: 0.6580 - precision: 0.0051 - recall: 0.9907 - auc: 0.9846 - prc: 0.6402 - val_loss: 0.7059 - val_accuracy: 0.6488 - val_precision: 0.0038 - val_recall: 0.9683 - val_auc: 0.9717 - val_prc: 0.5522\n",
            "1419/1419 [==============================] - 4s 3ms/step\n",
            "****************************** w =  0.11999999999999998\n",
            "****************************** w =  4\n",
            "Model: \"sequential_78\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_78 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_78 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_78 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_78 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "84/89 [===========================>..] - ETA: 0s - loss: 3.4315e-04 - accuracy: 0.1472 - precision: 0.0020 - recall: 0.9891 - auc: 0.9394 - prc: 0.4954"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 4s 21ms/step - loss: 3.3836e-04 - accuracy: 0.1411 - precision: 0.0020 - recall: 0.9896 - auc: 0.9411 - prc: 0.5052 - val_loss: 0.8773 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9694 - val_prc: 0.5658\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 2.8072e-04 - accuracy: 0.0020 - precision: 0.0018 - recall: 1.0000 - auc: 0.9629 - prc: 0.6373 - val_loss: 0.8744 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9748 - val_prc: 0.5975\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 2s 18ms/step - loss: 2.6846e-04 - accuracy: 0.0039 - precision: 0.0018 - recall: 1.0000 - auc: 0.9687 - prc: 0.6641 - val_loss: 0.8482 - val_accuracy: 0.0022 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9758 - val_prc: 0.6243\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 2.6065e-04 - accuracy: 0.0321 - precision: 0.0018 - recall: 1.0000 - auc: 0.9731 - prc: 0.6848 - val_loss: 0.8226 - val_accuracy: 0.0359 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9774 - val_prc: 0.6602\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 2.5474e-04 - accuracy: 0.1232 - precision: 0.0020 - recall: 1.0000 - auc: 0.9743 - prc: 0.7009 - val_loss: 0.7972 - val_accuracy: 0.1568 - val_precision: 0.0016 - val_recall: 1.0000 - val_auc: 0.9773 - val_prc: 0.6673\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 2.4694e-04 - accuracy: 0.2609 - precision: 0.0024 - recall: 1.0000 - auc: 0.9776 - prc: 0.7148 - val_loss: 0.7693 - val_accuracy: 0.3300 - val_precision: 0.0021 - val_recall: 1.0000 - val_auc: 0.9782 - val_prc: 0.6774\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 2s 24ms/step - loss: 2.4205e-04 - accuracy: 0.4294 - precision: 0.0031 - recall: 0.9969 - auc: 0.9782 - prc: 0.7191 - val_loss: 0.7462 - val_accuracy: 0.4672 - val_precision: 0.0026 - val_recall: 0.9841 - val_auc: 0.9780 - val_prc: 0.6837\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 2.3755e-04 - accuracy: 0.5254 - precision: 0.0037 - recall: 0.9907 - auc: 0.9782 - prc: 0.7210 - val_loss: 0.7217 - val_accuracy: 0.5776 - val_precision: 0.0032 - val_recall: 0.9841 - val_auc: 0.9771 - val_prc: 0.6878\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 2.3237e-04 - accuracy: 0.6032 - precision: 0.0044 - recall: 0.9876 - auc: 0.9808 - prc: 0.7249 - val_loss: 0.7000 - val_accuracy: 0.6520 - val_precision: 0.0039 - val_recall: 0.9841 - val_auc: 0.9768 - val_prc: 0.6897\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 2.2788e-04 - accuracy: 0.6729 - precision: 0.0054 - recall: 0.9907 - auc: 0.9813 - prc: 0.7260 - val_loss: 0.6786 - val_accuracy: 0.7047 - val_precision: 0.0046 - val_recall: 0.9841 - val_auc: 0.9768 - val_prc: 0.6911\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.2417e-04 - accuracy: 0.7178 - precision: 0.0062 - recall: 0.9845 - auc: 0.9818 - prc: 0.7295 - val_loss: 0.6590 - val_accuracy: 0.7424 - val_precision: 0.0053 - val_recall: 0.9841 - val_auc: 0.9765 - val_prc: 0.6927\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 2s 19ms/step - loss: 2.2023e-04 - accuracy: 0.7468 - precision: 0.0069 - recall: 0.9876 - auc: 0.9838 - prc: 0.7268 - val_loss: 0.6417 - val_accuracy: 0.7698 - val_precision: 0.0059 - val_recall: 0.9841 - val_auc: 0.9762 - val_prc: 0.6929\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 2.1694e-04 - accuracy: 0.7788 - precision: 0.0077 - recall: 0.9659 - auc: 0.9833 - prc: 0.7247 - val_loss: 0.6259 - val_accuracy: 0.7907 - val_precision: 0.0064 - val_recall: 0.9683 - val_auc: 0.9761 - val_prc: 0.6941\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 2.1289e-04 - accuracy: 0.7990 - precision: 0.0086 - recall: 0.9783 - auc: 0.9838 - prc: 0.7288 - val_loss: 0.6103 - val_accuracy: 0.8106 - val_precision: 0.0070 - val_recall: 0.9683 - val_auc: 0.9758 - val_prc: 0.6864\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 2.0907e-04 - accuracy: 0.8198 - precision: 0.0095 - recall: 0.9721 - auc: 0.9848 - prc: 0.7308 - val_loss: 0.5962 - val_accuracy: 0.8243 - val_precision: 0.0076 - val_recall: 0.9683 - val_auc: 0.9759 - val_prc: 0.6857\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 2s 19ms/step - loss: 2.0765e-04 - accuracy: 0.8291 - precision: 0.0100 - recall: 0.9721 - auc: 0.9841 - prc: 0.7170 - val_loss: 0.5845 - val_accuracy: 0.8322 - val_precision: 0.0079 - val_recall: 0.9683 - val_auc: 0.9756 - val_prc: 0.6887\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 2s 22ms/step - loss: 2.0513e-04 - accuracy: 0.8384 - precision: 0.0106 - recall: 0.9721 - auc: 0.9830 - prc: 0.7186 - val_loss: 0.5736 - val_accuracy: 0.8394 - val_precision: 0.0083 - val_recall: 0.9683 - val_auc: 0.9755 - val_prc: 0.6652\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 2s 21ms/step - loss: 2.0075e-04 - accuracy: 0.8398 - precision: 0.0107 - recall: 0.9690 - auc: 0.9857 - prc: 0.7127 - val_loss: 0.5593 - val_accuracy: 0.8489 - val_precision: 0.0088 - val_recall: 0.9683 - val_auc: 0.9753 - val_prc: 0.6545\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 2s 17ms/step - loss: 1.9883e-04 - accuracy: 0.8569 - precision: 0.0118 - recall: 0.9628 - auc: 0.9850 - prc: 0.7038 - val_loss: 0.5509 - val_accuracy: 0.8524 - val_precision: 0.0090 - val_recall: 0.9683 - val_auc: 0.9752 - val_prc: 0.6085\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 2s 23ms/step - loss: 1.9697e-04 - accuracy: 0.8523 - precision: 0.0116 - recall: 0.9721 - auc: 0.9851 - prc: 0.6852 - val_loss: 0.5400 - val_accuracy: 0.8580 - val_precision: 0.0092 - val_recall: 0.9524 - val_auc: 0.9752 - val_prc: 0.6001\n",
            "1419/1419 [==============================] - 6s 4ms/step\n",
            "****************************** w =  0.12999999999999998\n",
            "****************************** w =  4\n",
            "Model: \"sequential_79\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_79 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_79 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_79 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_79 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89/89 [==============================] - ETA: 0s - loss: 3.9155e-04 - accuracy: 0.1896 - precision: 0.0021 - recall: 0.9870 - auc: 0.9485 - prc: 0.5619"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 4s 24ms/step - loss: 3.9155e-04 - accuracy: 0.1896 - precision: 0.0021 - recall: 0.9870 - auc: 0.9485 - prc: 0.5619 - val_loss: 0.8444 - val_accuracy: 0.0074 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9578 - val_prc: 0.6332\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 3.3746e-04 - accuracy: 0.0407 - precision: 0.0019 - recall: 1.0000 - auc: 0.9637 - prc: 0.7059 - val_loss: 0.7999 - val_accuracy: 0.0721 - val_precision: 0.0015 - val_recall: 0.9841 - val_auc: 0.9654 - val_prc: 0.6618\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 3.1842e-04 - accuracy: 0.2010 - precision: 0.0022 - recall: 0.9969 - auc: 0.9711 - prc: 0.7209 - val_loss: 0.7492 - val_accuracy: 0.3248 - val_precision: 0.0020 - val_recall: 0.9841 - val_auc: 0.9687 - val_prc: 0.6739\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 3.0434e-04 - accuracy: 0.4743 - precision: 0.0033 - recall: 0.9876 - auc: 0.9725 - prc: 0.7355 - val_loss: 0.7059 - val_accuracy: 0.5718 - val_precision: 0.0032 - val_recall: 0.9841 - val_auc: 0.9709 - val_prc: 0.6759\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 2s 19ms/step - loss: 2.9229e-04 - accuracy: 0.6423 - precision: 0.0049 - recall: 0.9814 - auc: 0.9769 - prc: 0.7393 - val_loss: 0.6621 - val_accuracy: 0.7313 - val_precision: 0.0051 - val_recall: 0.9841 - val_auc: 0.9715 - val_prc: 0.6933\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 2.8224e-04 - accuracy: 0.7756 - precision: 0.0077 - recall: 0.9752 - auc: 0.9769 - prc: 0.7456 - val_loss: 0.6289 - val_accuracy: 0.8061 - val_precision: 0.0068 - val_recall: 0.9524 - val_auc: 0.9726 - val_prc: 0.6934\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 2.7434e-04 - accuracy: 0.8195 - precision: 0.0094 - recall: 0.9659 - auc: 0.9771 - prc: 0.7447 - val_loss: 0.5959 - val_accuracy: 0.8533 - val_precision: 0.0089 - val_recall: 0.9524 - val_auc: 0.9733 - val_prc: 0.6977\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 2.6612e-04 - accuracy: 0.8590 - precision: 0.0120 - recall: 0.9628 - auc: 0.9783 - prc: 0.7476 - val_loss: 0.5664 - val_accuracy: 0.8816 - val_precision: 0.0109 - val_recall: 0.9365 - val_auc: 0.9734 - val_prc: 0.7005\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 2.5878e-04 - accuracy: 0.8856 - precision: 0.0146 - recall: 0.9505 - auc: 0.9799 - prc: 0.7481 - val_loss: 0.5419 - val_accuracy: 0.8966 - val_precision: 0.0124 - val_recall: 0.9365 - val_auc: 0.9734 - val_prc: 0.6991\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 2.5251e-04 - accuracy: 0.8983 - precision: 0.0164 - recall: 0.9505 - auc: 0.9817 - prc: 0.7450 - val_loss: 0.5219 - val_accuracy: 0.9058 - val_precision: 0.0136 - val_recall: 0.9365 - val_auc: 0.9740 - val_prc: 0.7047\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 2.4504e-04 - accuracy: 0.9113 - precision: 0.0187 - recall: 0.9505 - auc: 0.9814 - prc: 0.7517 - val_loss: 0.5026 - val_accuracy: 0.9127 - val_precision: 0.0147 - val_recall: 0.9365 - val_auc: 0.9739 - val_prc: 0.7040\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 2.4081e-04 - accuracy: 0.9171 - precision: 0.0197 - recall: 0.9381 - auc: 0.9816 - prc: 0.7479 - val_loss: 0.4868 - val_accuracy: 0.9164 - val_precision: 0.0153 - val_recall: 0.9365 - val_auc: 0.9740 - val_prc: 0.7026\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 2.3637e-04 - accuracy: 0.9178 - precision: 0.0199 - recall: 0.9350 - auc: 0.9822 - prc: 0.7496 - val_loss: 0.4705 - val_accuracy: 0.9203 - val_precision: 0.0161 - val_recall: 0.9365 - val_auc: 0.9742 - val_prc: 0.7022\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 2.3000e-04 - accuracy: 0.9270 - precision: 0.0224 - recall: 0.9381 - auc: 0.9835 - prc: 0.7508 - val_loss: 0.4593 - val_accuracy: 0.9212 - val_precision: 0.0162 - val_recall: 0.9365 - val_auc: 0.9742 - val_prc: 0.7153\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 2.2780e-04 - accuracy: 0.9233 - precision: 0.0214 - recall: 0.9443 - auc: 0.9823 - prc: 0.7408 - val_loss: 0.4452 - val_accuracy: 0.9236 - val_precision: 0.0168 - val_recall: 0.9365 - val_auc: 0.9742 - val_prc: 0.7174\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 2.2282e-04 - accuracy: 0.9259 - precision: 0.0221 - recall: 0.9412 - auc: 0.9837 - prc: 0.7347 - val_loss: 0.4308 - val_accuracy: 0.9268 - val_precision: 0.0172 - val_recall: 0.9206 - val_auc: 0.9742 - val_prc: 0.7195\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 2.2065e-04 - accuracy: 0.9270 - precision: 0.0224 - recall: 0.9381 - auc: 0.9837 - prc: 0.7249 - val_loss: 0.4210 - val_accuracy: 0.9270 - val_precision: 0.0172 - val_recall: 0.9206 - val_auc: 0.9742 - val_prc: 0.7061\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.1544e-04 - accuracy: 0.9293 - precision: 0.0232 - recall: 0.9443 - auc: 0.9843 - prc: 0.7226 - val_loss: 0.4116 - val_accuracy: 0.9273 - val_precision: 0.0173 - val_recall: 0.9206 - val_auc: 0.9740 - val_prc: 0.6641\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.1363e-04 - accuracy: 0.9273 - precision: 0.0225 - recall: 0.9381 - auc: 0.9843 - prc: 0.7074 - val_loss: 0.4005 - val_accuracy: 0.9289 - val_precision: 0.0177 - val_recall: 0.9206 - val_auc: 0.9739 - val_prc: 0.6640\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.1165e-04 - accuracy: 0.9308 - precision: 0.0238 - recall: 0.9474 - auc: 0.9835 - prc: 0.7072 - val_loss: 0.3945 - val_accuracy: 0.9279 - val_precision: 0.0174 - val_recall: 0.9206 - val_auc: 0.9739 - val_prc: 0.6431\n",
            "1419/1419 [==============================] - 4s 2ms/step\n",
            "****************************** w =  0.13999999999999999\n",
            "****************************** w =  4\n",
            "Model: \"sequential_80\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_80 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_80 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_80 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_80 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88/89 [============================>.] - ETA: 0s - loss: 4.5119e-04 - accuracy: 0.3428 - precision: 0.0025 - recall: 0.9738 - auc: 0.9576 - prc: 0.5899"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 16ms/step - loss: 4.5371e-04 - accuracy: 0.3426 - precision: 0.0025 - recall: 0.9741 - auc: 0.9565 - prc: 0.5894 - val_loss: 0.7420 - val_accuracy: 0.2885 - val_precision: 0.0019 - val_recall: 1.0000 - val_auc: 0.9659 - val_prc: 0.6471\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.8734e-04 - accuracy: 0.4821 - precision: 0.0034 - recall: 0.9845 - auc: 0.9716 - prc: 0.7182 - val_loss: 0.6770 - val_accuracy: 0.7069 - val_precision: 0.0045 - val_recall: 0.9524 - val_auc: 0.9671 - val_prc: 0.6689\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.6263e-04 - accuracy: 0.7847 - precision: 0.0078 - recall: 0.9536 - auc: 0.9673 - prc: 0.7230 - val_loss: 0.6192 - val_accuracy: 0.8587 - val_precision: 0.0093 - val_recall: 0.9524 - val_auc: 0.9675 - val_prc: 0.6777\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.4105e-04 - accuracy: 0.8844 - precision: 0.0142 - recall: 0.9319 - auc: 0.9732 - prc: 0.7297 - val_loss: 0.5687 - val_accuracy: 0.9093 - val_precision: 0.0141 - val_recall: 0.9365 - val_auc: 0.9685 - val_prc: 0.6832\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.2486e-04 - accuracy: 0.9207 - precision: 0.0203 - recall: 0.9226 - auc: 0.9753 - prc: 0.7405 - val_loss: 0.5259 - val_accuracy: 0.9329 - val_precision: 0.0187 - val_recall: 0.9206 - val_auc: 0.9689 - val_prc: 0.6881\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.1180e-04 - accuracy: 0.9401 - precision: 0.0266 - recall: 0.9164 - auc: 0.9744 - prc: 0.7447 - val_loss: 0.4906 - val_accuracy: 0.9451 - val_precision: 0.0228 - val_recall: 0.9206 - val_auc: 0.9698 - val_prc: 0.6942\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.9901e-04 - accuracy: 0.9465 - precision: 0.0299 - recall: 0.9257 - auc: 0.9798 - prc: 0.7490 - val_loss: 0.4577 - val_accuracy: 0.9532 - val_precision: 0.0266 - val_recall: 0.9206 - val_auc: 0.9704 - val_prc: 0.6960\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 2.9011e-04 - accuracy: 0.9604 - precision: 0.0394 - recall: 0.9102 - auc: 0.9770 - prc: 0.7507 - val_loss: 0.4401 - val_accuracy: 0.9537 - val_precision: 0.0269 - val_recall: 0.9206 - val_auc: 0.9720 - val_prc: 0.6983\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 2.8010e-04 - accuracy: 0.9583 - precision: 0.0380 - recall: 0.9226 - auc: 0.9790 - prc: 0.7442 - val_loss: 0.4167 - val_accuracy: 0.9579 - val_precision: 0.0296 - val_recall: 0.9206 - val_auc: 0.9723 - val_prc: 0.7013\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 2.7257e-04 - accuracy: 0.9589 - precision: 0.0383 - recall: 0.9164 - auc: 0.9794 - prc: 0.7508 - val_loss: 0.3928 - val_accuracy: 0.9625 - val_precision: 0.0330 - val_recall: 0.9206 - val_auc: 0.9726 - val_prc: 0.7008\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.6563e-04 - accuracy: 0.9612 - precision: 0.0405 - recall: 0.9164 - auc: 0.9815 - prc: 0.7468 - val_loss: 0.3735 - val_accuracy: 0.9641 - val_precision: 0.0345 - val_recall: 0.9206 - val_auc: 0.9731 - val_prc: 0.7136\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.5864e-04 - accuracy: 0.9648 - precision: 0.0443 - recall: 0.9133 - auc: 0.9828 - prc: 0.7487 - val_loss: 0.3609 - val_accuracy: 0.9637 - val_precision: 0.0341 - val_recall: 0.9206 - val_auc: 0.9731 - val_prc: 0.7131\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.5202e-04 - accuracy: 0.9653 - precision: 0.0449 - recall: 0.9133 - auc: 0.9836 - prc: 0.7520 - val_loss: 0.3513 - val_accuracy: 0.9628 - val_precision: 0.0333 - val_recall: 0.9206 - val_auc: 0.9733 - val_prc: 0.7129\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.4913e-04 - accuracy: 0.9650 - precision: 0.0445 - recall: 0.9102 - auc: 0.9830 - prc: 0.7538 - val_loss: 0.3410 - val_accuracy: 0.9627 - val_precision: 0.0332 - val_recall: 0.9206 - val_auc: 0.9731 - val_prc: 0.7037\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.4193e-04 - accuracy: 0.9634 - precision: 0.0425 - recall: 0.9102 - auc: 0.9852 - prc: 0.7400 - val_loss: 0.3286 - val_accuracy: 0.9633 - val_precision: 0.0337 - val_recall: 0.9206 - val_auc: 0.9731 - val_prc: 0.7175\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.4127e-04 - accuracy: 0.9631 - precision: 0.0424 - recall: 0.9133 - auc: 0.9830 - prc: 0.7308 - val_loss: 0.3195 - val_accuracy: 0.9631 - val_precision: 0.0336 - val_recall: 0.9206 - val_auc: 0.9730 - val_prc: 0.7065\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 2.3388e-04 - accuracy: 0.9630 - precision: 0.0421 - recall: 0.9102 - auc: 0.9849 - prc: 0.7320 - val_loss: 0.3092 - val_accuracy: 0.9637 - val_precision: 0.0341 - val_recall: 0.9206 - val_auc: 0.9733 - val_prc: 0.7066\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.3133e-04 - accuracy: 0.9629 - precision: 0.0423 - recall: 0.9164 - auc: 0.9852 - prc: 0.7181 - val_loss: 0.3020 - val_accuracy: 0.9631 - val_precision: 0.0335 - val_recall: 0.9206 - val_auc: 0.9729 - val_prc: 0.6721\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.2912e-04 - accuracy: 0.9590 - precision: 0.0386 - recall: 0.9226 - auc: 0.9842 - prc: 0.7026 - val_loss: 0.2892 - val_accuracy: 0.9649 - val_precision: 0.0352 - val_recall: 0.9206 - val_auc: 0.9729 - val_prc: 0.6646\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.2581e-04 - accuracy: 0.9651 - precision: 0.0445 - recall: 0.9102 - auc: 0.9842 - prc: 0.6982 - val_loss: 0.2874 - val_accuracy: 0.9627 - val_precision: 0.0332 - val_recall: 0.9206 - val_auc: 0.9728 - val_prc: 0.6434\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.15\n",
            "****************************** w =  4\n",
            "Model: \"sequential_81\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_81 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_81 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_81 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_81 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86/89 [===========================>..] - ETA: 0s - loss: 5.3619e-04 - accuracy: 0.4139 - precision: 0.0027 - recall: 0.9486 - auc: 0.9480 - prc: 0.5830"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 16ms/step - loss: 5.3670e-04 - accuracy: 0.4163 - precision: 0.0028 - recall: 0.9482 - auc: 0.9480 - prc: 0.5866 - val_loss: 0.7061 - val_accuracy: 0.5438 - val_precision: 0.0029 - val_recall: 0.9683 - val_auc: 0.9595 - val_prc: 0.6516\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.6156e-04 - accuracy: 0.7398 - precision: 0.0064 - recall: 0.9381 - auc: 0.9480 - prc: 0.7198 - val_loss: 0.6114 - val_accuracy: 0.8764 - val_precision: 0.0104 - val_recall: 0.9365 - val_auc: 0.9611 - val_prc: 0.6691\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.2112e-04 - accuracy: 0.9145 - precision: 0.0187 - recall: 0.9133 - auc: 0.9520 - prc: 0.7305 - val_loss: 0.5399 - val_accuracy: 0.9451 - val_precision: 0.0228 - val_recall: 0.9206 - val_auc: 0.9615 - val_prc: 0.6762\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.9016e-04 - accuracy: 0.9515 - precision: 0.0326 - recall: 0.9164 - auc: 0.9579 - prc: 0.7354 - val_loss: 0.4793 - val_accuracy: 0.9662 - val_precision: 0.0365 - val_recall: 0.9206 - val_auc: 0.9629 - val_prc: 0.6816\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.6591e-04 - accuracy: 0.9677 - precision: 0.0475 - recall: 0.9009 - auc: 0.9610 - prc: 0.7359 - val_loss: 0.4314 - val_accuracy: 0.9732 - val_precision: 0.0457 - val_recall: 0.9206 - val_auc: 0.9647 - val_prc: 0.6867\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 3.4561e-04 - accuracy: 0.9740 - precision: 0.0580 - recall: 0.8947 - auc: 0.9648 - prc: 0.7433 - val_loss: 0.3932 - val_accuracy: 0.9767 - val_precision: 0.0514 - val_recall: 0.9048 - val_auc: 0.9657 - val_prc: 0.6950\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 3.3304e-04 - accuracy: 0.9779 - precision: 0.0672 - recall: 0.8885 - auc: 0.9658 - prc: 0.7453 - val_loss: 0.3677 - val_accuracy: 0.9775 - val_precision: 0.0531 - val_recall: 0.9048 - val_auc: 0.9665 - val_prc: 0.6981\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 3.1939e-04 - accuracy: 0.9773 - precision: 0.0659 - recall: 0.8916 - auc: 0.9701 - prc: 0.7516 - val_loss: 0.3402 - val_accuracy: 0.9789 - val_precision: 0.0564 - val_recall: 0.9048 - val_auc: 0.9674 - val_prc: 0.7101\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 3.0598e-04 - accuracy: 0.9788 - precision: 0.0699 - recall: 0.8854 - auc: 0.9752 - prc: 0.7496 - val_loss: 0.3184 - val_accuracy: 0.9797 - val_precision: 0.0585 - val_recall: 0.9048 - val_auc: 0.9683 - val_prc: 0.7139\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 2.9711e-04 - accuracy: 0.9800 - precision: 0.0740 - recall: 0.8885 - auc: 0.9768 - prc: 0.7517 - val_loss: 0.3044 - val_accuracy: 0.9791 - val_precision: 0.0569 - val_recall: 0.9048 - val_auc: 0.9689 - val_prc: 0.7159\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.8816e-04 - accuracy: 0.9791 - precision: 0.0709 - recall: 0.8885 - auc: 0.9782 - prc: 0.7483 - val_loss: 0.2877 - val_accuracy: 0.9793 - val_precision: 0.0575 - val_recall: 0.9048 - val_auc: 0.9693 - val_prc: 0.7282\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.8075e-04 - accuracy: 0.9789 - precision: 0.0704 - recall: 0.8885 - auc: 0.9802 - prc: 0.7523 - val_loss: 0.2745 - val_accuracy: 0.9793 - val_precision: 0.0575 - val_recall: 0.9048 - val_auc: 0.9696 - val_prc: 0.7278\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.7130e-04 - accuracy: 0.9792 - precision: 0.0714 - recall: 0.8885 - auc: 0.9829 - prc: 0.7526 - val_loss: 0.2632 - val_accuracy: 0.9789 - val_precision: 0.0566 - val_recall: 0.9048 - val_auc: 0.9696 - val_prc: 0.7319\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.6670e-04 - accuracy: 0.9786 - precision: 0.0698 - recall: 0.8947 - auc: 0.9833 - prc: 0.7477 - val_loss: 0.2539 - val_accuracy: 0.9785 - val_precision: 0.0554 - val_recall: 0.9048 - val_auc: 0.9701 - val_prc: 0.7298\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.6166e-04 - accuracy: 0.9788 - precision: 0.0703 - recall: 0.8947 - auc: 0.9822 - prc: 0.7453 - val_loss: 0.2473 - val_accuracy: 0.9779 - val_precision: 0.0542 - val_recall: 0.9048 - val_auc: 0.9705 - val_prc: 0.7311\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.5743e-04 - accuracy: 0.9777 - precision: 0.0673 - recall: 0.8947 - auc: 0.9829 - prc: 0.7355 - val_loss: 0.2406 - val_accuracy: 0.9772 - val_precision: 0.0525 - val_recall: 0.9048 - val_auc: 0.9702 - val_prc: 0.7323\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.5039e-04 - accuracy: 0.9777 - precision: 0.0671 - recall: 0.8947 - auc: 0.9843 - prc: 0.7519 - val_loss: 0.2346 - val_accuracy: 0.9767 - val_precision: 0.0514 - val_recall: 0.9048 - val_auc: 0.9703 - val_prc: 0.7059\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.4806e-04 - accuracy: 0.9766 - precision: 0.0643 - recall: 0.8947 - auc: 0.9843 - prc: 0.7326 - val_loss: 0.2287 - val_accuracy: 0.9761 - val_precision: 0.0502 - val_recall: 0.9048 - val_auc: 0.9705 - val_prc: 0.7080\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 2.4477e-04 - accuracy: 0.9768 - precision: 0.0651 - recall: 0.9009 - auc: 0.9841 - prc: 0.7241 - val_loss: 0.2266 - val_accuracy: 0.9750 - val_precision: 0.0489 - val_recall: 0.9206 - val_auc: 0.9703 - val_prc: 0.6830\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 2.4224e-04 - accuracy: 0.9752 - precision: 0.0612 - recall: 0.9009 - auc: 0.9843 - prc: 0.7202 - val_loss: 0.2226 - val_accuracy: 0.9744 - val_precision: 0.0477 - val_recall: 0.9206 - val_auc: 0.9708 - val_prc: 0.6745\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.16\n",
            "****************************** w =  4\n",
            "Model: \"sequential_82\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_82 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_82 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_82 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_82 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88/89 [============================>.] - ETA: 0s - loss: 6.3784e-04 - accuracy: 0.6511 - precision: 0.0045 - recall: 0.9239 - auc: 0.9323 - prc: 0.6004"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 16ms/step - loss: 6.3652e-04 - accuracy: 0.6519 - precision: 0.0045 - recall: 0.9249 - auc: 0.9332 - prc: 0.6060 - val_loss: 0.6520 - val_accuracy: 0.8252 - val_precision: 0.0075 - val_recall: 0.9524 - val_auc: 0.9652 - val_prc: 0.6831\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.2041e-04 - accuracy: 0.9215 - precision: 0.0200 - recall: 0.9009 - auc: 0.9532 - prc: 0.7256 - val_loss: 0.5266 - val_accuracy: 0.9734 - val_precision: 0.0446 - val_recall: 0.8889 - val_auc: 0.9658 - val_prc: 0.6988\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.6344e-04 - accuracy: 0.9781 - precision: 0.0671 - recall: 0.8762 - auc: 0.9595 - prc: 0.7295 - val_loss: 0.4426 - val_accuracy: 0.9838 - val_precision: 0.0702 - val_recall: 0.8730 - val_auc: 0.9651 - val_prc: 0.7092\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 4.2260e-04 - accuracy: 0.9848 - precision: 0.0936 - recall: 0.8669 - auc: 0.9591 - prc: 0.7377 - val_loss: 0.3926 - val_accuracy: 0.9847 - val_precision: 0.0740 - val_recall: 0.8730 - val_auc: 0.9648 - val_prc: 0.7202\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 3.9356e-04 - accuracy: 0.9873 - precision: 0.1101 - recall: 0.8700 - auc: 0.9597 - prc: 0.7410 - val_loss: 0.3479 - val_accuracy: 0.9853 - val_precision: 0.0771 - val_recall: 0.8730 - val_auc: 0.9648 - val_prc: 0.7282\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 3.6769e-04 - accuracy: 0.9872 - precision: 0.1099 - recall: 0.8700 - auc: 0.9668 - prc: 0.7500 - val_loss: 0.3111 - val_accuracy: 0.9858 - val_precision: 0.0794 - val_recall: 0.8730 - val_auc: 0.9659 - val_prc: 0.7329\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.4980e-04 - accuracy: 0.9869 - precision: 0.1073 - recall: 0.8731 - auc: 0.9696 - prc: 0.7572 - val_loss: 0.2782 - val_accuracy: 0.9864 - val_precision: 0.0826 - val_recall: 0.8730 - val_auc: 0.9663 - val_prc: 0.7358\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.3497e-04 - accuracy: 0.9868 - precision: 0.1067 - recall: 0.8731 - auc: 0.9728 - prc: 0.7547 - val_loss: 0.2559 - val_accuracy: 0.9862 - val_precision: 0.0817 - val_recall: 0.8730 - val_auc: 0.9677 - val_prc: 0.7389\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.2247e-04 - accuracy: 0.9871 - precision: 0.1093 - recall: 0.8762 - auc: 0.9733 - prc: 0.7520 - val_loss: 0.2414 - val_accuracy: 0.9856 - val_precision: 0.0787 - val_recall: 0.8730 - val_auc: 0.9686 - val_prc: 0.7413\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.0885e-04 - accuracy: 0.9868 - precision: 0.1072 - recall: 0.8731 - auc: 0.9783 - prc: 0.7586 - val_loss: 0.2299 - val_accuracy: 0.9851 - val_precision: 0.0783 - val_recall: 0.9048 - val_auc: 0.9705 - val_prc: 0.7406\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.0244e-04 - accuracy: 0.9858 - precision: 0.1002 - recall: 0.8762 - auc: 0.9807 - prc: 0.7542 - val_loss: 0.2167 - val_accuracy: 0.9851 - val_precision: 0.0782 - val_recall: 0.9048 - val_auc: 0.9710 - val_prc: 0.7406\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.9201e-04 - accuracy: 0.9859 - precision: 0.1011 - recall: 0.8793 - auc: 0.9814 - prc: 0.7596 - val_loss: 0.2076 - val_accuracy: 0.9846 - val_precision: 0.0762 - val_recall: 0.9048 - val_auc: 0.9718 - val_prc: 0.7420\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.8611e-04 - accuracy: 0.9854 - precision: 0.0977 - recall: 0.8762 - auc: 0.9809 - prc: 0.7534 - val_loss: 0.1984 - val_accuracy: 0.9844 - val_precision: 0.0751 - val_recall: 0.9048 - val_auc: 0.9729 - val_prc: 0.7433\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 2s 17ms/step - loss: 2.8094e-04 - accuracy: 0.9854 - precision: 0.0978 - recall: 0.8793 - auc: 0.9826 - prc: 0.7563 - val_loss: 0.1947 - val_accuracy: 0.9836 - val_precision: 0.0718 - val_recall: 0.9048 - val_auc: 0.9731 - val_prc: 0.7427\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 2.7474e-04 - accuracy: 0.9842 - precision: 0.0923 - recall: 0.8885 - auc: 0.9836 - prc: 0.7552 - val_loss: 0.1869 - val_accuracy: 0.9834 - val_precision: 0.0707 - val_recall: 0.9048 - val_auc: 0.9731 - val_prc: 0.7428\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 2.6971e-04 - accuracy: 0.9842 - precision: 0.0920 - recall: 0.8885 - auc: 0.9839 - prc: 0.7465 - val_loss: 0.1843 - val_accuracy: 0.9828 - val_precision: 0.0683 - val_recall: 0.9048 - val_auc: 0.9732 - val_prc: 0.7296\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 2.6531e-04 - accuracy: 0.9831 - precision: 0.0866 - recall: 0.8916 - auc: 0.9836 - prc: 0.7516 - val_loss: 0.1770 - val_accuracy: 0.9827 - val_precision: 0.0682 - val_recall: 0.9048 - val_auc: 0.9731 - val_prc: 0.7180\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 2.6159e-04 - accuracy: 0.9826 - precision: 0.0845 - recall: 0.8916 - auc: 0.9841 - prc: 0.7309 - val_loss: 0.1719 - val_accuracy: 0.9825 - val_precision: 0.0673 - val_recall: 0.9048 - val_auc: 0.9733 - val_prc: 0.7073\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.5586e-04 - accuracy: 0.9822 - precision: 0.0824 - recall: 0.8885 - auc: 0.9855 - prc: 0.7221 - val_loss: 0.1685 - val_accuracy: 0.9820 - val_precision: 0.0657 - val_recall: 0.9048 - val_auc: 0.9731 - val_prc: 0.6981\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.5134e-04 - accuracy: 0.9823 - precision: 0.0829 - recall: 0.8916 - auc: 0.9858 - prc: 0.7231 - val_loss: 0.1648 - val_accuracy: 0.9816 - val_precision: 0.0642 - val_recall: 0.9048 - val_auc: 0.9730 - val_prc: 0.6652\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.17\n",
            "****************************** w =  4\n",
            "Model: \"sequential_83\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_83 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_83 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_83 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_83 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86/89 [===========================>..] - ETA: 0s - loss: 7.4733e-04 - accuracy: 0.7311 - precision: 0.0056 - recall: 0.9128 - auc: 0.9375 - prc: 0.5951"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 16ms/step - loss: 7.4458e-04 - accuracy: 0.7362 - precision: 0.0059 - recall: 0.9145 - auc: 0.9395 - prc: 0.6115 - val_loss: 0.5925 - val_accuracy: 0.9496 - val_precision: 0.0244 - val_recall: 0.9048 - val_auc: 0.9642 - val_prc: 0.6886\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.8237e-04 - accuracy: 0.9759 - precision: 0.0614 - recall: 0.8793 - auc: 0.9499 - prc: 0.7294 - val_loss: 0.4508 - val_accuracy: 0.9854 - val_precision: 0.0784 - val_recall: 0.8889 - val_auc: 0.9661 - val_prc: 0.7132\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.0039e-04 - accuracy: 0.9891 - precision: 0.1262 - recall: 0.8638 - auc: 0.9530 - prc: 0.7375 - val_loss: 0.3580 - val_accuracy: 0.9898 - val_precision: 0.1078 - val_recall: 0.8730 - val_auc: 0.9668 - val_prc: 0.7237\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.4467e-04 - accuracy: 0.9916 - precision: 0.1583 - recall: 0.8607 - auc: 0.9525 - prc: 0.7327 - val_loss: 0.3030 - val_accuracy: 0.9898 - val_precision: 0.1061 - val_recall: 0.8571 - val_auc: 0.9671 - val_prc: 0.7155\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 4.0649e-04 - accuracy: 0.9909 - precision: 0.1472 - recall: 0.8638 - auc: 0.9591 - prc: 0.7394 - val_loss: 0.2565 - val_accuracy: 0.9904 - val_precision: 0.1125 - val_recall: 0.8571 - val_auc: 0.9692 - val_prc: 0.7185\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.7918e-04 - accuracy: 0.9914 - precision: 0.1564 - recall: 0.8669 - auc: 0.9625 - prc: 0.7480 - val_loss: 0.2265 - val_accuracy: 0.9903 - val_precision: 0.1109 - val_recall: 0.8571 - val_auc: 0.9698 - val_prc: 0.7277\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.6026e-04 - accuracy: 0.9906 - precision: 0.1439 - recall: 0.8669 - auc: 0.9677 - prc: 0.7530 - val_loss: 0.2024 - val_accuracy: 0.9901 - val_precision: 0.1095 - val_recall: 0.8571 - val_auc: 0.9724 - val_prc: 0.7305\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 3.4006e-04 - accuracy: 0.9908 - precision: 0.1469 - recall: 0.8669 - auc: 0.9726 - prc: 0.7558 - val_loss: 0.1904 - val_accuracy: 0.9892 - val_precision: 0.1011 - val_recall: 0.8571 - val_auc: 0.9733 - val_prc: 0.7321\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 3.2777e-04 - accuracy: 0.9902 - precision: 0.1391 - recall: 0.8669 - auc: 0.9742 - prc: 0.7577 - val_loss: 0.1807 - val_accuracy: 0.9886 - val_precision: 0.0973 - val_recall: 0.8730 - val_auc: 0.9744 - val_prc: 0.7328\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 3.1513e-04 - accuracy: 0.9897 - precision: 0.1332 - recall: 0.8669 - auc: 0.9787 - prc: 0.7558 - val_loss: 0.1725 - val_accuracy: 0.9877 - val_precision: 0.0921 - val_recall: 0.8889 - val_auc: 0.9746 - val_prc: 0.7338\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.0616e-04 - accuracy: 0.9885 - precision: 0.1214 - recall: 0.8762 - auc: 0.9820 - prc: 0.7591 - val_loss: 0.1603 - val_accuracy: 0.9881 - val_precision: 0.0934 - val_recall: 0.8730 - val_auc: 0.9746 - val_prc: 0.7351\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 2.9936e-04 - accuracy: 0.9889 - precision: 0.1254 - recall: 0.8762 - auc: 0.9820 - prc: 0.7542 - val_loss: 0.1580 - val_accuracy: 0.9867 - val_precision: 0.0873 - val_recall: 0.9048 - val_auc: 0.9746 - val_prc: 0.7367\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 2.9354e-04 - accuracy: 0.9877 - precision: 0.1144 - recall: 0.8793 - auc: 0.9808 - prc: 0.7533 - val_loss: 0.1521 - val_accuracy: 0.9866 - val_precision: 0.0865 - val_recall: 0.9048 - val_auc: 0.9745 - val_prc: 0.7239\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.8617e-04 - accuracy: 0.9873 - precision: 0.1113 - recall: 0.8824 - auc: 0.9838 - prc: 0.7501 - val_loss: 0.1469 - val_accuracy: 0.9863 - val_precision: 0.0849 - val_recall: 0.9048 - val_auc: 0.9742 - val_prc: 0.7119\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.8055e-04 - accuracy: 0.9872 - precision: 0.1106 - recall: 0.8824 - auc: 0.9839 - prc: 0.7498 - val_loss: 0.1424 - val_accuracy: 0.9862 - val_precision: 0.0839 - val_recall: 0.9048 - val_auc: 0.9741 - val_prc: 0.7126\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.7777e-04 - accuracy: 0.9873 - precision: 0.1118 - recall: 0.8854 - auc: 0.9835 - prc: 0.7489 - val_loss: 0.1425 - val_accuracy: 0.9852 - val_precision: 0.0788 - val_recall: 0.9048 - val_auc: 0.9744 - val_prc: 0.7107\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.7446e-04 - accuracy: 0.9854 - precision: 0.0994 - recall: 0.8947 - auc: 0.9837 - prc: 0.7370 - val_loss: 0.1335 - val_accuracy: 0.9859 - val_precision: 0.0825 - val_recall: 0.9048 - val_auc: 0.9742 - val_prc: 0.7080\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 2.6928e-04 - accuracy: 0.9863 - precision: 0.1050 - recall: 0.8916 - auc: 0.9850 - prc: 0.7305 - val_loss: 0.1342 - val_accuracy: 0.9852 - val_precision: 0.0786 - val_recall: 0.9048 - val_auc: 0.9739 - val_prc: 0.7094\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.6729e-04 - accuracy: 0.9860 - precision: 0.1034 - recall: 0.8947 - auc: 0.9841 - prc: 0.7296 - val_loss: 0.1329 - val_accuracy: 0.9846 - val_precision: 0.0762 - val_recall: 0.9048 - val_auc: 0.9739 - val_prc: 0.7027\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 2.6629e-04 - accuracy: 0.9850 - precision: 0.0969 - recall: 0.8947 - auc: 0.9843 - prc: 0.7171 - val_loss: 0.1297 - val_accuracy: 0.9844 - val_precision: 0.0753 - val_recall: 0.9048 - val_auc: 0.9738 - val_prc: 0.6689\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.18000000000000002\n",
            "****************************** w =  4\n",
            "Model: \"sequential_84\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_84 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_84 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_84 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_84 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85/89 [===========================>..] - ETA: 0s - loss: 8.2871e-04 - accuracy: 0.9216 - precision: 0.0189 - recall: 0.8943 - auc: 0.9375 - prc: 0.6144"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 15ms/step - loss: 8.2620e-04 - accuracy: 0.9236 - precision: 0.0195 - recall: 0.8912 - auc: 0.9358 - prc: 0.6220 - val_loss: 0.5139 - val_accuracy: 0.9803 - val_precision: 0.0583 - val_recall: 0.8730 - val_auc: 0.9571 - val_prc: 0.6765\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 6.2978e-04 - accuracy: 0.9871 - precision: 0.1070 - recall: 0.8545 - auc: 0.9395 - prc: 0.7147 - val_loss: 0.3661 - val_accuracy: 0.9906 - val_precision: 0.1113 - val_recall: 0.8254 - val_auc: 0.9599 - val_prc: 0.7032\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.2609e-04 - accuracy: 0.9924 - precision: 0.1712 - recall: 0.8514 - auc: 0.9377 - prc: 0.7201 - val_loss: 0.2849 - val_accuracy: 0.9924 - val_precision: 0.1351 - val_recall: 0.8254 - val_auc: 0.9617 - val_prc: 0.7062\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.5965e-04 - accuracy: 0.9938 - precision: 0.2025 - recall: 0.8545 - auc: 0.9513 - prc: 0.7306 - val_loss: 0.2254 - val_accuracy: 0.9940 - val_precision: 0.1661 - val_recall: 0.8254 - val_auc: 0.9622 - val_prc: 0.7157\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.1911e-04 - accuracy: 0.9942 - precision: 0.2153 - recall: 0.8545 - auc: 0.9551 - prc: 0.7399 - val_loss: 0.1922 - val_accuracy: 0.9937 - val_precision: 0.1600 - val_recall: 0.8254 - val_auc: 0.9625 - val_prc: 0.7215\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.9023e-04 - accuracy: 0.9941 - precision: 0.2125 - recall: 0.8545 - auc: 0.9622 - prc: 0.7437 - val_loss: 0.1766 - val_accuracy: 0.9920 - val_precision: 0.1309 - val_recall: 0.8413 - val_auc: 0.9635 - val_prc: 0.7314\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.6955e-04 - accuracy: 0.9923 - precision: 0.1703 - recall: 0.8669 - auc: 0.9685 - prc: 0.7513 - val_loss: 0.1555 - val_accuracy: 0.9921 - val_precision: 0.1315 - val_recall: 0.8413 - val_auc: 0.9663 - val_prc: 0.7316\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.5454e-04 - accuracy: 0.9923 - precision: 0.1703 - recall: 0.8607 - auc: 0.9709 - prc: 0.7532 - val_loss: 0.1442 - val_accuracy: 0.9917 - val_precision: 0.1262 - val_recall: 0.8413 - val_auc: 0.9673 - val_prc: 0.7344\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.3996e-04 - accuracy: 0.9917 - precision: 0.1600 - recall: 0.8638 - auc: 0.9749 - prc: 0.7547 - val_loss: 0.1354 - val_accuracy: 0.9912 - val_precision: 0.1202 - val_recall: 0.8413 - val_auc: 0.9703 - val_prc: 0.7348\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 3.2780e-04 - accuracy: 0.9912 - precision: 0.1530 - recall: 0.8700 - auc: 0.9806 - prc: 0.7626 - val_loss: 0.1258 - val_accuracy: 0.9912 - val_precision: 0.1194 - val_recall: 0.8413 - val_auc: 0.9715 - val_prc: 0.7361\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 3.2360e-04 - accuracy: 0.9911 - precision: 0.1508 - recall: 0.8700 - auc: 0.9783 - prc: 0.7601 - val_loss: 0.1233 - val_accuracy: 0.9905 - val_precision: 0.1130 - val_recall: 0.8571 - val_auc: 0.9721 - val_prc: 0.7371\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 3.1389e-04 - accuracy: 0.9907 - precision: 0.1455 - recall: 0.8700 - auc: 0.9807 - prc: 0.7529 - val_loss: 0.1220 - val_accuracy: 0.9892 - val_precision: 0.1024 - val_recall: 0.8730 - val_auc: 0.9728 - val_prc: 0.7391\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.0621e-04 - accuracy: 0.9901 - precision: 0.1383 - recall: 0.8731 - auc: 0.9822 - prc: 0.7582 - val_loss: 0.1183 - val_accuracy: 0.9889 - val_precision: 0.0996 - val_recall: 0.8730 - val_auc: 0.9729 - val_prc: 0.7410\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.0031e-04 - accuracy: 0.9892 - precision: 0.1283 - recall: 0.8793 - auc: 0.9820 - prc: 0.7514 - val_loss: 0.1118 - val_accuracy: 0.9894 - val_precision: 0.1040 - val_recall: 0.8730 - val_auc: 0.9735 - val_prc: 0.7166\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 2.9459e-04 - accuracy: 0.9895 - precision: 0.1313 - recall: 0.8731 - auc: 0.9840 - prc: 0.7524 - val_loss: 0.1104 - val_accuracy: 0.9887 - val_precision: 0.0980 - val_recall: 0.8730 - val_auc: 0.9730 - val_prc: 0.7140\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 2.8909e-04 - accuracy: 0.9889 - precision: 0.1259 - recall: 0.8854 - auc: 0.9839 - prc: 0.7512 - val_loss: 0.1054 - val_accuracy: 0.9890 - val_precision: 0.1007 - val_recall: 0.8730 - val_auc: 0.9729 - val_prc: 0.7147\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.8981e-04 - accuracy: 0.9887 - precision: 0.1235 - recall: 0.8793 - auc: 0.9830 - prc: 0.7458 - val_loss: 0.1043 - val_accuracy: 0.9887 - val_precision: 0.0984 - val_recall: 0.8730 - val_auc: 0.9729 - val_prc: 0.7149\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 2.8534e-04 - accuracy: 0.9889 - precision: 0.1256 - recall: 0.8824 - auc: 0.9847 - prc: 0.7470 - val_loss: 0.1059 - val_accuracy: 0.9877 - val_precision: 0.0920 - val_recall: 0.8889 - val_auc: 0.9730 - val_prc: 0.7158\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 2.7861e-04 - accuracy: 0.9889 - precision: 0.1268 - recall: 0.8885 - auc: 0.9849 - prc: 0.7490 - val_loss: 0.1062 - val_accuracy: 0.9868 - val_precision: 0.0866 - val_recall: 0.8889 - val_auc: 0.9732 - val_prc: 0.7210\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 2.7573e-04 - accuracy: 0.9877 - precision: 0.1164 - recall: 0.8947 - auc: 0.9850 - prc: 0.7328 - val_loss: 0.1030 - val_accuracy: 0.9870 - val_precision: 0.0879 - val_recall: 0.8889 - val_auc: 0.9727 - val_prc: 0.7087\n",
            "1419/1419 [==============================] - 9s 6ms/step\n",
            "****************************** w =  0.19000000000000003\n",
            "****************************** w =  4\n",
            "Model: \"sequential_85\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_85 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_85 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_85 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_85 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89/89 [==============================] - ETA: 0s - loss: 9.4251e-04 - accuracy: 0.9143 - precision: 0.0171 - recall: 0.8756 - auc: 0.9385 - prc: 0.6333"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 5s 29ms/step - loss: 9.4251e-04 - accuracy: 0.9143 - precision: 0.0171 - recall: 0.8756 - auc: 0.9385 - prc: 0.6333 - val_loss: 0.4473 - val_accuracy: 0.9965 - val_precision: 0.2600 - val_recall: 0.8254 - val_auc: 0.9593 - val_prc: 0.6980\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 2s 21ms/step - loss: 6.7941e-04 - accuracy: 0.9976 - precision: 0.4127 - recall: 0.8421 - auc: 0.9338 - prc: 0.7126 - val_loss: 0.3068 - val_accuracy: 0.9980 - val_precision: 0.3881 - val_recall: 0.8254 - val_auc: 0.9617 - val_prc: 0.7074\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 2s 23ms/step - loss: 5.4639e-04 - accuracy: 0.9979 - precision: 0.4565 - recall: 0.8452 - auc: 0.9428 - prc: 0.7252 - val_loss: 0.2239 - val_accuracy: 0.9982 - val_precision: 0.4298 - val_recall: 0.8254 - val_auc: 0.9625 - val_prc: 0.7127\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 2s 21ms/step - loss: 4.7462e-04 - accuracy: 0.9979 - precision: 0.4536 - recall: 0.8483 - auc: 0.9491 - prc: 0.7336 - val_loss: 0.1753 - val_accuracy: 0.9980 - val_precision: 0.3969 - val_recall: 0.8254 - val_auc: 0.9634 - val_prc: 0.7204\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 2s 18ms/step - loss: 4.2678e-04 - accuracy: 0.9975 - precision: 0.4065 - recall: 0.8483 - auc: 0.9575 - prc: 0.7415 - val_loss: 0.1488 - val_accuracy: 0.9970 - val_precision: 0.2971 - val_recall: 0.8254 - val_auc: 0.9646 - val_prc: 0.7302\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 2s 19ms/step - loss: 3.9628e-04 - accuracy: 0.9969 - precision: 0.3472 - recall: 0.8514 - auc: 0.9636 - prc: 0.7502 - val_loss: 0.1346 - val_accuracy: 0.9959 - val_precision: 0.2314 - val_recall: 0.8413 - val_auc: 0.9658 - val_prc: 0.7351\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 2s 17ms/step - loss: 3.7582e-04 - accuracy: 0.9959 - precision: 0.2834 - recall: 0.8607 - auc: 0.9685 - prc: 0.7559 - val_loss: 0.1220 - val_accuracy: 0.9953 - val_precision: 0.2062 - val_recall: 0.8413 - val_auc: 0.9685 - val_prc: 0.7351\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 17ms/step - loss: 3.6239e-04 - accuracy: 0.9952 - precision: 0.2502 - recall: 0.8607 - auc: 0.9719 - prc: 0.7525 - val_loss: 0.1121 - val_accuracy: 0.9948 - val_precision: 0.1900 - val_recall: 0.8413 - val_auc: 0.9703 - val_prc: 0.7366\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 3.4704e-04 - accuracy: 0.9947 - precision: 0.2319 - recall: 0.8638 - auc: 0.9759 - prc: 0.7562 - val_loss: 0.1046 - val_accuracy: 0.9944 - val_precision: 0.1791 - val_recall: 0.8413 - val_auc: 0.9723 - val_prc: 0.7378\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 2s 19ms/step - loss: 3.3731e-04 - accuracy: 0.9945 - precision: 0.2253 - recall: 0.8669 - auc: 0.9768 - prc: 0.7620 - val_loss: 0.1023 - val_accuracy: 0.9933 - val_precision: 0.1519 - val_recall: 0.8413 - val_auc: 0.9728 - val_prc: 0.7391\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 2s 28ms/step - loss: 3.3061e-04 - accuracy: 0.9938 - precision: 0.2057 - recall: 0.8700 - auc: 0.9785 - prc: 0.7591 - val_loss: 0.1000 - val_accuracy: 0.9926 - val_precision: 0.1410 - val_recall: 0.8571 - val_auc: 0.9730 - val_prc: 0.7394\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 3.1848e-04 - accuracy: 0.9930 - precision: 0.1859 - recall: 0.8669 - auc: 0.9825 - prc: 0.7592 - val_loss: 0.0957 - val_accuracy: 0.9924 - val_precision: 0.1388 - val_recall: 0.8571 - val_auc: 0.9736 - val_prc: 0.7410\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 3.1344e-04 - accuracy: 0.9925 - precision: 0.1745 - recall: 0.8669 - auc: 0.9815 - prc: 0.7596 - val_loss: 0.0897 - val_accuracy: 0.9927 - val_precision: 0.1429 - val_recall: 0.8571 - val_auc: 0.9743 - val_prc: 0.7436\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 3.1011e-04 - accuracy: 0.9934 - precision: 0.1950 - recall: 0.8669 - auc: 0.9818 - prc: 0.7625 - val_loss: 0.0948 - val_accuracy: 0.9909 - val_precision: 0.1179 - val_recall: 0.8571 - val_auc: 0.9735 - val_prc: 0.7286\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 3.0297e-04 - accuracy: 0.9919 - precision: 0.1643 - recall: 0.8731 - auc: 0.9832 - prc: 0.7581 - val_loss: 0.0908 - val_accuracy: 0.9911 - val_precision: 0.1203 - val_recall: 0.8571 - val_auc: 0.9737 - val_prc: 0.7174\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 3.0162e-04 - accuracy: 0.9918 - precision: 0.1634 - recall: 0.8731 - auc: 0.9822 - prc: 0.7588 - val_loss: 0.0871 - val_accuracy: 0.9913 - val_precision: 0.1247 - val_recall: 0.8730 - val_auc: 0.9734 - val_prc: 0.7199\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 2.9856e-04 - accuracy: 0.9917 - precision: 0.1615 - recall: 0.8762 - auc: 0.9830 - prc: 0.7533 - val_loss: 0.0843 - val_accuracy: 0.9914 - val_precision: 0.1253 - val_recall: 0.8730 - val_auc: 0.9739 - val_prc: 0.7192\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 2s 19ms/step - loss: 2.9305e-04 - accuracy: 0.9914 - precision: 0.1571 - recall: 0.8793 - auc: 0.9845 - prc: 0.7486 - val_loss: 0.0815 - val_accuracy: 0.9917 - val_precision: 0.1303 - val_recall: 0.8730 - val_auc: 0.9739 - val_prc: 0.7195\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 2s 19ms/step - loss: 2.8891e-04 - accuracy: 0.9915 - precision: 0.1598 - recall: 0.8824 - auc: 0.9848 - prc: 0.7497 - val_loss: 0.0787 - val_accuracy: 0.9920 - val_precision: 0.1335 - val_recall: 0.8730 - val_auc: 0.9738 - val_prc: 0.7225\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 2s 20ms/step - loss: 2.8355e-04 - accuracy: 0.9918 - precision: 0.1631 - recall: 0.8793 - auc: 0.9856 - prc: 0.7474 - val_loss: 0.0793 - val_accuracy: 0.9916 - val_precision: 0.1285 - val_recall: 0.8730 - val_auc: 0.9732 - val_prc: 0.7233\n",
            "1419/1419 [==============================] - 4s 3ms/step\n",
            "****************************** w =  0.01\n",
            "****************************** w =  5\n",
            "Model: \"sequential_86\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_86 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_86 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_86 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_86 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88/89 [============================>.] - ETA: 0s - loss: 1.9215e-04 - accuracy: 0.2076 - precision: 0.0021 - recall: 0.9766 - auc: 0.9304 - prc: 0.4460"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 4s 20ms/step - loss: 1.9080e-04 - accuracy: 0.2064 - precision: 0.0021 - recall: 0.9767 - auc: 0.9303 - prc: 0.4463 - val_loss: 1.1945 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9618 - val_prc: 0.3706\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 2s 18ms/step - loss: 7.7205e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9617 - prc: 0.3859 - val_loss: 1.3878 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9636 - val_prc: 0.2987\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 5.6290e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9635 - prc: 0.3286 - val_loss: 1.5422 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9631 - val_prc: 0.2438\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 4.3993e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9679 - prc: 0.2720 - val_loss: 1.6760 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9628 - val_prc: 0.1925\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 2s 19ms/step - loss: 3.6684e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9652 - prc: 0.2387 - val_loss: 1.7938 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9630 - val_prc: 0.1577\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 3.1135e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9683 - prc: 0.2103 - val_loss: 1.9032 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9620 - val_prc: 0.1284\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 2.7010e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9666 - prc: 0.1774 - val_loss: 2.0028 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9622 - val_prc: 0.1194\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 2.3560e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9667 - prc: 0.1538 - val_loss: 2.0943 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9610 - val_prc: 0.1091\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 2.1049e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9670 - prc: 0.1410 - val_loss: 2.1801 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9609 - val_prc: 0.0960\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 2s 17ms/step - loss: 1.9296e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9657 - prc: 0.1242 - val_loss: 2.2623 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9610 - val_prc: 0.0859\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 1.7484e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9647 - prc: 0.1120 - val_loss: 2.3396 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9606 - val_prc: 0.0816\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 2s 25ms/step - loss: 1.5584e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9652 - prc: 0.1023 - val_loss: 2.4114 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9603 - val_prc: 0.0748\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 2s 18ms/step - loss: 1.4347e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9654 - prc: 0.0964 - val_loss: 2.4795 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9591 - val_prc: 0.0715\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 1.3059e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9661 - prc: 0.0880 - val_loss: 2.5443 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9593 - val_prc: 0.0682\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 2s 22ms/step - loss: 1.2029e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9657 - prc: 0.0831 - val_loss: 2.6055 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9592 - val_prc: 0.0623\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 1.1206e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9648 - prc: 0.0778 - val_loss: 2.6637 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9575 - val_prc: 0.0585\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 1.0738e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9627 - prc: 0.0726 - val_loss: 2.7207 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9574 - val_prc: 0.0546\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 9.8367e-06 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9640 - prc: 0.0681 - val_loss: 2.7751 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9572 - val_prc: 0.0504\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.2908e-06 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9635 - prc: 0.0640 - val_loss: 2.8276 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9561 - val_prc: 0.0461\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 8.5809e-06 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9630 - prc: 0.0594 - val_loss: 2.8768 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9557 - val_prc: 0.0431\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.02\n",
            "****************************** w =  5\n",
            "Model: \"sequential_87\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_87 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_87 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_87 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_87 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85/89 [===========================>..] - ETA: 0s - loss: 1.6599e-04 - accuracy: 0.0043 - precision: 0.0017 - recall: 1.0000 - auc: 0.9045 - prc: 0.0653"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 16ms/step - loss: 1.5942e-04 - accuracy: 0.0042 - precision: 0.0017 - recall: 1.0000 - auc: 0.9096 - prc: 0.0730 - val_loss: 1.1366 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9648 - val_prc: 0.4774\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 7.4086e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9684 - prc: 0.4758 - val_loss: 1.3166 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9649 - val_prc: 0.3901\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 5.4695e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9708 - prc: 0.4141 - val_loss: 1.4631 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9646 - val_prc: 0.3124\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 4.3174e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9717 - prc: 0.3441 - val_loss: 1.5874 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9640 - val_prc: 0.2525\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 3.5397e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9728 - prc: 0.2961 - val_loss: 1.6978 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9633 - val_prc: 0.1996\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 2s 18ms/step - loss: 3.0546e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9722 - prc: 0.2639 - val_loss: 1.8001 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9629 - val_prc: 0.1719\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 2s 17ms/step - loss: 2.6864e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9714 - prc: 0.2279 - val_loss: 1.8935 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9633 - val_prc: 0.1557\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 2.3593e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9713 - prc: 0.2022 - val_loss: 1.9796 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9626 - val_prc: 0.1407\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 2.0844e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9720 - prc: 0.1797 - val_loss: 2.0608 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9626 - val_prc: 0.1282\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 1.9179e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9705 - prc: 0.1618 - val_loss: 2.1373 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9623 - val_prc: 0.1130\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 1.7463e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9703 - prc: 0.1470 - val_loss: 2.2095 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9615 - val_prc: 0.0991\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 1.6045e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9694 - prc: 0.1311 - val_loss: 2.2789 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9605 - val_prc: 0.0923\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 1.4738e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9694 - prc: 0.1216 - val_loss: 2.3444 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9603 - val_prc: 0.0842\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 1.3666e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9686 - prc: 0.1121 - val_loss: 2.4070 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9602 - val_prc: 0.0805\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 2s 19ms/step - loss: 1.2493e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9696 - prc: 0.1059 - val_loss: 2.4658 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9603 - val_prc: 0.0764\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 2s 20ms/step - loss: 1.1504e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9693 - prc: 0.0981 - val_loss: 2.5214 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9594 - val_prc: 0.0716\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 2s 17ms/step - loss: 1.0803e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9691 - prc: 0.0924 - val_loss: 2.5767 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9594 - val_prc: 0.0685\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 17ms/step - loss: 1.0108e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9685 - prc: 0.0867 - val_loss: 2.6282 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9585 - val_prc: 0.0636\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.5477e-06 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9680 - prc: 0.0820 - val_loss: 2.6778 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9584 - val_prc: 0.0604\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 8.9155e-06 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9680 - prc: 0.0762 - val_loss: 2.7259 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9585 - val_prc: 0.0558\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.03\n",
            "****************************** w =  5\n",
            "Model: \"sequential_88\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_88 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_88 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_88 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_88 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "84/89 [===========================>..] - ETA: 0s - loss: 1.9100e-04 - accuracy: 0.0304 - precision: 0.0017 - recall: 0.9919 - auc: 0.8897 - prc: 0.0758"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 17ms/step - loss: 1.8970e-04 - accuracy: 0.0292 - precision: 0.0017 - recall: 0.9922 - auc: 0.8916 - prc: 0.0798 - val_loss: 1.0627 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9583 - val_prc: 0.5597\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 8.1457e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9718 - prc: 0.5716 - val_loss: 1.2448 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9602 - val_prc: 0.4845\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 5.9754e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9713 - prc: 0.4858 - val_loss: 1.3913 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9610 - val_prc: 0.3798\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.6233e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9724 - prc: 0.4100 - val_loss: 1.5152 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9609 - val_prc: 0.3166\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.8610e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9715 - prc: 0.3585 - val_loss: 1.6250 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9613 - val_prc: 0.2671\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.3426e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9701 - prc: 0.3116 - val_loss: 1.7280 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9608 - val_prc: 0.2213\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.8662e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9715 - prc: 0.2689 - val_loss: 1.8217 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9607 - val_prc: 0.1935\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.5094e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9724 - prc: 0.2397 - val_loss: 1.9079 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9593 - val_prc: 0.1678\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.2194e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9724 - prc: 0.2130 - val_loss: 1.9882 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9605 - val_prc: 0.1477\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.0138e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9725 - prc: 0.1880 - val_loss: 2.0663 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9601 - val_prc: 0.1320\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.8461e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9707 - prc: 0.1698 - val_loss: 2.1387 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9594 - val_prc: 0.1177\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.6721e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9703 - prc: 0.1513 - val_loss: 2.2069 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9594 - val_prc: 0.1088\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.5498e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9701 - prc: 0.1403 - val_loss: 2.2729 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9587 - val_prc: 0.1020\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 1.4122e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9711 - prc: 0.1284 - val_loss: 2.3351 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9587 - val_prc: 0.0960\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 1.3029e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9708 - prc: 0.1204 - val_loss: 2.3949 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9577 - val_prc: 0.0888\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.2485e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9693 - prc: 0.1124 - val_loss: 2.4523 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9586 - val_prc: 0.0838\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.1502e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9697 - prc: 0.1063 - val_loss: 2.5067 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9584 - val_prc: 0.0792\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.0743e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9692 - prc: 0.0997 - val_loss: 2.5599 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9581 - val_prc: 0.0737\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.0081e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9692 - prc: 0.0930 - val_loss: 2.6107 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9571 - val_prc: 0.0677\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.4737e-06 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9690 - prc: 0.0870 - val_loss: 2.6588 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9568 - val_prc: 0.0627\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.04\n",
            "****************************** w =  5\n",
            "Model: \"sequential_89\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_89 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_89 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_89 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_89 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89/89 [==============================] - ETA: 0s - loss: 1.7119e-04 - accuracy: 0.0124 - precision: 0.0017 - recall: 1.0000 - auc: 0.9006 - prc: 0.0912"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 6s 20ms/step - loss: 1.7119e-04 - accuracy: 0.0124 - precision: 0.0017 - recall: 1.0000 - auc: 0.9006 - prc: 0.0912 - val_loss: 1.0616 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9641 - val_prc: 0.5416\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 7.9679e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9673 - prc: 0.5576 - val_loss: 1.2229 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9641 - val_prc: 0.4608\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.9537e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9697 - prc: 0.4831 - val_loss: 1.3559 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9641 - val_prc: 0.4012\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 4.7547e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9692 - prc: 0.4043 - val_loss: 1.4707 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9644 - val_prc: 0.2959\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 2s 19ms/step - loss: 3.9988e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9707 - prc: 0.3554 - val_loss: 1.5757 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9634 - val_prc: 0.2649\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 3.4302e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9710 - prc: 0.3139 - val_loss: 1.6718 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9630 - val_prc: 0.2282\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.9974e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9708 - prc: 0.2854 - val_loss: 1.7612 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9629 - val_prc: 0.1963\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.6413e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9717 - prc: 0.2542 - val_loss: 1.8450 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9625 - val_prc: 0.1772\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.3781e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9703 - prc: 0.2209 - val_loss: 1.9241 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9626 - val_prc: 0.1588\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 2.1262e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9708 - prc: 0.2027 - val_loss: 1.9983 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9619 - val_prc: 0.1406\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 2s 17ms/step - loss: 1.9582e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9695 - prc: 0.1823 - val_loss: 2.0688 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9616 - val_prc: 0.1291\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 3s 29ms/step - loss: 1.7590e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9704 - prc: 0.1668 - val_loss: 2.1349 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9611 - val_prc: 0.1235\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 2s 21ms/step - loss: 1.6382e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9700 - prc: 0.1534 - val_loss: 2.1990 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9611 - val_prc: 0.1096\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 2s 20ms/step - loss: 1.5396e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9693 - prc: 0.1403 - val_loss: 2.2614 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9602 - val_prc: 0.1023\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 1.4174e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9694 - prc: 0.1289 - val_loss: 2.3204 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9605 - val_prc: 0.0929\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 1.2915e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9699 - prc: 0.1216 - val_loss: 2.3758 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9600 - val_prc: 0.0893\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.2223e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9696 - prc: 0.1141 - val_loss: 2.4302 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9596 - val_prc: 0.0847\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.1571e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9684 - prc: 0.1070 - val_loss: 2.4821 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9595 - val_prc: 0.0801\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.0854e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9685 - prc: 0.1008 - val_loss: 2.5322 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9593 - val_prc: 0.0753\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.0245e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9680 - prc: 0.0952 - val_loss: 2.5804 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9590 - val_prc: 0.0710\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.05\n",
            "****************************** w =  5\n",
            "Model: \"sequential_90\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_90 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_90 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_90 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_90 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88/89 [============================>.] - ETA: 0s - loss: 1.9777e-04 - accuracy: 0.0339 - precision: 0.0017 - recall: 0.9921 - auc: 0.8802 - prc: 0.0914"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 16ms/step - loss: 1.9630e-04 - accuracy: 0.0338 - precision: 0.0017 - recall: 0.9922 - auc: 0.8815 - prc: 0.0945 - val_loss: 1.0325 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9624 - val_prc: 0.5283\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 8.3438e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9631 - prc: 0.5443 - val_loss: 1.2062 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9654 - val_prc: 0.5109\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.9272e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9693 - prc: 0.4790 - val_loss: 1.3422 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9646 - val_prc: 0.3995\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.7301e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9717 - prc: 0.4262 - val_loss: 1.4614 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9646 - val_prc: 0.3381\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.9721e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9717 - prc: 0.3685 - val_loss: 1.5683 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9646 - val_prc: 0.2752\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 3.3031e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9732 - prc: 0.3180 - val_loss: 1.6633 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9644 - val_prc: 0.2270\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 2.9279e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9716 - prc: 0.2786 - val_loss: 1.7511 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9634 - val_prc: 0.1842\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 2s 20ms/step - loss: 2.5992e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9709 - prc: 0.2391 - val_loss: 1.8325 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9633 - val_prc: 0.1666\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 2s 18ms/step - loss: 2.3590e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9712 - prc: 0.2184 - val_loss: 1.9104 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9628 - val_prc: 0.1516\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 2.0586e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9731 - prc: 0.1957 - val_loss: 1.9822 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9623 - val_prc: 0.1384\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.9018e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9717 - prc: 0.1779 - val_loss: 2.0500 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9621 - val_prc: 0.1276\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.7641e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9719 - prc: 0.1609 - val_loss: 2.1153 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9619 - val_prc: 0.1168\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.6458e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9706 - prc: 0.1474 - val_loss: 2.1784 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9621 - val_prc: 0.1057\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.5372e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9704 - prc: 0.1353 - val_loss: 2.2382 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9615 - val_prc: 0.0969\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.4209e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9701 - prc: 0.1262 - val_loss: 2.2948 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9607 - val_prc: 0.0910\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.3182e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9705 - prc: 0.1164 - val_loss: 2.3486 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9606 - val_prc: 0.0879\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.2467e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9704 - prc: 0.1105 - val_loss: 2.4003 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9606 - val_prc: 0.0830\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 1.1960e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9688 - prc: 0.1034 - val_loss: 2.4509 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9603 - val_prc: 0.0782\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 1.1171e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9694 - prc: 0.0982 - val_loss: 2.4991 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9606 - val_prc: 0.0745\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 1.0642e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9685 - prc: 0.0931 - val_loss: 2.5455 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9589 - val_prc: 0.0713\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.060000000000000005\n",
            "****************************** w =  5\n",
            "Model: \"sequential_91\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_91 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_91 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_91 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_91 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "84/89 [===========================>..] - ETA: 0s - loss: 1.7908e-04 - accuracy: 0.0109 - precision: 0.0017 - recall: 0.9867 - auc: 0.8844 - prc: 0.0848"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 16ms/step - loss: 1.7612e-04 - accuracy: 0.0105 - precision: 0.0017 - recall: 0.9870 - auc: 0.8852 - prc: 0.0862 - val_loss: 1.0985 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9640 - val_prc: 0.4153\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 7.3690e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9622 - prc: 0.4493 - val_loss: 1.2659 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9638 - val_prc: 0.3866\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.3838e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9655 - prc: 0.4070 - val_loss: 1.3962 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9635 - val_prc: 0.3139\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.3127e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9670 - prc: 0.3627 - val_loss: 1.5071 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9630 - val_prc: 0.2650\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 3.6945e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9673 - prc: 0.3104 - val_loss: 1.6085 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9624 - val_prc: 0.2436\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 3.1557e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9696 - prc: 0.2807 - val_loss: 1.7013 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9627 - val_prc: 0.2075\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 2.8366e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9687 - prc: 0.2506 - val_loss: 1.7863 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9624 - val_prc: 0.1754\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.5433e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9692 - prc: 0.2228 - val_loss: 1.8672 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9617 - val_prc: 0.1488\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.3320e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9684 - prc: 0.2032 - val_loss: 1.9430 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9623 - val_prc: 0.1471\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.0836e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9697 - prc: 0.1818 - val_loss: 2.0131 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9612 - val_prc: 0.1305\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.9328e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9695 - prc: 0.1671 - val_loss: 2.0794 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9610 - val_prc: 0.1210\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.8169e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9680 - prc: 0.1523 - val_loss: 2.1431 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9604 - val_prc: 0.1090\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.6552e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9698 - prc: 0.1420 - val_loss: 2.2035 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9603 - val_prc: 0.1011\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.5384e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9697 - prc: 0.1286 - val_loss: 2.2595 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9597 - val_prc: 0.0929\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.4858e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9682 - prc: 0.1213 - val_loss: 2.3144 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9600 - val_prc: 0.0880\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.4024e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9676 - prc: 0.1154 - val_loss: 2.3664 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9594 - val_prc: 0.0855\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.3191e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9681 - prc: 0.1065 - val_loss: 2.4161 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9595 - val_prc: 0.0822\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.2558e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9684 - prc: 0.1024 - val_loss: 2.4639 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9586 - val_prc: 0.0781\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.1992e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9680 - prc: 0.0982 - val_loss: 2.5099 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9586 - val_prc: 0.0749\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.1463e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9677 - prc: 0.0935 - val_loss: 2.5536 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9589 - val_prc: 0.0716\n",
            "1419/1419 [==============================] - 4s 3ms/step\n",
            "****************************** w =  0.07\n",
            "****************************** w =  5\n",
            "Model: \"sequential_92\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_92 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_92 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_92 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_92 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88/89 [============================>.] - ETA: 0s - loss: 1.6460e-04 - accuracy: 0.0072 - precision: 0.0017 - recall: 0.9922 - auc: 0.8949 - prc: 0.0843"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 7s 29ms/step - loss: 1.6481e-04 - accuracy: 0.0071 - precision: 0.0017 - recall: 0.9922 - auc: 0.8945 - prc: 0.0837 - val_loss: 1.0764 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9596 - val_prc: 0.4910\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 7.2402e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9629 - prc: 0.5069 - val_loss: 1.2288 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9616 - val_prc: 0.4342\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 17ms/step - loss: 5.4779e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9674 - prc: 0.4618 - val_loss: 1.3515 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9616 - val_prc: 0.4017\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 2s 17ms/step - loss: 4.4455e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9694 - prc: 0.4172 - val_loss: 1.4563 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9620 - val_prc: 0.3462\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 2s 21ms/step - loss: 3.8574e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9696 - prc: 0.3744 - val_loss: 1.5517 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9621 - val_prc: 0.2929\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 3.4062e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9700 - prc: 0.3333 - val_loss: 1.6398 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9621 - val_prc: 0.2608\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.0271e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9695 - prc: 0.3098 - val_loss: 1.7205 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9616 - val_prc: 0.2234\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 2.7395e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9699 - prc: 0.2765 - val_loss: 1.7955 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9615 - val_prc: 0.1912\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 2.4986e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9713 - prc: 0.2548 - val_loss: 1.8669 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9616 - val_prc: 0.1742\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 2.3291e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9707 - prc: 0.2293 - val_loss: 1.9330 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9616 - val_prc: 0.1612\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 2.1540e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9709 - prc: 0.2081 - val_loss: 1.9953 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9611 - val_prc: 0.1473\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 2.0233e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9704 - prc: 0.1933 - val_loss: 2.0537 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9610 - val_prc: 0.1355\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.9303e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9691 - prc: 0.1769 - val_loss: 2.1096 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9604 - val_prc: 0.1232\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.8134e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9699 - prc: 0.1638 - val_loss: 2.1628 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9603 - val_prc: 0.1139\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.7327e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9699 - prc: 0.1527 - val_loss: 2.2135 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9605 - val_prc: 0.1097\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.6632e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9693 - prc: 0.1400 - val_loss: 2.2620 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9599 - val_prc: 0.1025\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.5815e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9691 - prc: 0.1303 - val_loss: 2.3079 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9597 - val_prc: 0.0967\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.5284e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9691 - prc: 0.1282 - val_loss: 2.3518 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9587 - val_prc: 0.0957\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.4835e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9689 - prc: 0.1214 - val_loss: 2.3951 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9589 - val_prc: 0.0917\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.4048e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9700 - prc: 0.1140 - val_loss: 2.4350 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9594 - val_prc: 0.0882\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.08\n",
            "****************************** w =  5\n",
            "Model: \"sequential_93\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_93 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_93 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_93 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_93 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88/89 [============================>.] - ETA: 0s - loss: 1.8610e-04 - accuracy: 0.0153 - precision: 0.0017 - recall: 0.9765 - auc: 0.8809 - prc: 0.0892"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 16ms/step - loss: 1.8480e-04 - accuracy: 0.0152 - precision: 0.0017 - recall: 0.9767 - auc: 0.8819 - prc: 0.0910 - val_loss: 1.0412 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9594 - val_prc: 0.4565\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 7.8669e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9653 - prc: 0.5066 - val_loss: 1.1940 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9611 - val_prc: 0.4244\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.8208e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9675 - prc: 0.4710 - val_loss: 1.3103 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9605 - val_prc: 0.3971\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 4.8477e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9679 - prc: 0.4183 - val_loss: 1.4105 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9619 - val_prc: 0.3339\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 4.2028e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9700 - prc: 0.3741 - val_loss: 1.5006 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9623 - val_prc: 0.2869\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 3.7247e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9716 - prc: 0.3391 - val_loss: 1.5827 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9611 - val_prc: 0.2611\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 3.3927e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9709 - prc: 0.3115 - val_loss: 1.6580 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9611 - val_prc: 0.2436\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 3.1050e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9713 - prc: 0.2923 - val_loss: 1.7277 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9613 - val_prc: 0.2237\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.8897e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9716 - prc: 0.2614 - val_loss: 1.7935 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9616 - val_prc: 0.1917\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.7357e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9712 - prc: 0.2408 - val_loss: 1.8565 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9613 - val_prc: 0.1682\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 2.5620e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9717 - prc: 0.2185 - val_loss: 1.9145 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9610 - val_prc: 0.1614\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.4132e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9716 - prc: 0.2048 - val_loss: 1.9693 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9611 - val_prc: 0.1471\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 2.3138e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9709 - prc: 0.1885 - val_loss: 2.0218 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9607 - val_prc: 0.1322\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 2.2379e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9699 - prc: 0.1772 - val_loss: 2.0705 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9609 - val_prc: 0.1234\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 2.1289e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9720 - prc: 0.1679 - val_loss: 2.1162 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9605 - val_prc: 0.1181\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 2.0530e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9713 - prc: 0.1542 - val_loss: 2.1591 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9601 - val_prc: 0.1114\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 2.0129e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9709 - prc: 0.1461 - val_loss: 2.2008 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9598 - val_prc: 0.1059\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.9642e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9709 - prc: 0.1387 - val_loss: 2.2418 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9599 - val_prc: 0.1025\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.8963e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9713 - prc: 0.1300 - val_loss: 2.2793 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9602 - val_prc: 0.0982\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.8626e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9703 - prc: 0.1247 - val_loss: 2.3153 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9605 - val_prc: 0.0926\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.09\n",
            "****************************** w =  5\n",
            "Model: \"sequential_94\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_94 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_94 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_94 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_94 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88/89 [============================>.] - ETA: 0s - loss: 1.5944e-04 - accuracy: 0.0094 - precision: 0.0017 - recall: 0.9974 - auc: 0.8935 - prc: 0.1066"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 15ms/step - loss: 1.5981e-04 - accuracy: 0.0094 - precision: 0.0017 - recall: 0.9974 - auc: 0.8940 - prc: 0.1065 - val_loss: 1.0283 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9570 - val_prc: 0.5229\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 7.7484e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9644 - prc: 0.5160 - val_loss: 1.1686 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9587 - val_prc: 0.4643\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 6.0660e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9689 - prc: 0.4776 - val_loss: 1.2810 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9598 - val_prc: 0.4208\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 5.1021e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9704 - prc: 0.4296 - val_loss: 1.3775 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9604 - val_prc: 0.3583\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 4.5236e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9698 - prc: 0.3850 - val_loss: 1.4609 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9607 - val_prc: 0.2977\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.0739e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9718 - prc: 0.3478 - val_loss: 1.5356 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9607 - val_prc: 0.2701\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 3.7809e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9716 - prc: 0.3281 - val_loss: 1.6056 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9610 - val_prc: 0.2402\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.5390e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9708 - prc: 0.2995 - val_loss: 1.6681 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9605 - val_prc: 0.2163\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.3666e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9710 - prc: 0.2836 - val_loss: 1.7268 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9606 - val_prc: 0.2011\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.2188e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9709 - prc: 0.2618 - val_loss: 1.7814 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9606 - val_prc: 0.1843\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.0809e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9709 - prc: 0.2441 - val_loss: 1.8321 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9609 - val_prc: 0.1708\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 3.0075e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9696 - prc: 0.2214 - val_loss: 1.8802 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9601 - val_prc: 0.1604\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 2.8831e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9707 - prc: 0.2095 - val_loss: 1.9255 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9609 - val_prc: 0.1528\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 2.7654e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9728 - prc: 0.2001 - val_loss: 1.9672 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9601 - val_prc: 0.1422\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 2.7310e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9716 - prc: 0.1911 - val_loss: 2.0064 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9598 - val_prc: 0.1358\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 2.6627e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9714 - prc: 0.1794 - val_loss: 2.0430 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9597 - val_prc: 0.1285\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 2.6361e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9708 - prc: 0.1717 - val_loss: 2.0791 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9596 - val_prc: 0.1197\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 2.5817e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9712 - prc: 0.1642 - val_loss: 2.1124 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9597 - val_prc: 0.1134\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.5068e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9725 - prc: 0.1542 - val_loss: 2.1425 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9596 - val_prc: 0.1082\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.5205e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9711 - prc: 0.1501 - val_loss: 2.1726 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9602 - val_prc: 0.1050\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.09999999999999999\n",
            "****************************** w =  5\n",
            "Model: \"sequential_95\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_95 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_95 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_95 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_95 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86/89 [===========================>..] - ETA: 0s - loss: 1.6817e-04 - accuracy: 0.0172 - precision: 0.0017 - recall: 0.9813 - auc: 0.8894 - prc: 0.1127"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 5s 34ms/step - loss: 1.6688e-04 - accuracy: 0.0168 - precision: 0.0017 - recall: 0.9819 - auc: 0.8911 - prc: 0.1161 - val_loss: 1.0159 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9627 - val_prc: 0.4748\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 2s 22ms/step - loss: 7.8857e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9664 - prc: 0.5171 - val_loss: 1.1494 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9642 - val_prc: 0.4354\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 2s 26ms/step - loss: 6.2977e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9716 - prc: 0.4680 - val_loss: 1.2518 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9643 - val_prc: 0.3994\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 2s 22ms/step - loss: 5.5203e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9701 - prc: 0.4226 - val_loss: 1.3375 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9645 - val_prc: 0.3499\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 2s 20ms/step - loss: 5.0125e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9703 - prc: 0.3827 - val_loss: 1.4107 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9644 - val_prc: 0.3038\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 2s 20ms/step - loss: 4.6590e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9713 - prc: 0.3570 - val_loss: 1.4758 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9643 - val_prc: 0.2778\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 2s 26ms/step - loss: 4.3437e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9730 - prc: 0.3168 - val_loss: 1.5339 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9647 - val_prc: 0.2572\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 2s 18ms/step - loss: 4.2067e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9718 - prc: 0.2976 - val_loss: 1.5880 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9647 - val_prc: 0.2315\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 2s 20ms/step - loss: 4.0103e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9736 - prc: 0.2782 - val_loss: 1.6371 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9647 - val_prc: 0.2122\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 2s 17ms/step - loss: 3.8764e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9734 - prc: 0.2614 - val_loss: 1.6806 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9633 - val_prc: 0.1909\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 3.8000e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9725 - prc: 0.2450 - val_loss: 1.7214 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9643 - val_prc: 0.1763\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.7010e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9732 - prc: 0.2337 - val_loss: 1.7588 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9636 - val_prc: 0.1686\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.6199e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9738 - prc: 0.2215 - val_loss: 1.7928 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9637 - val_prc: 0.1567\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.6080e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9722 - prc: 0.2106 - val_loss: 1.8257 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9644 - val_prc: 0.1511\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 3.5278e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9724 - prc: 0.1984 - val_loss: 1.8552 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9638 - val_prc: 0.1493\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 3.5090e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9722 - prc: 0.1936 - val_loss: 1.8830 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9638 - val_prc: 0.1426\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 3.4487e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9728 - prc: 0.1844 - val_loss: 1.9082 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9636 - val_prc: 0.1335\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 3.4326e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9729 - prc: 0.1794 - val_loss: 1.9326 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9636 - val_prc: 0.1278\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 3.4192e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9725 - prc: 0.1695 - val_loss: 1.9561 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9641 - val_prc: 0.1245\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 3.3411e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9739 - prc: 0.1627 - val_loss: 1.9759 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9636 - val_prc: 0.1216\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.10999999999999999\n",
            "****************************** w =  5\n",
            "Model: \"sequential_96\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_96 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_96 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_96 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_96 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89/89 [==============================] - ETA: 0s - loss: 1.6331e-04 - accuracy: 0.0241 - precision: 0.0017 - recall: 0.9948 - auc: 0.9013 - prc: 0.1362"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 16ms/step - loss: 1.6331e-04 - accuracy: 0.0241 - precision: 0.0017 - recall: 0.9948 - auc: 0.9013 - prc: 0.1362 - val_loss: 0.9864 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9587 - val_prc: 0.5233\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 8.6653e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9708 - prc: 0.5326 - val_loss: 1.1112 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9610 - val_prc: 0.4789\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 7.1813e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9737 - prc: 0.5163 - val_loss: 1.2052 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9610 - val_prc: 0.4416\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 6.3921e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9732 - prc: 0.4796 - val_loss: 1.2814 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9613 - val_prc: 0.4017\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.9160e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9742 - prc: 0.4251 - val_loss: 1.3470 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9621 - val_prc: 0.3468\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 5.5986e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9743 - prc: 0.4013 - val_loss: 1.4036 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9621 - val_prc: 0.3236\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 5.3484e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9746 - prc: 0.3654 - val_loss: 1.4526 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9620 - val_prc: 0.2779\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.2067e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9747 - prc: 0.3402 - val_loss: 1.4969 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9622 - val_prc: 0.2723\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 5.0350e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9748 - prc: 0.3266 - val_loss: 1.5349 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9630 - val_prc: 0.2582\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.9849e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9746 - prc: 0.3107 - val_loss: 1.5711 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9625 - val_prc: 0.2492\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.8876e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9750 - prc: 0.3016 - val_loss: 1.6029 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9624 - val_prc: 0.2375\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.7938e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9753 - prc: 0.2907 - val_loss: 1.6316 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9621 - val_prc: 0.2214\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.7471e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9757 - prc: 0.2773 - val_loss: 1.6574 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9622 - val_prc: 0.2147\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 4.6783e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9755 - prc: 0.2666 - val_loss: 1.6802 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9624 - val_prc: 0.2021\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.6260e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9767 - prc: 0.2542 - val_loss: 1.7019 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9623 - val_prc: 0.1939\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.6284e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9762 - prc: 0.2537 - val_loss: 1.7230 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9627 - val_prc: 0.1831\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 4.5371e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9771 - prc: 0.2434 - val_loss: 1.7390 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9626 - val_prc: 0.1786\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 4.5569e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9767 - prc: 0.2384 - val_loss: 1.7555 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9629 - val_prc: 0.1715\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 4.5164e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9766 - prc: 0.2286 - val_loss: 1.7688 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9624 - val_prc: 0.1697\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 4.4934e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9772 - prc: 0.2230 - val_loss: 1.7809 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9632 - val_prc: 0.1683\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.11999999999999998\n",
            "****************************** w =  5\n",
            "Model: \"sequential_97\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_97 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_97 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_97 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_97 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88/89 [============================>.] - ETA: 0s - loss: 1.5552e-04 - accuracy: 0.0105 - precision: 0.0017 - recall: 1.0000 - auc: 0.9110 - prc: 0.1490"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 15ms/step - loss: 1.5526e-04 - accuracy: 0.0104 - precision: 0.0017 - recall: 1.0000 - auc: 0.9116 - prc: 0.1527 - val_loss: 1.0201 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9556 - val_prc: 0.4301\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.0636e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9673 - prc: 0.4584 - val_loss: 1.1331 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9575 - val_prc: 0.4129\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 7.8230e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9699 - prc: 0.4417 - val_loss: 1.2128 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9584 - val_prc: 0.3879\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 7.2154e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9724 - prc: 0.4126 - val_loss: 1.2741 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9592 - val_prc: 0.3496\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 6.8675e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9733 - prc: 0.3957 - val_loss: 1.3247 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9598 - val_prc: 0.3342\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 6.6412e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9731 - prc: 0.3672 - val_loss: 1.3664 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9603 - val_prc: 0.2996\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 6.5058e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9725 - prc: 0.3507 - val_loss: 1.4014 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9605 - val_prc: 0.2940\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 6.4150e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9734 - prc: 0.3402 - val_loss: 1.4320 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9604 - val_prc: 0.2806\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 6.2903e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9745 - prc: 0.3264 - val_loss: 1.4577 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9606 - val_prc: 0.2688\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 6.2419e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9743 - prc: 0.3171 - val_loss: 1.4804 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9619 - val_prc: 0.2640\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 6.1166e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9756 - prc: 0.3188 - val_loss: 1.4992 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9619 - val_prc: 0.2555\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 6.0957e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9751 - prc: 0.3084 - val_loss: 1.5141 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9617 - val_prc: 0.2488\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 6.1194e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9745 - prc: 0.3018 - val_loss: 1.5285 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9615 - val_prc: 0.2426\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 6.0556e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9752 - prc: 0.2949 - val_loss: 1.5403 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9622 - val_prc: 0.2318\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 6.0040e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9755 - prc: 0.2906 - val_loss: 1.5490 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9626 - val_prc: 0.2374\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 5.9533e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9768 - prc: 0.2886 - val_loss: 1.5560 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9626 - val_prc: 0.2341\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 2s 25ms/step - loss: 5.9639e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9763 - prc: 0.2927 - val_loss: 1.5636 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9632 - val_prc: 0.2332\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 2s 20ms/step - loss: 5.9382e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9763 - prc: 0.2879 - val_loss: 1.5690 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9633 - val_prc: 0.2386\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 17ms/step - loss: 5.9554e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9758 - prc: 0.2793 - val_loss: 1.5742 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9635 - val_prc: 0.2329\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 2s 23ms/step - loss: 5.9009e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9762 - prc: 0.2853 - val_loss: 1.5775 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9640 - val_prc: 0.2314\n",
            "1419/1419 [==============================] - 8s 5ms/step\n",
            "****************************** w =  0.12999999999999998\n",
            "****************************** w =  5\n",
            "Model: \"sequential_98\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_98 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_98 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_98 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_98 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88/89 [============================>.] - ETA: 0s - loss: 1.8588e-04 - accuracy: 0.0383 - precision: 0.0017 - recall: 0.9765 - auc: 0.8948 - prc: 0.2046"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 8s 56ms/step - loss: 1.8736e-04 - accuracy: 0.0380 - precision: 0.0017 - recall: 0.9767 - auc: 0.8938 - prc: 0.2033 - val_loss: 0.9654 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9571 - val_prc: 0.5085\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 2s 18ms/step - loss: 1.0565e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9674 - prc: 0.5409 - val_loss: 1.0710 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9596 - val_prc: 0.5035\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 2s 17ms/step - loss: 9.3435e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9690 - prc: 0.5236 - val_loss: 1.1389 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9607 - val_prc: 0.4605\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 8.8607e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9704 - prc: 0.4966 - val_loss: 1.1917 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9610 - val_prc: 0.4717\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 2s 17ms/step - loss: 8.5690e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9705 - prc: 0.4698 - val_loss: 1.2316 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9624 - val_prc: 0.4152\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 8.2875e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9726 - prc: 0.4640 - val_loss: 1.2608 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9629 - val_prc: 0.3940\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 2s 17ms/step - loss: 8.2364e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9717 - prc: 0.4443 - val_loss: 1.2851 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9626 - val_prc: 0.3881\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 2s 19ms/step - loss: 8.0752e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9738 - prc: 0.4269 - val_loss: 1.3034 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9624 - val_prc: 0.3719\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 2s 20ms/step - loss: 7.9693e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9742 - prc: 0.4171 - val_loss: 1.3180 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9637 - val_prc: 0.3677\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 7.9785e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9738 - prc: 0.3947 - val_loss: 1.3303 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9636 - val_prc: 0.3541\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 2s 19ms/step - loss: 7.9183e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9744 - prc: 0.3907 - val_loss: 1.3404 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9642 - val_prc: 0.3377\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 7.8439e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9755 - prc: 0.3855 - val_loss: 1.3480 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9641 - val_prc: 0.3293\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 7.7611e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9768 - prc: 0.3748 - val_loss: 1.3537 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9645 - val_prc: 0.3219\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 7.7667e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9755 - prc: 0.3803 - val_loss: 1.3568 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9650 - val_prc: 0.3211\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 7.7108e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9767 - prc: 0.3801 - val_loss: 1.3585 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9651 - val_prc: 0.3261\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 7.7104e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9761 - prc: 0.3743 - val_loss: 1.3604 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9650 - val_prc: 0.3279\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 7.6518e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9768 - prc: 0.3707 - val_loss: 1.3607 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9654 - val_prc: 0.3226\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 7.6540e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9772 - prc: 0.3749 - val_loss: 1.3620 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9655 - val_prc: 0.3237\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 7.5919e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9782 - prc: 0.3729 - val_loss: 1.3609 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9655 - val_prc: 0.3249\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 2s 19ms/step - loss: 7.5292e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9791 - prc: 0.3736 - val_loss: 1.3580 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9660 - val_prc: 0.3253\n",
            "1419/1419 [==============================] - 4s 3ms/step\n",
            "****************************** w =  0.13999999999999999\n",
            "****************************** w =  5\n",
            "Model: \"sequential_99\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_99 (Conv1D)          (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_99 (Dropout)        (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_99 (Flatten)        (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_99 (Dense)            (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89/89 [==============================] - ETA: 0s - loss: 1.8669e-04 - accuracy: 0.0553 - precision: 0.0018 - recall: 0.9767 - auc: 0.9078 - prc: 0.3053"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 5s 37ms/step - loss: 1.8669e-04 - accuracy: 0.0553 - precision: 0.0018 - recall: 0.9767 - auc: 0.9078 - prc: 0.3053 - val_loss: 0.9035 - val_accuracy: 0.0015 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9642 - val_prc: 0.6135\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 2s 20ms/step - loss: 1.1923e-04 - accuracy: 0.0019 - precision: 0.0018 - recall: 1.0000 - auc: 0.9703 - prc: 0.6651 - val_loss: 0.9892 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9675 - val_prc: 0.6046\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 1.0902e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9744 - prc: 0.6459 - val_loss: 1.0430 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9687 - val_prc: 0.6034\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 1.0472e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9769 - prc: 0.6314 - val_loss: 1.0789 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9696 - val_prc: 0.5795\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 2s 17ms/step - loss: 1.0280e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9761 - prc: 0.6095 - val_loss: 1.1037 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9697 - val_prc: 0.5673\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.0026e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9782 - prc: 0.6053 - val_loss: 1.1211 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9701 - val_prc: 0.5652\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.9468e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9782 - prc: 0.5855 - val_loss: 1.1329 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9697 - val_prc: 0.5657\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 9.8336e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9782 - prc: 0.5779 - val_loss: 1.1394 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9706 - val_prc: 0.5703\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 9.7512e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9798 - prc: 0.5716 - val_loss: 1.1438 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9711 - val_prc: 0.5725\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 2s 20ms/step - loss: 9.6664e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9797 - prc: 0.5637 - val_loss: 1.1457 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9703 - val_prc: 0.5471\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 9.5861e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9807 - prc: 0.5613 - val_loss: 1.1462 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9713 - val_prc: 0.5300\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 9.5844e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9798 - prc: 0.5557 - val_loss: 1.1479 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9710 - val_prc: 0.5080\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 9.4972e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9802 - prc: 0.5292 - val_loss: 1.1451 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9711 - val_prc: 0.4813\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 9.4460e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9807 - prc: 0.5203 - val_loss: 1.1414 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9712 - val_prc: 0.4803\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 9.4325e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9788 - prc: 0.5284 - val_loss: 1.1373 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9712 - val_prc: 0.4581\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 2s 19ms/step - loss: 9.3609e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9806 - prc: 0.5110 - val_loss: 1.1329 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9711 - val_prc: 0.4538\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 17ms/step - loss: 9.2359e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9821 - prc: 0.5173 - val_loss: 1.1263 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9709 - val_prc: 0.4488\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 9.1956e-05 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9820 - prc: 0.5191 - val_loss: 1.1201 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9716 - val_prc: 0.4515\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 2s 18ms/step - loss: 9.1514e-05 - accuracy: 0.0019 - precision: 0.0018 - recall: 1.0000 - auc: 0.9828 - prc: 0.5219 - val_loss: 1.1151 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9717 - val_prc: 0.4573\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 2s 24ms/step - loss: 9.1313e-05 - accuracy: 0.0019 - precision: 0.0018 - recall: 1.0000 - auc: 0.9825 - prc: 0.5085 - val_loss: 1.1092 - val_accuracy: 0.0015 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9714 - val_prc: 0.4518\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.15\n",
            "****************************** w =  5\n",
            "Model: \"sequential_100\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_100 (Conv1D)         (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_100 (Dropout)       (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_100 (Flatten)       (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_100 (Dense)           (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88/89 [============================>.] - ETA: 0s - loss: 1.9926e-04 - accuracy: 0.0299 - precision: 0.0017 - recall: 0.9843 - auc: 0.9291 - prc: 0.3691"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 16ms/step - loss: 1.9963e-04 - accuracy: 0.0297 - precision: 0.0017 - recall: 0.9845 - auc: 0.9291 - prc: 0.3691 - val_loss: 0.9073 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9680 - val_prc: 0.5756\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.3867e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9712 - prc: 0.6133 - val_loss: 0.9677 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9693 - val_prc: 0.5783\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.2965e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9739 - prc: 0.6231 - val_loss: 0.9968 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9703 - val_prc: 0.5643\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.2656e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9762 - prc: 0.6126 - val_loss: 1.0138 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9705 - val_prc: 0.5828\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.2371e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9765 - prc: 0.6213 - val_loss: 1.0215 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9711 - val_prc: 0.5887\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.2218e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9785 - prc: 0.6065 - val_loss: 1.0260 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9709 - val_prc: 0.5892\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.2066e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9788 - prc: 0.6077 - val_loss: 1.0234 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9720 - val_prc: 0.5950\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.1948e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9792 - prc: 0.6111 - val_loss: 1.0189 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9724 - val_prc: 0.5895\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.1813e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9797 - prc: 0.6103 - val_loss: 1.0120 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9724 - val_prc: 0.5883\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.1718e-04 - accuracy: 0.0018 - precision: 0.0018 - recall: 1.0000 - auc: 0.9800 - prc: 0.6129 - val_loss: 1.0046 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9724 - val_prc: 0.5891\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.1577e-04 - accuracy: 0.0021 - precision: 0.0018 - recall: 1.0000 - auc: 0.9812 - prc: 0.6097 - val_loss: 0.9983 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9727 - val_prc: 0.5853\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.1536e-04 - accuracy: 0.0023 - precision: 0.0018 - recall: 1.0000 - auc: 0.9812 - prc: 0.6083 - val_loss: 0.9901 - val_accuracy: 0.0015 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9726 - val_prc: 0.5873\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.1482e-04 - accuracy: 0.0031 - precision: 0.0018 - recall: 1.0000 - auc: 0.9801 - prc: 0.6098 - val_loss: 0.9821 - val_accuracy: 0.0020 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9727 - val_prc: 0.5796\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.1361e-04 - accuracy: 0.0053 - precision: 0.0018 - recall: 1.0000 - auc: 0.9805 - prc: 0.6041 - val_loss: 0.9741 - val_accuracy: 0.0031 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9731 - val_prc: 0.5810\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.1270e-04 - accuracy: 0.0074 - precision: 0.0018 - recall: 1.0000 - auc: 0.9823 - prc: 0.6002 - val_loss: 0.9643 - val_accuracy: 0.0056 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9732 - val_prc: 0.5333\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.1169e-04 - accuracy: 0.0127 - precision: 0.0018 - recall: 1.0000 - auc: 0.9826 - prc: 0.6074 - val_loss: 0.9545 - val_accuracy: 0.0111 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9732 - val_prc: 0.5344\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.1059e-04 - accuracy: 0.0235 - precision: 0.0018 - recall: 1.0000 - auc: 0.9833 - prc: 0.6098 - val_loss: 0.9462 - val_accuracy: 0.0190 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9731 - val_prc: 0.5360\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.0988e-04 - accuracy: 0.0354 - precision: 0.0018 - recall: 1.0000 - auc: 0.9830 - prc: 0.6047 - val_loss: 0.9381 - val_accuracy: 0.0306 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9732 - val_prc: 0.5331\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.0906e-04 - accuracy: 0.0502 - precision: 0.0019 - recall: 1.0000 - auc: 0.9831 - prc: 0.5933 - val_loss: 0.9285 - val_accuracy: 0.0472 - val_precision: 0.0015 - val_recall: 1.0000 - val_auc: 0.9735 - val_prc: 0.5338\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.0834e-04 - accuracy: 0.0627 - precision: 0.0019 - recall: 1.0000 - auc: 0.9840 - prc: 0.5882 - val_loss: 0.9188 - val_accuracy: 0.0659 - val_precision: 0.0015 - val_recall: 1.0000 - val_auc: 0.9736 - val_prc: 0.5417\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.16\n",
            "****************************** w =  5\n",
            "Model: \"sequential_101\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_101 (Conv1D)         (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_101 (Dropout)       (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_101 (Flatten)       (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_101 (Dense)           (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89/89 [==============================] - ETA: 0s - loss: 2.1834e-04 - accuracy: 0.0378 - precision: 0.0017 - recall: 0.9819 - auc: 0.9300 - prc: 0.4221"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 17ms/step - loss: 2.1834e-04 - accuracy: 0.0378 - precision: 0.0017 - recall: 0.9819 - auc: 0.9300 - prc: 0.4221 - val_loss: 0.8953 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9622 - val_prc: 0.5407\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.6246e-04 - accuracy: 0.0019 - precision: 0.0018 - recall: 1.0000 - auc: 0.9704 - prc: 0.5927 - val_loss: 0.9314 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9659 - val_prc: 0.5646\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.5460e-04 - accuracy: 0.0019 - precision: 0.0018 - recall: 1.0000 - auc: 0.9735 - prc: 0.6140 - val_loss: 0.9386 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9683 - val_prc: 0.5724\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.5130e-04 - accuracy: 0.0019 - precision: 0.0018 - recall: 1.0000 - auc: 0.9740 - prc: 0.6311 - val_loss: 0.9364 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9697 - val_prc: 0.5803\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.4854e-04 - accuracy: 0.0020 - precision: 0.0018 - recall: 1.0000 - auc: 0.9767 - prc: 0.6299 - val_loss: 0.9282 - val_accuracy: 0.0014 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9700 - val_prc: 0.5995\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.4556e-04 - accuracy: 0.0026 - precision: 0.0018 - recall: 1.0000 - auc: 0.9767 - prc: 0.6424 - val_loss: 0.9161 - val_accuracy: 0.0015 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9715 - val_prc: 0.6129\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.4402e-04 - accuracy: 0.0048 - precision: 0.0018 - recall: 1.0000 - auc: 0.9779 - prc: 0.6507 - val_loss: 0.9039 - val_accuracy: 0.0027 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9717 - val_prc: 0.6188\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.4176e-04 - accuracy: 0.0110 - precision: 0.0018 - recall: 1.0000 - auc: 0.9798 - prc: 0.6597 - val_loss: 0.8909 - val_accuracy: 0.0070 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9719 - val_prc: 0.6284\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.4099e-04 - accuracy: 0.0224 - precision: 0.0018 - recall: 1.0000 - auc: 0.9791 - prc: 0.6617 - val_loss: 0.8791 - val_accuracy: 0.0153 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9726 - val_prc: 0.6318\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 1.3836e-04 - accuracy: 0.0478 - precision: 0.0019 - recall: 1.0000 - auc: 0.9806 - prc: 0.6688 - val_loss: 0.8672 - val_accuracy: 0.0335 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9722 - val_prc: 0.6350\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 1.3783e-04 - accuracy: 0.0714 - precision: 0.0019 - recall: 1.0000 - auc: 0.9797 - prc: 0.6776 - val_loss: 0.8551 - val_accuracy: 0.0667 - val_precision: 0.0015 - val_recall: 1.0000 - val_auc: 0.9722 - val_prc: 0.6350\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.3566e-04 - accuracy: 0.1145 - precision: 0.0020 - recall: 1.0000 - auc: 0.9811 - prc: 0.6713 - val_loss: 0.8413 - val_accuracy: 0.1198 - val_precision: 0.0015 - val_recall: 0.9841 - val_auc: 0.9730 - val_prc: 0.6384\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.3417e-04 - accuracy: 0.1609 - precision: 0.0021 - recall: 1.0000 - auc: 0.9824 - prc: 0.6723 - val_loss: 0.8275 - val_accuracy: 0.1821 - val_precision: 0.0017 - val_recall: 0.9841 - val_auc: 0.9731 - val_prc: 0.6388\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.3287e-04 - accuracy: 0.2288 - precision: 0.0023 - recall: 1.0000 - auc: 0.9825 - prc: 0.6742 - val_loss: 0.8150 - val_accuracy: 0.2450 - val_precision: 0.0018 - val_recall: 0.9841 - val_auc: 0.9731 - val_prc: 0.6376\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.3133e-04 - accuracy: 0.2953 - precision: 0.0025 - recall: 0.9969 - auc: 0.9822 - prc: 0.6910 - val_loss: 0.8026 - val_accuracy: 0.3108 - val_precision: 0.0020 - val_recall: 0.9841 - val_auc: 0.9731 - val_prc: 0.6446\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.2960e-04 - accuracy: 0.3371 - precision: 0.0027 - recall: 1.0000 - auc: 0.9844 - prc: 0.6864 - val_loss: 0.7897 - val_accuracy: 0.3729 - val_precision: 0.0022 - val_recall: 0.9841 - val_auc: 0.9735 - val_prc: 0.6467\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.2940e-04 - accuracy: 0.3957 - precision: 0.0029 - recall: 0.9938 - auc: 0.9831 - prc: 0.6802 - val_loss: 0.7788 - val_accuracy: 0.4222 - val_precision: 0.0024 - val_recall: 0.9841 - val_auc: 0.9735 - val_prc: 0.6360\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.2783e-04 - accuracy: 0.4474 - precision: 0.0032 - recall: 0.9969 - auc: 0.9831 - prc: 0.6846 - val_loss: 0.7682 - val_accuracy: 0.4668 - val_precision: 0.0026 - val_recall: 0.9841 - val_auc: 0.9736 - val_prc: 0.6372\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.2682e-04 - accuracy: 0.4828 - precision: 0.0034 - recall: 0.9938 - auc: 0.9830 - prc: 0.6734 - val_loss: 0.7569 - val_accuracy: 0.5129 - val_precision: 0.0028 - val_recall: 0.9841 - val_auc: 0.9735 - val_prc: 0.6266\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.2527e-04 - accuracy: 0.5327 - precision: 0.0038 - recall: 0.9969 - auc: 0.9842 - prc: 0.6669 - val_loss: 0.7469 - val_accuracy: 0.5479 - val_precision: 0.0030 - val_recall: 0.9841 - val_auc: 0.9738 - val_prc: 0.6276\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.17\n",
            "****************************** w =  5\n",
            "Model: \"sequential_102\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_102 (Conv1D)         (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_102 (Dropout)       (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_102 (Flatten)       (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_102 (Dense)           (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89/89 [==============================] - ETA: 0s - loss: 2.3325e-04 - accuracy: 0.1420 - precision: 0.0019 - recall: 0.9845 - auc: 0.9532 - prc: 0.4629"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 18ms/step - loss: 2.3325e-04 - accuracy: 0.1420 - precision: 0.0019 - recall: 0.9845 - auc: 0.9532 - prc: 0.4629 - val_loss: 0.8632 - val_accuracy: 0.0048 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9684 - val_prc: 0.4977\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.9098e-04 - accuracy: 0.0090 - precision: 0.0018 - recall: 1.0000 - auc: 0.9706 - prc: 0.5698 - val_loss: 0.8712 - val_accuracy: 0.0066 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9703 - val_prc: 0.5449\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.8434e-04 - accuracy: 0.0154 - precision: 0.0018 - recall: 1.0000 - auc: 0.9709 - prc: 0.5994 - val_loss: 0.8606 - val_accuracy: 0.0157 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9726 - val_prc: 0.5723\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.7902e-04 - accuracy: 0.0366 - precision: 0.0018 - recall: 1.0000 - auc: 0.9739 - prc: 0.6429 - val_loss: 0.8455 - val_accuracy: 0.0346 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9737 - val_prc: 0.5917\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.7467e-04 - accuracy: 0.0729 - precision: 0.0019 - recall: 0.9969 - auc: 0.9763 - prc: 0.6634 - val_loss: 0.8255 - val_accuracy: 0.0747 - val_precision: 0.0015 - val_recall: 1.0000 - val_auc: 0.9741 - val_prc: 0.6189\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.7187e-04 - accuracy: 0.1191 - precision: 0.0020 - recall: 0.9938 - auc: 0.9753 - prc: 0.6757 - val_loss: 0.8031 - val_accuracy: 0.1507 - val_precision: 0.0016 - val_recall: 0.9841 - val_auc: 0.9750 - val_prc: 0.6383\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.6790e-04 - accuracy: 0.2257 - precision: 0.0023 - recall: 0.9938 - auc: 0.9792 - prc: 0.6898 - val_loss: 0.7828 - val_accuracy: 0.2494 - val_precision: 0.0018 - val_recall: 0.9841 - val_auc: 0.9752 - val_prc: 0.6518\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.6511e-04 - accuracy: 0.3193 - precision: 0.0026 - recall: 0.9907 - auc: 0.9788 - prc: 0.6945 - val_loss: 0.7629 - val_accuracy: 0.3609 - val_precision: 0.0021 - val_recall: 0.9841 - val_auc: 0.9756 - val_prc: 0.6569\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.6262e-04 - accuracy: 0.4141 - precision: 0.0030 - recall: 0.9938 - auc: 0.9794 - prc: 0.7072 - val_loss: 0.7452 - val_accuracy: 0.4576 - val_precision: 0.0025 - val_recall: 0.9841 - val_auc: 0.9760 - val_prc: 0.6580\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.5971e-04 - accuracy: 0.4958 - precision: 0.0035 - recall: 0.9938 - auc: 0.9814 - prc: 0.7153 - val_loss: 0.7263 - val_accuracy: 0.5423 - val_precision: 0.0030 - val_recall: 0.9841 - val_auc: 0.9759 - val_prc: 0.6625\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.5760e-04 - accuracy: 0.5769 - precision: 0.0041 - recall: 0.9876 - auc: 0.9800 - prc: 0.7134 - val_loss: 0.7100 - val_accuracy: 0.6065 - val_precision: 0.0035 - val_recall: 0.9841 - val_auc: 0.9758 - val_prc: 0.6633\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 1.5619e-04 - accuracy: 0.6338 - precision: 0.0048 - recall: 0.9907 - auc: 0.9816 - prc: 0.7191 - val_loss: 0.6970 - val_accuracy: 0.6502 - val_precision: 0.0039 - val_recall: 0.9841 - val_auc: 0.9758 - val_prc: 0.6709\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 1.5368e-04 - accuracy: 0.6758 - precision: 0.0054 - recall: 0.9876 - auc: 0.9801 - prc: 0.7257 - val_loss: 0.6820 - val_accuracy: 0.6900 - val_precision: 0.0044 - val_recall: 0.9841 - val_auc: 0.9759 - val_prc: 0.6716\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.5116e-04 - accuracy: 0.7052 - precision: 0.0059 - recall: 0.9907 - auc: 0.9825 - prc: 0.7267 - val_loss: 0.6666 - val_accuracy: 0.7251 - val_precision: 0.0049 - val_recall: 0.9841 - val_auc: 0.9760 - val_prc: 0.6766\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.4944e-04 - accuracy: 0.7287 - precision: 0.0064 - recall: 0.9876 - auc: 0.9824 - prc: 0.7276 - val_loss: 0.6516 - val_accuracy: 0.7554 - val_precision: 0.0056 - val_recall: 0.9841 - val_auc: 0.9760 - val_prc: 0.6774\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.4801e-04 - accuracy: 0.7598 - precision: 0.0072 - recall: 0.9814 - auc: 0.9825 - prc: 0.7234 - val_loss: 0.6389 - val_accuracy: 0.7761 - val_precision: 0.0061 - val_recall: 0.9841 - val_auc: 0.9761 - val_prc: 0.6799\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.4521e-04 - accuracy: 0.7777 - precision: 0.0078 - recall: 0.9845 - auc: 0.9836 - prc: 0.7304 - val_loss: 0.6256 - val_accuracy: 0.7955 - val_precision: 0.0066 - val_recall: 0.9841 - val_auc: 0.9762 - val_prc: 0.6797\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.4410e-04 - accuracy: 0.8000 - precision: 0.0086 - recall: 0.9721 - auc: 0.9828 - prc: 0.7322 - val_loss: 0.6141 - val_accuracy: 0.8091 - val_precision: 0.0070 - val_recall: 0.9683 - val_auc: 0.9759 - val_prc: 0.6782\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.4277e-04 - accuracy: 0.8112 - precision: 0.0091 - recall: 0.9783 - auc: 0.9829 - prc: 0.7278 - val_loss: 0.6031 - val_accuracy: 0.8204 - val_precision: 0.0074 - val_recall: 0.9683 - val_auc: 0.9759 - val_prc: 0.6794\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.4124e-04 - accuracy: 0.8192 - precision: 0.0095 - recall: 0.9721 - auc: 0.9835 - prc: 0.7299 - val_loss: 0.5917 - val_accuracy: 0.8308 - val_precision: 0.0079 - val_recall: 0.9683 - val_auc: 0.9761 - val_prc: 0.6793\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.18000000000000002\n",
            "****************************** w =  5\n",
            "Model: \"sequential_103\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_103 (Conv1D)         (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_103 (Dropout)       (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_103 (Flatten)       (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_103 (Dense)           (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89/89 [==============================] - ETA: 0s - loss: 2.6502e-04 - accuracy: 0.2294 - precision: 0.0022 - recall: 0.9845 - auc: 0.9439 - prc: 0.5571"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 21ms/step - loss: 2.6502e-04 - accuracy: 0.2294 - precision: 0.0022 - recall: 0.9845 - auc: 0.9439 - prc: 0.5571 - val_loss: 0.8142 - val_accuracy: 0.0134 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9587 - val_prc: 0.6327\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 2.2670e-04 - accuracy: 0.0403 - precision: 0.0018 - recall: 1.0000 - auc: 0.9620 - prc: 0.6783 - val_loss: 0.7978 - val_accuracy: 0.0550 - val_precision: 0.0015 - val_recall: 1.0000 - val_auc: 0.9636 - val_prc: 0.6502\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.1607e-04 - accuracy: 0.1235 - precision: 0.0020 - recall: 0.9969 - auc: 0.9693 - prc: 0.7013 - val_loss: 0.7669 - val_accuracy: 0.1938 - val_precision: 0.0017 - val_recall: 0.9841 - val_auc: 0.9661 - val_prc: 0.6599\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.0725e-04 - accuracy: 0.3092 - precision: 0.0026 - recall: 0.9938 - auc: 0.9704 - prc: 0.7219 - val_loss: 0.7365 - val_accuracy: 0.3836 - val_precision: 0.0022 - val_recall: 0.9841 - val_auc: 0.9678 - val_prc: 0.6732\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.0175e-04 - accuracy: 0.4731 - precision: 0.0033 - recall: 0.9814 - auc: 0.9704 - prc: 0.7323 - val_loss: 0.7052 - val_accuracy: 0.5840 - val_precision: 0.0032 - val_recall: 0.9683 - val_auc: 0.9683 - val_prc: 0.6760\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.9558e-04 - accuracy: 0.6530 - precision: 0.0050 - recall: 0.9752 - auc: 0.9739 - prc: 0.7380 - val_loss: 0.6776 - val_accuracy: 0.7087 - val_precision: 0.0046 - val_recall: 0.9683 - val_auc: 0.9685 - val_prc: 0.6824\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 1.9062e-04 - accuracy: 0.7400 - precision: 0.0066 - recall: 0.9659 - auc: 0.9745 - prc: 0.7430 - val_loss: 0.6501 - val_accuracy: 0.7833 - val_precision: 0.0061 - val_recall: 0.9524 - val_auc: 0.9691 - val_prc: 0.6824\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.8585e-04 - accuracy: 0.7981 - precision: 0.0084 - recall: 0.9628 - auc: 0.9770 - prc: 0.7394 - val_loss: 0.6250 - val_accuracy: 0.8290 - val_precision: 0.0077 - val_recall: 0.9524 - val_auc: 0.9695 - val_prc: 0.6915\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.8169e-04 - accuracy: 0.8327 - precision: 0.0101 - recall: 0.9628 - auc: 0.9768 - prc: 0.7485 - val_loss: 0.6017 - val_accuracy: 0.8619 - val_precision: 0.0095 - val_recall: 0.9524 - val_auc: 0.9698 - val_prc: 0.6960\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.7916e-04 - accuracy: 0.8631 - precision: 0.0122 - recall: 0.9505 - auc: 0.9786 - prc: 0.7478 - val_loss: 0.5831 - val_accuracy: 0.8799 - val_precision: 0.0109 - val_recall: 0.9524 - val_auc: 0.9702 - val_prc: 0.7106\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.7525e-04 - accuracy: 0.8817 - precision: 0.0139 - recall: 0.9381 - auc: 0.9768 - prc: 0.7468 - val_loss: 0.5653 - val_accuracy: 0.8928 - val_precision: 0.0122 - val_recall: 0.9524 - val_auc: 0.9705 - val_prc: 0.7101\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.7293e-04 - accuracy: 0.8914 - precision: 0.0150 - recall: 0.9319 - auc: 0.9766 - prc: 0.7439 - val_loss: 0.5488 - val_accuracy: 0.9029 - val_precision: 0.0134 - val_recall: 0.9524 - val_auc: 0.9712 - val_prc: 0.7232\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.6975e-04 - accuracy: 0.9024 - precision: 0.0168 - recall: 0.9350 - auc: 0.9780 - prc: 0.7496 - val_loss: 0.5336 - val_accuracy: 0.9103 - val_precision: 0.0145 - val_recall: 0.9524 - val_auc: 0.9711 - val_prc: 0.7247\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 1.6588e-04 - accuracy: 0.9078 - precision: 0.0180 - recall: 0.9474 - auc: 0.9810 - prc: 0.7467 - val_loss: 0.5181 - val_accuracy: 0.9169 - val_precision: 0.0154 - val_recall: 0.9365 - val_auc: 0.9712 - val_prc: 0.7246\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.6283e-04 - accuracy: 0.9188 - precision: 0.0201 - recall: 0.9350 - auc: 0.9822 - prc: 0.7511 - val_loss: 0.5052 - val_accuracy: 0.9209 - val_precision: 0.0162 - val_recall: 0.9365 - val_auc: 0.9720 - val_prc: 0.7264\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.6041e-04 - accuracy: 0.9196 - precision: 0.0203 - recall: 0.9350 - auc: 0.9816 - prc: 0.7497 - val_loss: 0.4917 - val_accuracy: 0.9251 - val_precision: 0.0171 - val_recall: 0.9365 - val_auc: 0.9716 - val_prc: 0.7278\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.5846e-04 - accuracy: 0.9250 - precision: 0.0217 - recall: 0.9319 - auc: 0.9811 - prc: 0.7470 - val_loss: 0.4802 - val_accuracy: 0.9277 - val_precision: 0.0177 - val_recall: 0.9365 - val_auc: 0.9722 - val_prc: 0.7307\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.5639e-04 - accuracy: 0.9268 - precision: 0.0223 - recall: 0.9350 - auc: 0.9829 - prc: 0.7495 - val_loss: 0.4697 - val_accuracy: 0.9296 - val_precision: 0.0181 - val_recall: 0.9365 - val_auc: 0.9721 - val_prc: 0.7285\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.5477e-04 - accuracy: 0.9291 - precision: 0.0229 - recall: 0.9319 - auc: 0.9833 - prc: 0.7494 - val_loss: 0.4613 - val_accuracy: 0.9301 - val_precision: 0.0183 - val_recall: 0.9365 - val_auc: 0.9722 - val_prc: 0.7251\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.5063e-04 - accuracy: 0.9244 - precision: 0.0218 - recall: 0.9474 - auc: 0.9853 - prc: 0.7442 - val_loss: 0.4483 - val_accuracy: 0.9334 - val_precision: 0.0188 - val_recall: 0.9206 - val_auc: 0.9719 - val_prc: 0.7286\n",
            "1419/1419 [==============================] - 3s 2ms/step\n",
            "****************************** w =  0.19000000000000003\n",
            "****************************** w =  5\n",
            "Model: \"sequential_104\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_104 (Conv1D)         (None, 29, 128)           512       \n",
            "                                                                 \n",
            " dropout_104 (Dropout)       (None, 29, 128)           0         \n",
            "                                                                 \n",
            " flatten_104 (Flatten)       (None, 3712)              0         \n",
            "                                                                 \n",
            " dense_104 (Dense)           (None, 1)                 3713      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,225\n",
            "Trainable params: 4,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89/89 [==============================] - ETA: 0s - loss: 3.0404e-04 - accuracy: 0.2274 - precision: 0.0022 - recall: 0.9819 - auc: 0.9621 - prc: 0.5405"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 18ms/step - loss: 3.0404e-04 - accuracy: 0.2274 - precision: 0.0022 - recall: 0.9819 - auc: 0.9621 - prc: 0.5405 - val_loss: 0.7985 - val_accuracy: 0.0234 - val_precision: 0.0014 - val_recall: 1.0000 - val_auc: 0.9695 - val_prc: 0.5494\n",
            "Epoch 2/20\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 2.6481e-04 - accuracy: 0.1251 - precision: 0.0020 - recall: 0.9938 - auc: 0.9673 - prc: 0.6508 - val_loss: 0.7511 - val_accuracy: 0.2212 - val_precision: 0.0018 - val_recall: 0.9841 - val_auc: 0.9722 - val_prc: 0.6266\n",
            "Epoch 3/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 2.5025e-04 - accuracy: 0.4147 - precision: 0.0030 - recall: 0.9845 - auc: 0.9709 - prc: 0.7041 - val_loss: 0.7032 - val_accuracy: 0.5737 - val_precision: 0.0032 - val_recall: 0.9841 - val_auc: 0.9736 - val_prc: 0.6617\n",
            "Epoch 4/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.3896e-04 - accuracy: 0.6675 - precision: 0.0052 - recall: 0.9752 - auc: 0.9729 - prc: 0.7246 - val_loss: 0.6553 - val_accuracy: 0.7889 - val_precision: 0.0063 - val_recall: 0.9683 - val_auc: 0.9743 - val_prc: 0.6703\n",
            "Epoch 5/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 2.2824e-04 - accuracy: 0.8222 - precision: 0.0095 - recall: 0.9598 - auc: 0.9750 - prc: 0.7303 - val_loss: 0.6140 - val_accuracy: 0.8778 - val_precision: 0.0107 - val_recall: 0.9524 - val_auc: 0.9747 - val_prc: 0.6787\n",
            "Epoch 6/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.2054e-04 - accuracy: 0.8918 - precision: 0.0151 - recall: 0.9319 - auc: 0.9735 - prc: 0.7315 - val_loss: 0.5819 - val_accuracy: 0.9110 - val_precision: 0.0146 - val_recall: 0.9524 - val_auc: 0.9747 - val_prc: 0.6804\n",
            "Epoch 7/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.1192e-04 - accuracy: 0.9215 - precision: 0.0208 - recall: 0.9350 - auc: 0.9762 - prc: 0.7383 - val_loss: 0.5512 - val_accuracy: 0.9308 - val_precision: 0.0185 - val_recall: 0.9365 - val_auc: 0.9748 - val_prc: 0.6824\n",
            "Epoch 8/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 2.0601e-04 - accuracy: 0.9338 - precision: 0.0245 - recall: 0.9319 - auc: 0.9768 - prc: 0.7388 - val_loss: 0.5231 - val_accuracy: 0.9421 - val_precision: 0.0220 - val_recall: 0.9365 - val_auc: 0.9740 - val_prc: 0.6867\n",
            "Epoch 9/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.9970e-04 - accuracy: 0.9455 - precision: 0.0293 - recall: 0.9226 - auc: 0.9776 - prc: 0.7436 - val_loss: 0.4995 - val_accuracy: 0.9489 - val_precision: 0.0248 - val_recall: 0.9365 - val_auc: 0.9745 - val_prc: 0.6890\n",
            "Epoch 10/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.9445e-04 - accuracy: 0.9507 - precision: 0.0323 - recall: 0.9226 - auc: 0.9799 - prc: 0.7426 - val_loss: 0.4773 - val_accuracy: 0.9529 - val_precision: 0.0269 - val_recall: 0.9365 - val_auc: 0.9740 - val_prc: 0.6940\n",
            "Epoch 11/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.9101e-04 - accuracy: 0.9535 - precision: 0.0341 - recall: 0.9195 - auc: 0.9795 - prc: 0.7454 - val_loss: 0.4575 - val_accuracy: 0.9563 - val_precision: 0.0290 - val_recall: 0.9365 - val_auc: 0.9740 - val_prc: 0.6962\n",
            "Epoch 12/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.8698e-04 - accuracy: 0.9589 - precision: 0.0382 - recall: 0.9133 - auc: 0.9763 - prc: 0.7509 - val_loss: 0.4412 - val_accuracy: 0.9584 - val_precision: 0.0303 - val_recall: 0.9365 - val_auc: 0.9741 - val_prc: 0.6977\n",
            "Epoch 13/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.8290e-04 - accuracy: 0.9602 - precision: 0.0394 - recall: 0.9133 - auc: 0.9792 - prc: 0.7488 - val_loss: 0.4263 - val_accuracy: 0.9596 - val_precision: 0.0307 - val_recall: 0.9206 - val_auc: 0.9742 - val_prc: 0.6974\n",
            "Epoch 14/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.7897e-04 - accuracy: 0.9611 - precision: 0.0404 - recall: 0.9164 - auc: 0.9787 - prc: 0.7530 - val_loss: 0.4114 - val_accuracy: 0.9616 - val_precision: 0.0323 - val_recall: 0.9206 - val_auc: 0.9744 - val_prc: 0.6996\n",
            "Epoch 15/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.7509e-04 - accuracy: 0.9608 - precision: 0.0398 - recall: 0.9102 - auc: 0.9813 - prc: 0.7530 - val_loss: 0.3964 - val_accuracy: 0.9635 - val_precision: 0.0339 - val_recall: 0.9206 - val_auc: 0.9745 - val_prc: 0.7010\n",
            "Epoch 16/20\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 1.7204e-04 - accuracy: 0.9632 - precision: 0.0424 - recall: 0.9102 - auc: 0.9822 - prc: 0.7488 - val_loss: 0.3841 - val_accuracy: 0.9643 - val_precision: 0.0346 - val_recall: 0.9206 - val_auc: 0.9745 - val_prc: 0.7009\n",
            "Epoch 17/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.7060e-04 - accuracy: 0.9645 - precision: 0.0437 - recall: 0.9071 - auc: 0.9794 - prc: 0.7466 - val_loss: 0.3740 - val_accuracy: 0.9645 - val_precision: 0.0349 - val_recall: 0.9206 - val_auc: 0.9748 - val_prc: 0.7028\n",
            "Epoch 18/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.6710e-04 - accuracy: 0.9661 - precision: 0.0454 - recall: 0.9009 - auc: 0.9817 - prc: 0.7508 - val_loss: 0.3657 - val_accuracy: 0.9645 - val_precision: 0.0349 - val_recall: 0.9206 - val_auc: 0.9746 - val_prc: 0.7045\n",
            "Epoch 19/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.6384e-04 - accuracy: 0.9648 - precision: 0.0444 - recall: 0.9133 - auc: 0.9831 - prc: 0.7507 - val_loss: 0.3567 - val_accuracy: 0.9647 - val_precision: 0.0350 - val_recall: 0.9206 - val_auc: 0.9746 - val_prc: 0.7049\n",
            "Epoch 20/20\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 1.6161e-04 - accuracy: 0.9633 - precision: 0.0429 - recall: 0.9195 - auc: 0.9840 - prc: 0.7461 - val_loss: 0.3470 - val_accuracy: 0.9653 - val_precision: 0.0356 - val_recall: 0.9206 - val_auc: 0.9745 - val_prc: 0.7145\n",
            "1419/1419 [==============================] - 3s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (len(val_preds_rms_gamma)):\n",
        "  print(\"parameters for this case : (w,gamma) \" ,parameters[i] )\n",
        "  print(classification_report(y_val, val_preds_rms_gamma[i]))\n",
        "        "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZ0-7fEBj5K5",
        "outputId": "43ae15a1-1452-47ca-a1bc-e533b4449287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "parameters for this case : (w,gamma)  [0.01, 2]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.02      0.04     45333\n",
            "           1       0.00      1.00      0.00        63\n",
            "\n",
            "    accuracy                           0.02     45396\n",
            "   macro avg       0.50      0.51      0.02     45396\n",
            "weighted avg       1.00      0.02      0.04     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.02, 2]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.91      0.95     45333\n",
            "           1       0.01      0.94      0.03        63\n",
            "\n",
            "    accuracy                           0.91     45396\n",
            "   macro avg       0.51      0.92      0.49     45396\n",
            "weighted avg       1.00      0.91      0.95     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.03, 2]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.97      0.98     45333\n",
            "           1       0.04      0.90      0.07        63\n",
            "\n",
            "    accuracy                           0.97     45396\n",
            "   macro avg       0.52      0.94      0.53     45396\n",
            "weighted avg       1.00      0.97      0.98     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.04, 2]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99     45333\n",
            "           1       0.06      0.90      0.11        63\n",
            "\n",
            "    accuracy                           0.98     45396\n",
            "   macro avg       0.53      0.94      0.55     45396\n",
            "weighted avg       1.00      0.98      0.99     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.05, 2]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      0.99     45333\n",
            "           1       0.11      0.89      0.19        63\n",
            "\n",
            "    accuracy                           0.99     45396\n",
            "   macro avg       0.55      0.94      0.59     45396\n",
            "weighted avg       1.00      0.99      0.99     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.060000000000000005, 2]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      1.00     45333\n",
            "           1       0.12      0.89      0.21        63\n",
            "\n",
            "    accuracy                           0.99     45396\n",
            "   macro avg       0.56      0.94      0.60     45396\n",
            "weighted avg       1.00      0.99      0.99     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.07, 2]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      1.00     45333\n",
            "           1       0.16      0.87      0.27        63\n",
            "\n",
            "    accuracy                           0.99     45396\n",
            "   macro avg       0.58      0.93      0.63     45396\n",
            "weighted avg       1.00      0.99      1.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.08, 2]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.22      0.84      0.35        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.61      0.92      0.67     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.09, 2]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.29      0.84      0.43        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.65      0.92      0.72     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.09999999999999999, 2]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.42      0.84      0.56        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.71      0.92      0.78     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.10999999999999999, 2]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.44      0.84      0.58        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.72      0.92      0.79     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.11999999999999998, 2]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.42      0.83      0.56        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.71      0.91      0.78     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.12999999999999998, 2]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.44      0.83      0.57        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.72      0.91      0.79     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.13999999999999999, 2]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.51      0.81      0.63        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.75      0.90      0.81     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.15, 2]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.51      0.81      0.63        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.75      0.90      0.81     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.16, 2]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.55      0.81      0.66        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.78      0.90      0.83     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.17, 2]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.64      0.81      0.71        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.82      0.90      0.86     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.18000000000000002, 2]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.68      0.81      0.74        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.84      0.90      0.87     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.19000000000000003, 2]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.65      0.81      0.72        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.82      0.90      0.86     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.01, 3]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00     45333\n",
            "           1       0.00      1.00      0.00        63\n",
            "\n",
            "    accuracy                           0.00     45396\n",
            "   macro avg       0.00      0.50      0.00     45396\n",
            "weighted avg       0.00      0.00      0.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.02, 3]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00     45333\n",
            "           1       0.00      1.00      0.00        63\n",
            "\n",
            "    accuracy                           0.00     45396\n",
            "   macro avg       0.00      0.50      0.00     45396\n",
            "weighted avg       0.00      0.00      0.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.03, 3]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00     45333\n",
            "           1       0.00      1.00      0.00        63\n",
            "\n",
            "    accuracy                           0.00     45396\n",
            "   macro avg       0.00      0.50      0.00     45396\n",
            "weighted avg       0.00      0.00      0.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.04, 3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00     45333\n",
            "           1       0.00      1.00      0.00        63\n",
            "\n",
            "    accuracy                           0.00     45396\n",
            "   macro avg       0.00      0.50      0.00     45396\n",
            "weighted avg       0.00      0.00      0.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.05, 3]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.23      0.37     45333\n",
            "           1       0.00      0.98      0.00        63\n",
            "\n",
            "    accuracy                           0.23     45396\n",
            "   macro avg       0.50      0.61      0.19     45396\n",
            "weighted avg       1.00      0.23      0.37     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.060000000000000005, 3]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.77      0.87     45333\n",
            "           1       0.01      0.97      0.01        63\n",
            "\n",
            "    accuracy                           0.77     45396\n",
            "   macro avg       0.50      0.87      0.44     45396\n",
            "weighted avg       1.00      0.77      0.87     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.07, 3]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.91      0.95     45333\n",
            "           1       0.01      0.94      0.03        63\n",
            "\n",
            "    accuracy                           0.91     45396\n",
            "   macro avg       0.51      0.92      0.49     45396\n",
            "weighted avg       1.00      0.91      0.95     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.08, 3]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.95      0.97     45333\n",
            "           1       0.03      0.92      0.05        63\n",
            "\n",
            "    accuracy                           0.95     45396\n",
            "   macro avg       0.51      0.94      0.51     45396\n",
            "weighted avg       1.00      0.95      0.97     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.09, 3]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.97      0.98     45333\n",
            "           1       0.04      0.90      0.08        63\n",
            "\n",
            "    accuracy                           0.97     45396\n",
            "   macro avg       0.52      0.94      0.53     45396\n",
            "weighted avg       1.00      0.97      0.98     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.09999999999999999, 3]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99     45333\n",
            "           1       0.05      0.90      0.10        63\n",
            "\n",
            "    accuracy                           0.98     45396\n",
            "   macro avg       0.53      0.94      0.54     45396\n",
            "weighted avg       1.00      0.98      0.99     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.10999999999999999, 3]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99     45333\n",
            "           1       0.07      0.90      0.13        63\n",
            "\n",
            "    accuracy                           0.98     45396\n",
            "   macro avg       0.53      0.94      0.56     45396\n",
            "weighted avg       1.00      0.98      0.99     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.11999999999999998, 3]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      0.99     45333\n",
            "           1       0.09      0.89      0.17        63\n",
            "\n",
            "    accuracy                           0.99     45396\n",
            "   macro avg       0.55      0.94      0.58     45396\n",
            "weighted avg       1.00      0.99      0.99     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.12999999999999998, 3]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      1.00     45333\n",
            "           1       0.13      0.89      0.23        63\n",
            "\n",
            "    accuracy                           0.99     45396\n",
            "   macro avg       0.57      0.94      0.61     45396\n",
            "weighted avg       1.00      0.99      0.99     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.13999999999999999, 3]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      1.00     45333\n",
            "           1       0.16      0.84      0.28        63\n",
            "\n",
            "    accuracy                           0.99     45396\n",
            "   macro avg       0.58      0.92      0.64     45396\n",
            "weighted avg       1.00      0.99      1.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.15, 3]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      1.00     45333\n",
            "           1       0.18      0.86      0.30        63\n",
            "\n",
            "    accuracy                           0.99     45396\n",
            "   macro avg       0.59      0.93      0.65     45396\n",
            "weighted avg       1.00      0.99      1.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.16, 3]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.26      0.84      0.40        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.63      0.92      0.70     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.17, 3]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.25      0.84      0.38        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.62      0.92      0.69     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.18000000000000002, 3]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.37      0.84      0.52        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.69      0.92      0.76     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.19000000000000003, 3]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.35      0.84      0.50        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.68      0.92      0.75     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.01, 4]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00     45333\n",
            "           1       0.00      1.00      0.00        63\n",
            "\n",
            "    accuracy                           0.00     45396\n",
            "   macro avg       0.00      0.50      0.00     45396\n",
            "weighted avg       0.00      0.00      0.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.02, 4]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00     45333\n",
            "           1       0.00      1.00      0.00        63\n",
            "\n",
            "    accuracy                           0.00     45396\n",
            "   macro avg       0.00      0.50      0.00     45396\n",
            "weighted avg       0.00      0.00      0.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.03, 4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00     45333\n",
            "           1       0.00      1.00      0.00        63\n",
            "\n",
            "    accuracy                           0.00     45396\n",
            "   macro avg       0.00      0.50      0.00     45396\n",
            "weighted avg       0.00      0.00      0.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.04, 4]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00     45333\n",
            "           1       0.00      1.00      0.00        63\n",
            "\n",
            "    accuracy                           0.00     45396\n",
            "   macro avg       0.00      0.50      0.00     45396\n",
            "weighted avg       0.00      0.00      0.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.05, 4]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00     45333\n",
            "           1       0.00      1.00      0.00        63\n",
            "\n",
            "    accuracy                           0.00     45396\n",
            "   macro avg       0.00      0.50      0.00     45396\n",
            "weighted avg       0.00      0.00      0.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.060000000000000005, 4]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00     45333\n",
            "           1       0.00      1.00      0.00        63\n",
            "\n",
            "    accuracy                           0.00     45396\n",
            "   macro avg       0.00      0.50      0.00     45396\n",
            "weighted avg       0.00      0.00      0.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.07, 4]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00     45333\n",
            "           1       0.00      1.00      0.00        63\n",
            "\n",
            "    accuracy                           0.00     45396\n",
            "   macro avg       0.00      0.50      0.00     45396\n",
            "weighted avg       0.00      0.00      0.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.08, 4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00     45333\n",
            "           1       0.00      1.00      0.00        63\n",
            "\n",
            "    accuracy                           0.00     45396\n",
            "   macro avg       0.00      0.50      0.00     45396\n",
            "weighted avg       0.00      0.00      0.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.09, 4]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00     45333\n",
            "           1       0.00      1.00      0.00        63\n",
            "\n",
            "    accuracy                           0.00     45396\n",
            "   macro avg       0.00      0.50      0.00     45396\n",
            "weighted avg       0.00      0.00      0.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.09999999999999999, 4]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.11      0.20     45333\n",
            "           1       0.00      0.98      0.00        63\n",
            "\n",
            "    accuracy                           0.11     45396\n",
            "   macro avg       0.50      0.55      0.10     45396\n",
            "weighted avg       1.00      0.11      0.20     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.10999999999999999, 4]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.65      0.79     45333\n",
            "           1       0.00      0.97      0.01        63\n",
            "\n",
            "    accuracy                           0.65     45396\n",
            "   macro avg       0.50      0.81      0.40     45396\n",
            "weighted avg       1.00      0.65      0.79     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.11999999999999998, 4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.86      0.92     45333\n",
            "           1       0.01      0.95      0.02        63\n",
            "\n",
            "    accuracy                           0.86     45396\n",
            "   macro avg       0.50      0.91      0.47     45396\n",
            "weighted avg       1.00      0.86      0.92     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.12999999999999998, 4]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.93      0.96     45333\n",
            "           1       0.02      0.92      0.03        63\n",
            "\n",
            "    accuracy                           0.93     45396\n",
            "   macro avg       0.51      0.92      0.50     45396\n",
            "weighted avg       1.00      0.93      0.96     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.13999999999999999, 4]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.96      0.98     45333\n",
            "           1       0.03      0.92      0.06        63\n",
            "\n",
            "    accuracy                           0.96     45396\n",
            "   macro avg       0.52      0.94      0.52     45396\n",
            "weighted avg       1.00      0.96      0.98     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.15, 4]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.97      0.99     45333\n",
            "           1       0.05      0.92      0.09        63\n",
            "\n",
            "    accuracy                           0.97     45396\n",
            "   macro avg       0.52      0.95      0.54     45396\n",
            "weighted avg       1.00      0.97      0.99     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.16, 4]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99     45333\n",
            "           1       0.06      0.90      0.12        63\n",
            "\n",
            "    accuracy                           0.98     45396\n",
            "   macro avg       0.53      0.94      0.56     45396\n",
            "weighted avg       1.00      0.98      0.99     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.17, 4]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99     45333\n",
            "           1       0.08      0.90      0.14        63\n",
            "\n",
            "    accuracy                           0.98     45396\n",
            "   macro avg       0.54      0.94      0.57     45396\n",
            "weighted avg       1.00      0.98      0.99     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.18000000000000002, 4]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      0.99     45333\n",
            "           1       0.09      0.89      0.16        63\n",
            "\n",
            "    accuracy                           0.99     45396\n",
            "   macro avg       0.54      0.94      0.58     45396\n",
            "weighted avg       1.00      0.99      0.99     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.19000000000000003, 4]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      1.00     45333\n",
            "           1       0.13      0.87      0.22        63\n",
            "\n",
            "    accuracy                           0.99     45396\n",
            "   macro avg       0.56      0.93      0.61     45396\n",
            "weighted avg       1.00      0.99      0.99     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.01, 5]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00     45333\n",
            "           1       0.00      1.00      0.00        63\n",
            "\n",
            "    accuracy                           0.00     45396\n",
            "   macro avg       0.00      0.50      0.00     45396\n",
            "weighted avg       0.00      0.00      0.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.02, 5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00     45333\n",
            "           1       0.00      1.00      0.00        63\n",
            "\n",
            "    accuracy                           0.00     45396\n",
            "   macro avg       0.00      0.50      0.00     45396\n",
            "weighted avg       0.00      0.00      0.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.03, 5]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00     45333\n",
            "           1       0.00      1.00      0.00        63\n",
            "\n",
            "    accuracy                           0.00     45396\n",
            "   macro avg       0.00      0.50      0.00     45396\n",
            "weighted avg       0.00      0.00      0.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.04, 5]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00     45333\n",
            "           1       0.00      1.00      0.00        63\n",
            "\n",
            "    accuracy                           0.00     45396\n",
            "   macro avg       0.00      0.50      0.00     45396\n",
            "weighted avg       0.00      0.00      0.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.05, 5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00     45333\n",
            "           1       0.00      1.00      0.00        63\n",
            "\n",
            "    accuracy                           0.00     45396\n",
            "   macro avg       0.00      0.50      0.00     45396\n",
            "weighted avg       0.00      0.00      0.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.060000000000000005, 5]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00     45333\n",
            "           1       0.00      1.00      0.00        63\n",
            "\n",
            "    accuracy                           0.00     45396\n",
            "   macro avg       0.00      0.50      0.00     45396\n",
            "weighted avg       0.00      0.00      0.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.07, 5]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00     45333\n",
            "           1       0.00      1.00      0.00        63\n",
            "\n",
            "    accuracy                           0.00     45396\n",
            "   macro avg       0.00      0.50      0.00     45396\n",
            "weighted avg       0.00      0.00      0.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.08, 5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00     45333\n",
            "           1       0.00      1.00      0.00        63\n",
            "\n",
            "    accuracy                           0.00     45396\n",
            "   macro avg       0.00      0.50      0.00     45396\n",
            "weighted avg       0.00      0.00      0.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.09, 5]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00     45333\n",
            "           1       0.00      1.00      0.00        63\n",
            "\n",
            "    accuracy                           0.00     45396\n",
            "   macro avg       0.00      0.50      0.00     45396\n",
            "weighted avg       0.00      0.00      0.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.09999999999999999, 5]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00     45333\n",
            "           1       0.00      1.00      0.00        63\n",
            "\n",
            "    accuracy                           0.00     45396\n",
            "   macro avg       0.00      0.50      0.00     45396\n",
            "weighted avg       0.00      0.00      0.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.10999999999999999, 5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00     45333\n",
            "           1       0.00      1.00      0.00        63\n",
            "\n",
            "    accuracy                           0.00     45396\n",
            "   macro avg       0.00      0.50      0.00     45396\n",
            "weighted avg       0.00      0.00      0.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.11999999999999998, 5]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00     45333\n",
            "           1       0.00      1.00      0.00        63\n",
            "\n",
            "    accuracy                           0.00     45396\n",
            "   macro avg       0.00      0.50      0.00     45396\n",
            "weighted avg       0.00      0.00      0.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.12999999999999998, 5]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00     45333\n",
            "           1       0.00      1.00      0.00        63\n",
            "\n",
            "    accuracy                           0.00     45396\n",
            "   macro avg       0.00      0.50      0.00     45396\n",
            "weighted avg       0.00      0.00      0.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.13999999999999999, 5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.00      0.00     45333\n",
            "           1       0.00      1.00      0.00        63\n",
            "\n",
            "    accuracy                           0.00     45396\n",
            "   macro avg       0.50      0.50      0.00     45396\n",
            "weighted avg       1.00      0.00      0.00     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.15, 5]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.06      0.12     45333\n",
            "           1       0.00      1.00      0.00        63\n",
            "\n",
            "    accuracy                           0.07     45396\n",
            "   macro avg       0.50      0.53      0.06     45396\n",
            "weighted avg       1.00      0.07      0.12     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.16, 5]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.55      0.71     45333\n",
            "           1       0.00      0.98      0.01        63\n",
            "\n",
            "    accuracy                           0.55     45396\n",
            "   macro avg       0.50      0.77      0.36     45396\n",
            "weighted avg       1.00      0.55      0.71     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.17, 5]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.83      0.91     45333\n",
            "           1       0.01      0.97      0.02        63\n",
            "\n",
            "    accuracy                           0.83     45396\n",
            "   macro avg       0.50      0.90      0.46     45396\n",
            "weighted avg       1.00      0.83      0.91     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.18000000000000002, 5]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.93      0.97     45333\n",
            "           1       0.02      0.92      0.04        63\n",
            "\n",
            "    accuracy                           0.93     45396\n",
            "   macro avg       0.51      0.93      0.50     45396\n",
            "weighted avg       1.00      0.93      0.96     45396\n",
            "\n",
            "parameters for this case : (w,gamma)  [0.19000000000000003, 5]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.97      0.98     45333\n",
            "           1       0.04      0.92      0.07        63\n",
            "\n",
            "    accuracy                           0.97     45396\n",
            "   macro avg       0.52      0.94      0.53     45396\n",
            "weighted avg       1.00      0.97      0.98     45396\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_pd = pd.DataFrame(val_preds_rms_gamma)\n",
        "parameters_pd = pd.DataFrame(parameters)"
      ],
      "metadata": {
        "id": "DlX-vUXZna-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_preds_rms_gamma[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpzSMsckn3bC",
        "outputId": "eea3187a-db17-42a0-e585-7ab0cef9f3b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(45396, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature Selection"
      ],
      "metadata": {
        "id": "oYJ6WB92WQeM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature selection using variance"
      ],
      "metadata": {
        "id": "xopzkyBBYo0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_FS = data[['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9','V10', 'Amount', 'Class']]"
      ],
      "metadata": {
        "id": "HI0vLEWzRIAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_FS = data_FS.drop(columns=['Class'])\n",
        "Y_FS = pd.DataFrame(data['Class'], columns=['Class'])"
      ],
      "metadata": {
        "id": "ew_g1sItWqnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_fs,X_test_fs,y_fs,Y_test_fs= train_test_split(X_FS,Y_FS, random_state=0, test_size=0.2)\n",
        "X_train_fs,X_val_fs,Y_train_fs,Y_val_fs= train_test_split(x_fs,y_fs, random_state=0, test_size=0.2)"
      ],
      "metadata": {
        "id": "PCdn5GHaXKaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler_fs=StandardScaler().fit(X_train_fs)\n",
        "X_train_norm_fs= scaler_fs.transform(X_train_fs)\n",
        "X_val_norm_fs  = scaler_fs.transform(X_val_fs)\n",
        "X_test_norm_fs = scaler_fs.transform(X_test_fs)"
      ],
      "metadata": {
        "id": "Oy57tBkaZaD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def create_cnn():\n",
        "  model = Sequential()\n",
        "  #model.add(Dense(32, activation='relu', input_dim=29))\n",
        "  model.add(Conv1D(128, 3, activation='relu', padding ='same', input_shape=(X_train_fs.shape[-1],1)))\n",
        "  model.add(Dropout(0.2))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  model.compile(optimizer=Adam(0.01),loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),metrics=METRICS)\n",
        "  return model\n",
        "\n"
      ],
      "metadata": {
        "id": "H2h_wWYSZ4os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_fs = create_cnn()\n",
        "cnn_fs.summary()\n",
        "cnn_fs.fit(X_train_norm_fs,Y_train_fs, batch_size=2048, epochs=100, callbacks=[early_stopping],validation_data=(X_val_norm_fs,Y_val_fs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEU8k37_aQa7",
        "outputId": "cf38dfff-d7d0-4282-b6a3-c77f013b19b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_113\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_113 (Conv1D)         (None, 12, 128)           512       \n",
            "                                                                 \n",
            " dropout_113 (Dropout)       (None, 12, 128)           0         \n",
            "                                                                 \n",
            " flatten_113 (Flatten)       (None, 1536)              0         \n",
            "                                                                 \n",
            " dense_113 (Dense)           (None, 1)                 1537      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,049\n",
            "Trainable params: 2,049\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89/89 [==============================] - ETA: 0s - loss: 0.0386 - accuracy: 0.9918 - precision: 0.0609 - recall: 0.2668 - auc: 0.7220 - prc: 0.2388"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r89/89 [==============================] - 3s 16ms/step - loss: 0.0386 - accuracy: 0.9918 - precision: 0.0609 - recall: 0.2668 - auc: 0.7220 - prc: 0.2388 - val_loss: 0.0041 - val_accuracy: 0.9992 - val_precision: 0.8857 - val_recall: 0.4921 - val_auc: 0.8863 - val_prc: 0.6583\n",
            "Epoch 2/100\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0050 - accuracy: 0.9991 - precision: 0.8618 - recall: 0.5789 - auc: 0.9189 - prc: 0.6696 - val_loss: 0.0034 - val_accuracy: 0.9993 - val_precision: 0.8667 - val_recall: 0.6190 - val_auc: 0.9266 - val_prc: 0.7207\n",
            "Epoch 3/100\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 0.0045 - accuracy: 0.9991 - precision: 0.8610 - recall: 0.5944 - auc: 0.9195 - prc: 0.6873 - val_loss: 0.0034 - val_accuracy: 0.9993 - val_precision: 0.9167 - val_recall: 0.5238 - val_auc: 0.9194 - val_prc: 0.7262\n",
            "Epoch 4/100\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 0.0045 - accuracy: 0.9992 - precision: 0.8772 - recall: 0.6192 - auc: 0.9195 - prc: 0.6791 - val_loss: 0.0039 - val_accuracy: 0.9992 - val_precision: 0.7414 - val_recall: 0.6825 - val_auc: 0.9189 - val_prc: 0.6370\n",
            "Epoch 5/100\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 0.0043 - accuracy: 0.9991 - precision: 0.8630 - recall: 0.5851 - auc: 0.9259 - prc: 0.6854 - val_loss: 0.0036 - val_accuracy: 0.9994 - val_precision: 0.8269 - val_recall: 0.6825 - val_auc: 0.9188 - val_prc: 0.6900\n",
            "Epoch 6/100\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 0.0042 - accuracy: 0.9992 - precision: 0.8596 - recall: 0.6254 - auc: 0.9226 - prc: 0.7012 - val_loss: 0.0037 - val_accuracy: 0.9993 - val_precision: 0.7818 - val_recall: 0.6825 - val_auc: 0.9267 - val_prc: 0.6906\n",
            "Epoch 7/100\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 0.0042 - accuracy: 0.9992 - precision: 0.8548 - recall: 0.6378 - auc: 0.9243 - prc: 0.7185 - val_loss: 0.0033 - val_accuracy: 0.9993 - val_precision: 0.9167 - val_recall: 0.5238 - val_auc: 0.9123 - val_prc: 0.7395\n",
            "Epoch 8/100\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 0.0042 - accuracy: 0.9991 - precision: 0.8590 - recall: 0.6223 - auc: 0.9288 - prc: 0.7089 - val_loss: 0.0033 - val_accuracy: 0.9993 - val_precision: 0.8919 - val_recall: 0.5238 - val_auc: 0.9117 - val_prc: 0.7403\n",
            "Epoch 9/100\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 0.0041 - accuracy: 0.9992 - precision: 0.8697 - recall: 0.6409 - auc: 0.9258 - prc: 0.7241 - val_loss: 0.0031 - val_accuracy: 0.9994 - val_precision: 0.9474 - val_recall: 0.5714 - val_auc: 0.9120 - val_prc: 0.7520\n",
            "Epoch 10/100\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 0.0038 - accuracy: 0.9992 - precision: 0.8712 - recall: 0.6285 - auc: 0.9383 - prc: 0.7474 - val_loss: 0.0031 - val_accuracy: 0.9994 - val_precision: 0.8696 - val_recall: 0.6349 - val_auc: 0.9198 - val_prc: 0.7502\n",
            "Epoch 11/100\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 0.0041 - accuracy: 0.9992 - precision: 0.8595 - recall: 0.6440 - auc: 0.9258 - prc: 0.7248 - val_loss: 0.0035 - val_accuracy: 0.9993 - val_precision: 0.8000 - val_recall: 0.6349 - val_auc: 0.9347 - val_prc: 0.7262\n",
            "Epoch 12/100\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 0.0038 - accuracy: 0.9991 - precision: 0.8615 - recall: 0.6161 - auc: 0.9398 - prc: 0.7550 - val_loss: 0.0033 - val_accuracy: 0.9994 - val_precision: 0.8269 - val_recall: 0.6825 - val_auc: 0.9116 - val_prc: 0.7065\n",
            "Epoch 13/100\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 0.0038 - accuracy: 0.9993 - precision: 0.8638 - recall: 0.6873 - auc: 0.9367 - prc: 0.7466 - val_loss: 0.0030 - val_accuracy: 0.9994 - val_precision: 0.8864 - val_recall: 0.6190 - val_auc: 0.9273 - val_prc: 0.7574\n",
            "Epoch 14/100\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 0.0036 - accuracy: 0.9992 - precision: 0.8875 - recall: 0.6594 - auc: 0.9384 - prc: 0.7679 - val_loss: 0.0032 - val_accuracy: 0.9992 - val_precision: 0.8333 - val_recall: 0.5556 - val_auc: 0.9120 - val_prc: 0.7477\n",
            "Epoch 15/100\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 0.0037 - accuracy: 0.9993 - precision: 0.8848 - recall: 0.6656 - auc: 0.9276 - prc: 0.7602 - val_loss: 0.0031 - val_accuracy: 0.9994 - val_precision: 0.9512 - val_recall: 0.6190 - val_auc: 0.9123 - val_prc: 0.7631\n",
            "Epoch 16/100\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 0.0037 - accuracy: 0.9992 - precision: 0.8648 - recall: 0.6533 - auc: 0.9307 - prc: 0.7642 - val_loss: 0.0030 - val_accuracy: 0.9994 - val_precision: 0.8776 - val_recall: 0.6825 - val_auc: 0.9121 - val_prc: 0.7637\n",
            "Epoch 17/100\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 0.0037 - accuracy: 0.9993 - precision: 0.8857 - recall: 0.6718 - auc: 0.9323 - prc: 0.7698 - val_loss: 0.0036 - val_accuracy: 0.9994 - val_precision: 0.8070 - val_recall: 0.7302 - val_auc: 0.9423 - val_prc: 0.7041\n",
            "Epoch 18/100\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 0.0036 - accuracy: 0.9992 - precision: 0.8720 - recall: 0.6749 - auc: 0.9430 - prc: 0.7741 - val_loss: 0.0044 - val_accuracy: 0.9993 - val_precision: 0.7101 - val_recall: 0.7778 - val_auc: 0.9495 - val_prc: 0.6905\n",
            "Epoch 19/100\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0039 - accuracy: 0.9993 - precision: 0.8760 - recall: 0.6780 - auc: 0.9290 - prc: 0.7575 - val_loss: 0.0035 - val_accuracy: 0.9993 - val_precision: 0.8235 - val_recall: 0.6667 - val_auc: 0.9119 - val_prc: 0.6681\n",
            "Epoch 20/100\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 0.0037 - accuracy: 0.9992 - precision: 0.8566 - recall: 0.6842 - auc: 0.9384 - prc: 0.7681 - val_loss: 0.0031 - val_accuracy: 0.9994 - val_precision: 0.8148 - val_recall: 0.6984 - val_auc: 0.9195 - val_prc: 0.7440\n",
            "Epoch 21/100\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 0.0037 - accuracy: 0.9993 - precision: 0.8780 - recall: 0.6904 - auc: 0.9306 - prc: 0.7703 - val_loss: 0.0031 - val_accuracy: 0.9993 - val_precision: 0.9231 - val_recall: 0.5714 - val_auc: 0.9121 - val_prc: 0.7604\n",
            "Epoch 22/100\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 0.0036 - accuracy: 0.9993 - precision: 0.8943 - recall: 0.6811 - auc: 0.9293 - prc: 0.7840 - val_loss: 0.0035 - val_accuracy: 0.9994 - val_precision: 0.8491 - val_recall: 0.7143 - val_auc: 0.9122 - val_prc: 0.6842\n",
            "Epoch 23/100\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 0.0037 - accuracy: 0.9992 - precision: 0.8635 - recall: 0.6656 - auc: 0.9322 - prc: 0.7730 - val_loss: 0.0042 - val_accuracy: 0.9994 - val_precision: 0.8070 - val_recall: 0.7302 - val_auc: 0.9425 - val_prc: 0.6847\n",
            "Epoch 24/100\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 0.0039 - accuracy: 0.9992 - precision: 0.8611 - recall: 0.6718 - auc: 0.9337 - prc: 0.7425 - val_loss: 0.0034 - val_accuracy: 0.9993 - val_precision: 0.8113 - val_recall: 0.6825 - val_auc: 0.9273 - val_prc: 0.6716\n",
            "Epoch 25/100\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 0.0036 - accuracy: 0.9993 - precision: 0.8929 - recall: 0.6966 - auc: 0.9323 - prc: 0.7859 - val_loss: 0.0029 - val_accuracy: 0.9994 - val_precision: 0.9302 - val_recall: 0.6349 - val_auc: 0.9279 - val_prc: 0.7849\n",
            "Epoch 26/100\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 0.0038 - accuracy: 0.9992 - precision: 0.8745 - recall: 0.6687 - auc: 0.9353 - prc: 0.7497 - val_loss: 0.0039 - val_accuracy: 0.9994 - val_precision: 0.7778 - val_recall: 0.7778 - val_auc: 0.9494 - val_prc: 0.6976\n",
            "Epoch 27/100\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 0.0039 - accuracy: 0.9993 - precision: 0.8866 - recall: 0.6780 - auc: 0.9275 - prc: 0.7635 - val_loss: 0.0038 - val_accuracy: 0.9994 - val_precision: 0.8103 - val_recall: 0.7460 - val_auc: 0.9196 - val_prc: 0.6866\n",
            "Epoch 28/100\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 0.0041 - accuracy: 0.9992 - precision: 0.8352 - recall: 0.6749 - auc: 0.9259 - prc: 0.7290 - val_loss: 0.0030 - val_accuracy: 0.9994 - val_precision: 0.8542 - val_recall: 0.6508 - val_auc: 0.9122 - val_prc: 0.7456\n",
            "Epoch 29/100\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 0.0045 - accuracy: 0.9991 - precision: 0.8340 - recall: 0.6378 - auc: 0.9213 - prc: 0.7094 - val_loss: 0.0038 - val_accuracy: 0.9994 - val_precision: 0.8519 - val_recall: 0.7302 - val_auc: 0.9198 - val_prc: 0.6452\n",
            "Epoch 30/100\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 0.0037 - accuracy: 0.9993 - precision: 0.8828 - recall: 0.6997 - auc: 0.9338 - prc: 0.7673 - val_loss: 0.0032 - val_accuracy: 0.9993 - val_precision: 0.9211 - val_recall: 0.5556 - val_auc: 0.9123 - val_prc: 0.7546\n",
            "Epoch 31/100\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 0.0040 - accuracy: 0.9992 - precision: 0.8635 - recall: 0.6656 - auc: 0.9322 - prc: 0.7305 - val_loss: 0.0039 - val_accuracy: 0.9993 - val_precision: 0.7460 - val_recall: 0.7460 - val_auc: 0.9499 - val_prc: 0.6887\n",
            "Epoch 32/100\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 0.0036 - accuracy: 0.9992 - precision: 0.8571 - recall: 0.6687 - auc: 0.9353 - prc: 0.7701 - val_loss: 0.0044 - val_accuracy: 0.9994 - val_precision: 0.7966 - val_recall: 0.7460 - val_auc: 0.9119 - val_prc: 0.6692\n",
            "Epoch 33/100\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 0.0038 - accuracy: 0.9993 - precision: 0.8725 - recall: 0.6780 - auc: 0.9370 - prc: 0.7480 - val_loss: 0.0032 - val_accuracy: 0.9994 - val_precision: 0.8462 - val_recall: 0.6984 - val_auc: 0.9122 - val_prc: 0.6855\n",
            "Epoch 34/100\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 0.0036 - accuracy: 0.9993 - precision: 0.9012 - recall: 0.7059 - auc: 0.9338 - prc: 0.7787 - val_loss: 0.0036 - val_accuracy: 0.9994 - val_precision: 0.7869 - val_recall: 0.7619 - val_auc: 0.9273 - val_prc: 0.7062\n",
            "Epoch 35/100\n",
            "88/89 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9993 - precision: 0.8725 - recall: 0.6822 - auc: 0.9256 - prc: 0.7567Restoring model weights from the end of the best epoch: 25.\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 0.0039 - accuracy: 0.9993 - precision: 0.8735 - recall: 0.6842 - auc: 0.9261 - prc: 0.7585 - val_loss: 0.0039 - val_accuracy: 0.9994 - val_precision: 0.8654 - val_recall: 0.7143 - val_auc: 0.9124 - val_prc: 0.6820\n",
            "Epoch 35: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa945062b20>"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_fs_val_preds = cnn_fs.predict(X_val_norm_fs)>0.5\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ibfqn3-baH_",
        "outputId": "3d8660ce-ef8c-4e5e-f2b7-85248a54b84a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1419/1419 [==============================] - 2s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_results(Y_val_fs,cnn_fs_val_preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "id": "Fi8_BSpEercl",
        "outputId": "75fcafda-6786-4446-962d-efc5fb452414"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     45333\n",
            "           1       0.93      0.63      0.75        63\n",
            "\n",
            "    accuracy                           1.00     45396\n",
            "   macro avg       0.96      0.82      0.88     45396\n",
            "weighted avg       1.00      1.00      1.00     45396\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "qhduQHGjex1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GziAbHKXiwA7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}